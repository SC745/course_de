<h2>Системы для обработки больших данных</h2>
<h3>1. Hadoop</h3>

Apache Hadoop — это фреймворк, предназначенный для масштабируемой, надежной и распределенной обработки и хранения больших объемов данных (большие данные). Hadoop использует простую модель программирования, которая позволяет обрабатывать большие объемы данных на кластерах из тысяч компьютеров, причем каждый из них может быть относительно дешевым и ненадежным. В основе архитектуры Hadoop лежат два основных компонента: Hadoop Distributed File System (HDFS) для хранения данных и MapReduce для обработки данных.

Основные компоненты:
- HDFS - распределенная файловая система
- MapReduce - система обработки и анализа больших данных
- YARN (Yet Another Resource Negotiator) - система управления кластерами
- Экосистема Hadoop - компоненты для расширения функциональности:
  - Hive: Платформа для обработки и анализа больших наборов данных, хранящихся в Hadoop, с помощью SQL-подобного языка запросов (HQL).
  - Pig: Высокоуровневая платформа для создания программ обработки данных на Hadoop с использованием скриптового языка Pig Latin.
  - HBase: Нереляционная распределенная база данных, работающая поверх HDFS, предназначенная для эффективной работы с большими таблицами.
  - Sqoop: Инструмент для переноса данных между Hadoop и реляционными базами данных.
  - Oozie: Система управления рабочими процессами, которая позволяет координировать задания Hadoop, включая задания MapReduce, Pig и Hive.

Подходы к оптимизации:
- Настройка размера блока HDFS: по умолчанию 128 МБ, можно увеличить для уменьшения метаданных.
- Настройка памяти и виртуальных ядер для контейнеров YARN (yarn-site.xml, mapred-site.xml).
- Использование комбинаторов (combiner) в MapReduce для уменьшения передачи данных.
- Настройка сжатия: включить сжатие промежуточных данных (map output) и выходных данных (reduce output).

Настройка кластера:
1. Установка Java (требуется версия 8+).
2. Настройка SSH для беспарольного доступа между узлами.
3. Конфигурация Hadoop (файлы в etc/hadoop/):
  - core-site.xml: Указать URI HDFS (fs.defaultFS).
  - hdfs-site.xml: Настройки репликации, путей для NameNode и DataNode.
  - mapred-site.xml: Выбор фреймворка (YARN).
  - yarn-site.xml: Настройки ResourceManager и NodeManager.
4. Форматирование HDFS командой hdfs namenode -format.

Запуск кластера:
- Старт HDFS: start-dfs.sh.
- Старт YARN: start-yarn.sh.

Безопасность в Hadoop является комплексной и включает несколько компонентов:
- Аутентификация: использование Kerberos для сильной аутентификации и настройка безопасного взаимодействия между службами.
- Авторизация: Использование ACL (Access Control Lists) в HDFS, разрешений на основе ролей (RBAC) с помощью Apache Ranger или Sentry, настройка сервисных уровневых авторизаций (Service Level Authorization).
- Шифрование: В движении (Data in motion) с помощью SSL/TLS и на отдыхе (Data at rest) с помощью HDFS Transparent Encryption.
- Безопасность данных: Маскировка данных, аудит доступа к данным.
- Безопасность YARN: Настройка контроля доступа к очередям и приложениям.


<h3>2. HDFS</h3>

HDFS (Hadoop Distributed File System), является ключевым компонентом экосистемы Hadoop, предназначенным для хранения больших объемов данных на нескольких машинах в распределенном порядке. HDFS обеспечивает высокую пропускную способность к данным, обладает высокой устойчивостью к отказам и предназначен для развертывания на недорогом оборудовании. Она разбивает большие файлы на меньшие блоки (размером по умолчанию 128MB или 256MB, однако этот параметр можно перенастроить) и распределяет их по нескольким узлам в кластере. Это позволяет эффективно обрабатывать большие наборы данных с помощью программной модели MapReduce.

Основные компоненты
- NameNode - управляющий узел, который хранит метаданные всей файловой системы. NameNode отслеживает структуру каталогов и метаданные для всех файлов, включая информацию о том, какие блоки данных составляют каждый файл и на каких DataNodes эти блоки расположены.
- DataNode - узлы хранящие фактические данные. В HDFS файл разбивается на один или несколько блоков, и каждый блок копируется на несколько DataNodes в соответствии с заданным фактором репликации для обеспечения отказоустойчивости.
- Secondary NameNode - несмотря на свое название, не является резервным узлом для NameNode. Его основная функция — периодически сливать изменения журнала NameNode с его текущим состоянием в файловой системе, помогая уменьшить размер журнала и предотвращая потерю данных.

Типы данных:
- Текстовые форматы (CSV, TSV, JSON, лог-файлы)
- Последовательные файлы (Sequence Files):
- Колоночные форматы (Parquet, ORC)
- Форматы сериализации (Avro, Protocol Buffers, Thrift)
- Специализированные форматы (RCFile)

<h4>Чтение и запись</h4>

Процесс чтения:
1. Клиент запрашивает у NameNode расположение блоков файла
2. NameNode возвращает список DataNode, содержащих каждый блок
3. Клиент обращается напрямую к DataNode за блоками
4. DataNode передают данные клиенту
5. Если DataNode недоступен, клиент запрашивает блок у реплики

Процесс записи:
1. Клиент запрашивает у NameNode создание нового файла
2. NameNode проверяет права и возвращает список DataNode для записи
3. Клиент начинает передачу данных первому DataNode в конвейере
4. DataNode передает данные следующему DataNode в цепочке
5. После записи всех блоков клиент сообщает NameNode о завершении
6. NameNode обновляет метаданные файловой системы

<h4>Шифрование данных</h4>

Encryption Zone — зашифрованный каталог в HDFS
Key Management Server (KMS) — сервер управления ключами
Crypto Codec — алгоритмы шифрования (AES/CTR/NoPadding)

Процесс работы с зашифрованными данными:
1. Создание зоны шифрования:
```bash
hdfs crypto -createZone -keyName mykey -path /secure_data
```
2. Запись в зашифрованную зону - данные автоматически шифруются перед записью на диск (используется Data Encryption Key (DEK))
3. Чтение из зашифрованной зоны - данные автоматически расшифровываются при чтении

<h3>3. MapReduce</h3>

MapReduce — это модель обработки для анализа больших объемов данных. Работает, разделяя задачу на множество маленьких задач, которые могут обрабатываться параллельно.

Основная идея: Разделить обработку данных на две основные фазы — Map и Reduce — и позволить системе автоматически заботиться о сложных аспектах распределенных вычислений, таких как параллелизация, распределение данных, отказоустойчивость и межпроцессное взаимодействие.

Ключевые принципы:
- Масштабируемость (Scalability) -  модель легко масштабируется "горизонтально" — чтобы обработать больше данных, вы просто добавляете больше серверов в кластер, а не улучшаете существующие.
- Устойчивость к сбоям (Fault Tolerance) - система автоматически обрабатывает отказы отдельных узлов, переназначая их задачи другим рабочим узлам.

<h4>Основные этапы выполнения задач</h4>

Выполнение задачи в MapReduce состоит из нескольких четко определенных этапов:
1. Подготовка (Input Splits) - система разбивает входные данные на логические куски фиксированного размера, называемые сплитами, которые будут обработаны отдельными задачами на фазе Map.
2. Фаза Map (Отображение) - на этом этапе запускается множество Mapper'ов (задач Map), обычно по одной на каждый сплит. Mapper читает данные построчно (или по другим записям), пользовательская функция map применяется к каждой входной записи. Её задача — преобразовать входные данные в промежуточный формат "ключ-значение".
3. Shuffle и Sort (Перемешивание и Сортировка) - состоит из двух подэтапов:
  - Partitioning (Разделение): Выходные данные всех Mapper'ов распределяются по Reduce-задачам. По умолчанию используется хэш-функция от ключа: hash(ключ) mod N, где N — количество Reduce-задач. Это гарантирует, что все пары с одинаковым ключом попадут в одну и ту же Reduce-задачу.
  - Sorting (Сортировка): Система собирает все промежуточные пары "ключ-значение" для одного и того же ключа (например, для слова "hello") и сортирует их по ключу. Перед началом фазы Reduce данные для каждого Reducer'а представляют собой отсортированный список, где за одним ключом следует список всех его значений.
4. Фаза Reduce (Сведение) - на этом этапе запускаются Reducer'ы (задачи Reduce). Пользовательская функция reduce применяется к каждому уникальному ключу и его списку значений. Её задача — агрегировать эти значения и выдать финальный результат - отсортированную группу данных, где уникальный ключ ассоциирован со списком всех его значений.
5. Запись результата (Output) - каждый Reducer записывает свои выходные данные в конечные файлы (обычно по одному на узел, где работал Reducer). Эти файлы хранятся в распределенной файловой системе (например, HDFS в Hadoop).

<h3>4. YARN</h3>

YARN обеспечивает управление ресурсами кластера, позволяя множеству приложений эффективно разделять общие ресурсы.

Роль YARN в экосистеме Hadoop:
- Отделение управления ресурсами от программируемой модели
- Поддержка разнородных рабочих нагрузок (не только MapReduce)
- Централизованное управление кластерными ресурсами
- Обеспечение многопользовательской среды

YARN состоит из следующих компонентов:
- ResourceManager (RM) - менеджер ресурсов, который отвечает за распределение ресурсов, необходимых для работы распределенных приложений, и наблюдение за узлами кластера, где эти приложения выполняются. ResourceManager включает планировщик ресурсов (Scheduler) и диспетчер приложений (ApplicationsManager, AsM).
- ApplicationMaster (AM) - мастер приложения, ответственный за планирование его жизненного цикла, координацию и отслеживание статуса выполнения, включая динамическое масштабирование потребления ресурсов, управление потоком выполнения, обработку ошибок и искажений вычислений, выполнение локальных оптимизаций. Каждое приложение имеет свой экземпляр ApplicationMaster. ApplicationMaster выполняет произвольный пользовательский код и может быть написан на любом языке программирования благодаря расширяемым протоколам связи с менеджером ресурсов и менеджером узлов.
- NodeManager (NM) – агент, запущенный на узле кластера, который отвечает за отслеживание используемых вычислительных ресурсов (CPU, RAM и пр.), управление логами и отправку отчетов об использовании ресурсов планировщику. NodeManager управляет абстрактными контейнерами – ресурсами узла, доступными для конкретного приложения.
- Container - набор физических ресурсов (ЦП, память, диск, сеть) в одном вычислительном узле кластера.

<h4>Управление ресурсами</h4>

Управление ресурсами в YARN осуществляется с помощью планировщиков. Планировщик обрабатывает распределение ресурсов для заданий, отправленных в YARN, согласно политике выделения, установленной для его типа. Различают 3-х типа планировщиков:
- FIFO (First In First Out): Самый простой, который запускает приложения в порядке подачи, помещая их в очередь. Приложение, отправленное первым, сначала получает ресурсы, а по его завершении планировщик обслуживает следующее приложение в очереди. FIFO не подходит для общих кластеров, поскольку большие приложения будут занимать все ресурсы, а очереди станут длиннее из-за более низкой скорости обслуживания.
- Capacity: Более сложный вариант FIFO, который позволяет совместно использовать кластер Hadoop между группами внутри организации, выделяя каждой команде определенную емкость общего кластера. Очереди могут быть дополнительно разделены иерархически, но приложения в них по умолчанию планируются с использованием расписания FIFO. Отдельная выделенная очередь позволяет запускать небольшие задания сразу после их отправки.
- Fair: Самый продвинутый вариант планировщика с учетом приоритетности заданий, который стремится распределить ресурсы так, чтобы все запущенные приложения получили одинаковую долю. Fair Scheduler позволяет приложениям YARN совместно и динамически использовать ресурсы в большом кластере Hadoop без предварительного указания их емкости.

FIFO-планировщик блокирует небольшое задание до завершения большого задания. Capacity-планировщик поддерживает отдельную очередь для небольших заданий, чтобы запускать их сразу после поступления запроса. Однако в этом случае выполнение больших заданий требует больше времени из-за лимитов емкости кластера, ограниченной очередью. Так Capacity-планировщик гарантирует, что приоритетные и мелкие задания не будут слишком долго задерживаться по сравнению с планировщиком FIFO. Но при наличии свободных ресурсов, Capacity-планировщик может выделить их для заданий в очереди, даже если это повысит ее емкость. Это называется эластичностью очереди.

У Fair-планировщика нет требований к резервированию емкости: он динамически распределяет ресурсы по всем принятым заданиям. Когда задание запускается, оно получает все ресурсы кластера, если является единственным. Следующее запущенное задание получит ресурсы при высвобождении контейнеров – фиксированного объема ОЗУ и ЦП. По завершении небольшого задания планировщик назначает ресурсы большому. Таким образом, Fair-планировщик устраняет недостатки FIFO и Capacity, позволяя своевременно завершать небольшие задания с высокой степенью утилизации кластера, которая показывает эффективность использования его ресурсов. На практике именно этот вариант используется чаще всего.

<h3>5. Apache Spark</h3>

Apache Spark — это универсальная вычислительная система для обработки больших данных, предоставляющая комплексные API на Java, Scala, Python и R, а также поддерживающая разнообразные задачи обработки данных: пакетную обработку, потоковую передачу данных, обработку графов, и машинное обучение. Spark разработан для работы с большими объемами данных в распределенной среде, предлагая более быструю и гибкую альтернативу MapReduce Hadoop. Spark позволяет проводить обработку данных в памяти, что значительно увеличивает скорость выполнения приложений по сравнению с MapReduce. Поддержка чтения и записи данных в HDFS, а также совместимость с Hadoop YARN для управления ресурсами кластера, обеспечивает легкую интеграцию Spark в существующую инфраструктуру Hadoop.

Основные компоненты:
- Spark Core: Основная часть фреймворка, предоставляющая основные функции распределенной обработки данных, в том числе управление памятью, отказоустойчивость, распределение задач и базовый API.
- Spark SQL: Модуль для работы с структурированными данными. Позволяет выполнять SQL-запросы, используя традиционный синтаксис SQL, а также HiveQL для интеграции с Apache Hive.
- Spark Streaming: Предназначен для обработки потоковых данных в реальном времени. Данные могут поступать из различных источников, таких как Kafka, Flume или Kinesis.
- MLlib (Machine Learning Library): Библиотека машинного обучения, предоставляющая множество алгоритмов машинного обучения и утилит для обработки данных.
- GraphX: Библиотека для обработки графов и выполнения графовых вычислений, предоставляющая API для создания и трансформации неизменяемых графов, а также выполнения параллельных операций над ними.

Архитектура:
- Driver Program: Программа, которая создает SparkContext, инициирующий Spark-приложение. Это центральный узел управления, который конвертирует пользовательскую программу в задачи и распределяет их между исполнителями (Executors). Драйвер также отвечает за планирование задач и восстановление от сбоев.
- SparkContext: Контекст выполнения приложения, управляющий доступом к кластеру через Cluster Manager.
- Cluster Manager: Spark может работать на различных менеджерах кластеров, включая Spark Standalone, YARN (Hadoop), Mesos и Kubernetes. Менеджер кластера отвечает за выделение ресурсов драйверу и исполнителям.
- Executors: Процессы, выполняющиеся на узлах рабочего кластера, которые обрабатывают задачи вычислений и хранят данные приложения в памяти или на диске. Executors взаимодействуют с программой "Driver Program" для выполнения кода.
- RDD (Resilient Distributed Dataset): Это основная абстракция данных в Spark, представляющая собой неизменяемую коллекцию элементов, распределенных по кластеру, которую можно обрабатывать в параллельном режиме. Именно RDD является основным вычислительным примитивом Spark, над которым можно делать параллельные вычисления и преобразования с помощью встроенных и произвольных функций, в том числе с помощью временных окон.

Настройка кластера:
Режимы развертывания:
- Standalone: Встроенный менеджер ресурсов Spark.
- YARN: Интеграция с Hadoop YARN.
- Kubernetes: Запуск в контейнерах.

Этапы настройки:
1. Установка Scala/Java.
2. Конфигурация:
  - Настройка spark-env.sh (память, ядра, мастер).
  - Указание рабочих узлов в slaves.
3. Запуск:
  - Мастер: ./sbin/start-master.sh.
  - Воркеры: ./sbin/start-workers.sh.

Интеграция с Hadoop:
- Использование HDFS для хранения данных.
- Настройка HADOOP_CONF_DIR в Spark для доступа к YARN.

<h4>RDD и DataFrame</h4>

RDD (Устойчивые Распределенные Наборы Данных) — это основополагающая структура данных в Apache Spark, представляющая собой неизменяемую, распределённую коллекцию объектов, которую можно обрабатывать параллельно. RDD предоставляют мощную абстракцию для работы с данными, что позволяет Spark обеспечивать высокую производительность при обработке больших объёмов данных. Вот детальный обзор RDD, включая их ключевые характеристики, методы создания, операции и преимущества.

Ключевые характеристики RDD:
- Неизменяемость и разбиение - RDD неизменяемы, то есть однажды созданные, они не могут быть изменены. Это упрощает программирование и способствует достижению консистентности в вычислениях. RDD распределены по узлам кластера, что обеспечивает параллельную обработку.
- Устойчивость к ошибкам - RDD разработаны таким образом, чтобы быть устойчивыми к сбоям за счёт информации о происхождении. В случае потери части RDD из-за сбоя узла, Spark может восстановить её, используя историю операций, которые изначально создали эту часть.
- Постоянство - пользователи могут сохранять или кэшировать RDD в памяти или на диске узлов кластера. Это особенно полезно для итеративных алгоритмов, которым требуется многократный доступ к одному и тому же набору данных.
- Ленивые вычисления - операции с RDD являются ленивыми, то есть они не выполняются немедленно. Spark начинает вычисления RDD только когда вызывается действие, требующее возврата результата в программу-драйвер.
- Прозрачность размещения - RDD предоставляют абстрактный интерфейс, скрывая сложности распределения данных и позволяя вычислительным процессам получать непосредственный доступ к данным.

Создание:
```python
# Параллелизация
data = [1, 2, 3, 4, 5]
rdd = sparkContext.parallelize(data)

# Ссылка на датасет
rdd = sparkContext.textFile("hdfs://namenode:8020/path/to/file.txt")
```

Пример использования (подсчет слов):

```python
from pyspark import SparkContext
sc = SparkContext()
textFile = sc.textFile("hdfs://namenode:8020/path/to/text.txt")
words = textFile.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)
words.collect()
```

В Spark есть возможность создания DataFrame:
```python
from pyspark.sql import functions as F

df = spark.sql("SELECT * FROM employee WHERE age > 20")
filtered_df = df.filter(df["age"] > 20)                                          # Фильтрация
aggregated_df = df.groupBy("department").agg(F.avg("wage").alias("avg_wage"))    # Агрегация
aggregated_df.write.parquet("path/to/output")                                    # Сохранение
```

<h4>Трансформации и действия</h4>

Трансформации (Transformations) - это операции, которые создают новый RDD или DataFrame из существующего. Они ленивые, то есть Spark не выполняет их немедленно, а вместо этого строит граф направленных операций (Directed Acyclic Graph, DAG), запоминая все преобразования, которые нужно применить к исходным данным.

Примеры:
- `map(func)`: Применяет функцию func к каждому элементу RDD.
- `filter(func)`: Возвращает элементы, для которых функция func возвращает true.
- `flatMap(func)`: Подобна map, но каждый входной элемент может быть отображен в 0 или более выходных элементов.
- `distinct()`: Возвращает уникальные элементы.
- `union(otherRDD)`: Объединяет два RDD.
- `join(otherRDD)`: Для парных RDD (ключ-значение) возвращает RDD со всеми парами элементов с совпадающими ключами.
- `groupByKey()`: Группирует значения по ключу в парном RDD.
- `select()`, `where()`, `groupBy()`: Преобразования для DataFrame (аналоги `map`, `filter` и т.д.).

Действия - это операции, которые запускают вычисления, чтобы вернуть результат драйвер-программе или сохранить его во внешнюю систему хранения. Они активные, то есть при вызове действия Spark просматривает весь построенный DAG, оптимизирует его (например, с помощью Catalyst Optimizer) и затем выполняет все необходимые трансформации для получения результата.

Примеры:
- `collect()`: Возвращает все элементы набора данных в виде массива на драйвере. (Осторожно: используйте только на маленьких наборах данных!)
- `count()`: Возвращает количество элементов в RDD/DataFrame.
- `first()`: Возвращает первый элемент (аналогично take(1)).
- `take(n)`: Возвращает первые n элементов массива.
- `reduce(func)`: Агрегирует элементы набора данных с помощью функции func (которая должна быть коммутативной и ассоциативной).
- `foreach(func)`: Применяет функцию func к каждому элементу (например, для сохранения в базу данных).
- `saveAsTextFile(path)`, `saveAsTable(...)`: Сохраняют результат в файловую систему или таблицу.

<h4>Оптимизация</h4>

Оптимизация Spark:
- Кэширование данных: если данные используются многократно, можно кэшировать их в памяти (cache() или persist()).
- Настройка уровня параллелизма: количество партиций RDD. Можно увеличить с помощью repartition() или coalesce().
- Настройка памяти Executor: задать достаточное количество памяти для исполнителей (executors) и настроить управление памятью (например, spark.memory.fraction).
- Использование вещательных переменных (broadcast variables) для больших read-only данных, которые используются в нескольких операциях.
- Использование аккумуляторов (accumulators) для эффективного агрегирования информации.
- Выбор правильных трансформаций: например, reduceByKey вместо groupByKey, когда это возможно, чтобы уменьшить shuffling.
- Настройка сериализации: использование Kryo сериализации для повышения производительности.
- Мониторинг и настройка Garbage Collection: чтобы минимизировать паузы.

Catalyst Optimizer - это движок оптимизации запросов в Spark SQL, который автоматически преобразует и оптимизирует логические планы выполнения запросов. Основные этапы работы:
1. Анализ (Analysis):
  - Проверка существования таблиц, столбцов
  - Разрешение типов данных
  - Построение неоптимизированного логического плана
2. Логическая оптимизация:
  - Predicate Pushdown: Фильтры переносятся ближе к источнику данных
  - Constant Folding: Вычисление константных выражений
  - Column Pruning: Выбор только необходимых столбцов
  - Null Propagation: Оптимизация обработки NULL-значений
3. Физическое планирование:
  - Выбор алгоритмов выполнения (Broadcast Hash Join vs Sort Merge Join)
  - Определение стратегий партиционирования
4. Генерация кода (Whole-Stage Code Generation):
  - Генерация специализированного байт-кода Java
  - Устранение виртуальных вызовов методов
  - Векторизация операций

<h4>Потоковая обработка данных</h4>

Spark Streaming (устаревший) и Structured Streaming (рекомендуемый) - это компоненты для обработки потоковых данных.

Structured Streaming построен на основе Spark SQL и использует тот же движок оптимизации (Catalyst). Он обрабатывает потоковые данные как бесконечные таблицы.

Основные концепции Structured Streaming:
- Input and Output: Источники данных (Kafka, файлы, сокеты) и приемники (файлы, Kafka, консоль, память).
- Trigger: Определяет, когда проверять новые данные (например, каждые 1 секунду).
- Watermark: Для обработки задержанных данных в окновых агрегациях.
- Output Modes: Append (только новые строки), Update (обновленные строки), Complete (все агрегированные данные).

Особенности:
- Exactly-once семантика: Гарантия обработки каждого события ровно один раз.
- Интеграция с источниками: Поддержка Kafka, Amazon Kinesis, HDFS и др.
- Оконные операции: Агрегация по временным окнам (например, каждые 5 минут).
