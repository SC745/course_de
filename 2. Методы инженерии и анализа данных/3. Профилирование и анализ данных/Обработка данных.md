<h2>Обработка данных</h2>
<h3>1. Профилирование данных</h3>

Профилирование данных — это процесс детального анализа исходных данных перед их использованием в аналитических целях. Этот процесс позволяет аналитикам данных оценить качество данных, обнаружить потенциальные проблемы и понять структуру данных.

Цели:
- Понимание структуры данных - идентификация основных атрибутов, типов данных и связей между различными элементами данных.
- Оценка качества данных - выявление проблем с данными, таких как пропущенные значения, дубликаты, неконсистентные или аномальные данные.
- Обнаружение паттернов - анализ распределения данных, частоты значений и выявление возможных взаимосвязей между различными полями данных.
- Подготовка к интеграции данных - профилирование данных помогает оценить совместимость данных из разных источников и разработать правила консолидации.
- Снижение рисков - профилирование данных позволяет выявить потенциальные проблемы до того, как данные будут использованы в критически важных процессах, что снижает риск принятия неправильных решений на основе некорректных данных.

Основные методы:
- Статистический анализ - расчет средних значений, медиан, стандартных отклонений, минимальных и максимальных значений для количественных данных, анализ частоты встречаемости различных значений для качественных данных.
- Анализ уникальности и дубликатов - поиск уникальных записей и идентификация дубликатов, что важно для разработки процедур по очистке данных.
- Проверка правил валидации - применение предварительно определенных правил для проверки соответствия данных требуемым форматам и ограничениям.
- Анализ отсутствующих данных - определение полей с пропущенными значениями и оценка их влияния на анализ.
- Корреляционный анализ - исследование взаимосвязей между различными полями данных для выявления потенциальных зависимостей.

Инструменты:
- Специализированные инструменты профилирования - такие как Informatica Data Quality, Talend, DataCleaner предлагают готовые решения для анализа и оценки качества данных.
- SQL и скриптовые языки - Написание пользовательских запросов и скриптов для специфического анализа данных, используя SQL, Python (pandas), R.
- BI инструменты - Использование инструментов бизнес-аналитики, таких как Tableau или Power BI, для визуализации и анализа данных.

<h4>Типы аномалий в данных</h4>

Аномалии (или отклонения) — это наблюдения, которые значительно отличаются от остальных данных и могут искажать анализ и модели. Их делят на три основных типа:
- Выбросы - точки данных, которые значительно отклоняются от общего распределения:
  - Глобальные - значение аномально по отношению ко всему набору данных.
  - Контекстные - значение аномально в определенном контексте.
  - Коллективные выбросы - группа точек данных, которые вместе являются аномальными.
- Пропуски - отсутствующие значения в наборе данных:
  - MCAR (Missing Completely At Random): Пропуск никак не связан с другими данными (например, случайный сбой датчика).
  - MAR (Missing At Random): Пропуск связан с другими наблюдаемыми переменными. Например, вероятность пропуска зарплаты выше у безработных (значение "безработный" есть в данных).
  - MNAR (Missing Not At Random): Пропуск связан с самим пропущенным значением. Например, люди с высокой зарплатой чаще отказываются её указывать.
- Дубликаты - Полностью или почти полностью повторяющиеся записи.
  - Точные - полностью идентичные записи
  - Неполные - дубликаты по одному или нескольким столбцам
  - Неточные - почти идентичные записи

<h4>Базовые статистические метрики</h4>

Для количественных данных:
- Меры центральной тенденции - среднее, медиана, мода
- Меры изменчивости: минимум / максимум, Дисперсия и стандартное отклонение, 25-й и 75-й процентили

Для качественных данных:
- Количество уникальных значений
- Частота категорий

Общие показатели:
- Количество записей
- Количество пропусков
- Доля пропусков

<h4>Обнаружение пропущенных значений</h4>

Методы обнаружения пропущенных значений:
- Визуальный осмотр с помощью тепловых карт -  визуализация, где цветом кодируется наличие пропуска (например, белый цвет для NaN). Позволяет быстро увидеть паттерны пропусков.
- Статистический анализ - `df.isnull().sum()` для подсчета пропусков в каждом столбце и `df.isnull().mean()` для подсчета доли пропусков.
- Автоматизированные инструменты - библиотеки pandas-profiling (теперь ydata-profiling) автоматически генерируют отчеты с количеством и процентом пропусков.

<h4>Обнаружение дубликатов</h4>

Методы обнаружения различных видов дубликатов:
- Точные дубликаты - поиск полностью идентичных строк во всем наборе данных с помощью `df.duplicated()`.
- Неполные дубликаты - поиск дубликатов по одному или нескольким столбцам, которые должны быть уникальными (например, user_id, email). Пример: `df.duplicated(subset=['email'])`
- Неточные дубликаты - поиск записей, которые почти идентичны, но могут содержать опечатки или небольшие различия с помощью алгоритмов нечеткого сравнения строк, такие как расстояние Левенштейна (Levenshtein distance).

<h4>Оценка полноты данных</h4>

Полнота — это степень отсутствия пропущенных (null, пустых) значений в наборе данных. Низкая полнота напрямую влияет на достоверность любых анализов и моделей, построенных на этих данных.

Основные методы оценки полноты данных:
- Подсчет пропусков (Null Count) - абсолютное количество пропущенных значений для каждого столбца (атрибута).
- Процент заполненности (Completeness Rate) - доля непустых значений в столбце.
- Анализ на уровне записей (Record-Level Completeness) - определяется, сколько записей являются "полностью заполненными" (все обязательные поля присутствуют).
- Анализ "скрытых" пропусков (Analysis of "Hidden" Nulls) - данные могут быть технически не NULL, но по смыслу являться пропусками. Например, строки-заглушки: "N/A", "Unknown", "Not Provided", "0" (если 0 не является валидным значением), пустая строка "". Необходимо составить словарь таких "заглушек" и искать их в данных теми же методами, что и обычные NULL.
- Оценка против бизнес-правил (Completeness vs. Business Rules) - полнота оценивается не абстрактно, а в контексте бизнес-требований. Некоторые поля могут быть обязательными только при определенных условиях.

<h4>Применение машинного обучения</h4>

Есть несколько методов машинного обучения:
- K-Means: После кластеризации точки, которые находятся далеко от центроида своего кластера, могут считаться аномалиями. Также можно использовать расстояние до ближайшего центроида.
- DBSCAN: Помечает точки, которые не попадают ни в один кластер (шумовые точки), как аномалии.
- Isolation Forest: Строит деревья, изолирующие каждую точку. Чем меньше средняя глубина изоляции точки, тем более она аномальна.

<h3>2. Картирование данных</h3>

Картирование данных — это процесс создания соответствий между исходными и целевыми данными. Это ключевой шаг в процессе интеграции, миграции, трансформации и подготовки данных, который обеспечивает правильное и эффективное использование данных в аналитических и бизнес-процессах. Для аналитиков данных картирование играет критическую роль в обеспечении точности и целостности аналитических данных. Оно помогает понять структуру исходных данных, обнаружить и устранить несоответствия и проблемы качества, а также гарантировать, что данные адекватно представлены для анализа и отчетности. Эффективное картирование данных требует тесного сотрудничества между аналитиками и инженерами данных. Инженеры обеспечивают техническую реализацию процессов ETL (Extract, Transform, Load), в то время как аналитики определяют бизнес-правила и требования к данным.

Основные этапы картирования данных
- Анализ исходных данных - понимание структуры, типов данных и качества исходных данных.
- Определение целевой структуры - выявление структуры и форматов целевых данных в соответствии с бизнес-требованиями и аналитическими задачами.
- Создание карты соответствий - разработка детального описания того, как каждое поле исходных данных соответствует полю в целевой структуре, включая трансформации и правила обработки.
- Реализация и проверка - техническая реализация картирования с использованием инструментов ETL и последующая проверка корректности передачи данных.

<h4>Анализ бизнес-требований</h4>

Цель анализа - Перевести язык бизнес-требований на язык данных и технических спецификаций.

Ключевые действия:
- Выявление Stakeholders (Заинтересованных сторон): Кто является потребителем данных? Маркетологи, аналитики, отдел продаж, руководство? У каждого свои потребности и метрики.
- Формулировка бизнес-вопросов: Превращение расплывчатых пожеланий в конкретные вопросы.
- Определение Ключевых Показателей Эффективности (KPI): Какие метрики будут отвечать на бизнес-вопросы?
- Согласование требований к качеству данных: Насколько точными, полными и актуальными должны быть данные для принятия решений? Например, для финансовой отчетности требуется 100% точность, а для рекомендательной системы в реальном времени допустимы небольшие погрешности.
- Приоритизация: Какие потребности наиболее критичны для бизнеса? Это поможет спланировать разработку поэтапно.

<h4>Виды анализа данных</h4>

Основные виды анализа на этапе картирования:
- Профилирование данных (Data Profiling): Автоматизированный сбор статистики о данных для понимания их общей структуры и содержания:
  - Структура: Типы данных (int, string, date), длина полей.
  - Статистика: Min, max, среднее, медиана, процентили.
  - Распределение: Частотность значений, гистограммы.
  - Заполненность: Количество NULL/пустых значений в каждом столбце.
- Анализ качества данных (Data Quality Assessment): Оценка данных по заранее определенным метрикам качества:
  - Точность: Соответствуют ли данные реальности? (Например, возраст клиента = 200 лет — неточное значение).
  - Полнота: Все ли необходимые поля заполнены?
  - Непротиворечивость: Одинаково ли данные представлены в разных источниках? (Дата в формате DD.MM.YYYY в одном и MM-DD-YYYY в другом).
  - Уникальность: Нет ли дубликатов записей?
  - Своевременность (Timeliness): Актуальны ли данные на момент использования?
- Структурный анализ: Изучение формальной структуры данных — схемы таблиц, форматов файлов (JSON, Parquet, Avro), иерархий. JSON-сообщение: какие вложенные поля есть, являются ли они обязательными, какого они типа.
- Семантический анализ:
  - Создание глоссария (Data Dictionary): Что означает поле revenue? Это доход с НДС или без? Это подтвержденный доход или ожидаемый?
  - Классификация данных: Являются ли данные персональными (PII), финансовыми, операционными?

<h4>Анализ зависимостей данных</h4>

Типы зависимостей:

Зависимости "Наследования" (Lineage): Прослеживание жизненного цикла данных от источника до потребителя:
- Прямая трассировка (Forward Lineage): "Если я изменю эту таблицу в источнике, какие дашборды и отчеты это затронет?"
- Обратная трассировка (Backward Lineage): "Чтобы понять, как формируется этот показатель в дашборде, к каким исходным таблицам и полям мне нужно обратиться?"
Зависимости "Влияния" (Impact Analysis): Частный случай Lineage, фокусирующийся на оценке последствий изменений.
Структурные зависимости (внутри данных):
- Внешние ключи (Foreign Keys).
- Функциональные зависимости: Когда значение одного поля однозначно определяет значение другого (например, OrderID -> OrderDate).
- Ссылочная целостность: Проверка, что все ссылки ведут на существующие объекты (нет "битых" связей).

Инструменты и подходы для анализа зависимостей в Big Data:
- Метаданные-менеджеры и Data Catalogs: Это специализированные системы, которые автоматически сканируют источники данных, ETL-процессы, скрипты и выстраивают граф зависимостей. Примеры: Apache Atlas, Amundsen, DataHub, Collibra.
- Парсинг кода: Анализ SQL-запросов (в dbt, Hive, Spark SQL), DAG-ов (в Apache Airflow), скриптов для выявления ссылок на таблицы и поля.
- Логирование: В Big Data-платформах (как YARN) и системах обработки (как Spark) есть логи, которые можно анализировать для понимания, какие данные кем и когда читались.

<h4>Картирование неструктурированных данных</h4>

Основные шаги картирования неструктурированных данных:
- Обнаружение и инвентаризация: найти все источники неструктурированных данных в организации.
- Классификация и категоризация: определить тип данных (текст, изображение, etc) и их содержание.
- Извлечение метаданных: получить информацию о данных (дата создания, автор, размер, etc).
- Индексирование и организация: сделать данные доступными для поиска и анализа.

Методы анализа неструктурированных данных:
- Текстовая аналитика: NLP (обработка естественного языка), включая тональный анализ, извлечение сущностей, тематическое моделирование.
- Компьютерное зрение: для изображений и видео (объекты, лица, сцены).
- Аудиоаналитика: распознавание речи, классификация звуков.
- ML и AI: модели для классификации, кластеризации, прогнозирования.

<h3>3. Очистка и подготовка данных</h3>

Очистка и подготовка данных — это критически важные этапы в процессе работы с данными, которые значительно влияют на качество анализа и точность выводов. Этот процесс включает в себя серию действий по трансформации "сырых" данных в формат, пригодный для анализа.

Основные этапы очистки и подготовки данных:
- Оценка качества данных - идентификация и анализ проблем в данных, таких как пропущенные значения, дубликаты, ошибочные данные, аномалии.
- Очистка данных - удаление, замена средним/медианой, интерполяция на основе моделей пустых значений, коррекция явных ошибок и фильтрация аномальных значений, определение и удаление дублирующих записей.
- Трансформация данных - приведение данных к единому формату и масштабу, их агрегация, генерация новых атрибутов из существующих данных для улучшения аналитических моделей.
- Интеграция данных - слияние данных из различных источников для создания комплексного представления информации, обеспечение целостности данных путем сопоставления ключевых атрибутов между разными наборами данных.
- Повышение качества данных - проверка данных на соответствие предварительно определенным правилам и стандартам (валидация) и дополнение данных информацией из внешних источников (обогащение).

Основные инструменты для очистки данных:
- Python:
  - Pandas: Основная библиотека для работы с данными в памяти. Отлично подходит для прототипирования и очистки небольших (объемом до оперативной памяти) датасетов. Для Big Data часто используется на этапе исследования и создания скриптов для отдельных узлов.
  - PySpark (Spark с Python API): Это главный инструмент для очистки в распределенной среде. Он позволяет использовать знакомый синтаксис Python/Pandas, но выполняет операции на кластере компьютеров, обрабатывая терабайты и петабайты данных.
  - Dask: Еще одна библиотека для параллельных вычислений в Python, которая имитирует API Pandas, но масштабируется на кластеры.
  - Специализированные библиотеки: great_expectations (для проверки качества данных), scikit-learn (для импутации, кодирования), numpy.
- SQL:
  - Distributed SQL Engines: Hive, Impala, PrestoDB, Spark SQL. Позволяют выполнять операции очистки (фильтрация, агрегация, джойны) прямо в распределенной системе, используя знакомый синтаксис SQL. Часто это самый эффективный способ начать очистку.
- Apache Spark: Флагманский инструмент. Предоставляет единую платформу для пакетной обработки, стриминга и сложных ETL-пайплайнов. Его основная сила — в распределенных вычислениях в памяти.
- Apache Flink: Часто используется для обработки данных в реальном времени (streaming). Имеет мощный API для работы с временными рядами и обработки событий.
- Традиционные ETL-инструменты: Informatica, Talend, IBM DataStage. Имеют визуальные интерфейсы для построения пайплайнов очистки и могут интегрироваться с Hadoop/Spark.

<h4>Методы очистки данных</h4>

Ручные:
- Профилирование данных (Data Profiling) - Автоматизированный анализ датасета для сбора статистики: типы данных, количество уникальных значений, частота, минимум/максимум, количество пропусков, распределение. Инструменты: pandas-profiling, great_expectations, встроенные функции в Spark (describe(), summary()).
- Визуальный осмотр и анализ (EDA - Exploratory Data Analysis) - построение гистограмм, боксплотов, scatter-plot'ов для визуального обнаружения выбросов, аномалий и странных распределений. Инструменты: Matplotlib, Seaborn, Plotly в Python.
- Создание правил и скриптов - Data engineer на основе доменных знаний и EDA пишет конкретный код для обработки: "заменить все значения 'N/A' на NaN", "удалить строки, где возраст > 120", "привести все названия городов к верхнему регистру".

Автоматические:
- Обнаружение и импутация пропусков (Missing Data Imputation) - замена на среднее, медиану, моду (в Spark/Pandas это делается одной строкой).
- Обнаружение и обработка выбросов (Outlier Detection) - правило трех сигм (Z-score), метод межквартильного размаха (IQR).
- Алгоритмы ML: Isolation Forest, Local Outlier Factor (LOF). Эффективны для многомерных данных.
- Стандартизация и нормализация - автоматические пайплайны: В scikit-learn или PySpark MLlib можно создать пайплайн, который автоматически применяет StandardScaler, MinMaxScaler ко всем числовым признакам.
- Дедубликация (Deduplication) - Spark имеет встроенные функции dropDuplicates() или более сложные методы на основе приблизительного сопоставления (например, MinHash для LSH - Locality-Sensitive Hashing).
- Валидация данных (Data Validation) - great_expectations позволяет декларативно описать ожидания от данных ("столбец A должен быть уникальным", "значения в столбце B должны быть между 0 и 100"). Эти правила затем автоматически проверяются при поступлении новых данных.

<h4>Заполнение пропущенных данных</h4>

Методы:
- Удаление (Deletion) - Удаление строк в зависимости от доли пропусков. Применяется только если пропуски не зависят от других значений (MCAR) и их мало.
- Заполнение константами - замена пропуска на заранее заданное значение. Для числовых данных: 0, -1. Для категориальных данных: "Unknown", "N/A", "Other". Применяется когда сам факт пропуска может быть информативен (MNAR).
- Статистическое заполнение - замена пропуска на центральную тенденцию (среднее или медиану) или наиболее частое значение (моду). Сохраняет общее распределение данных, но не учитывает взаимосвязи между переменными и занижает дисперсию. Может создавать "неестественные" пики в распределении.
- K-ближайших соседей (K-Nearest Neighbors Imputation) - для объекта с пропуском находятся k наиболее похожих объектов (по другим признакам). Пропущенное значение заполняется средним (или модой) значений этих k соседей.
- Множественное заполнение (Multiple Imputation by Chained Equations - MICE) - заполнение на основе интерполяции.

<h4>Сложности при очистке временных рядов</h4>

Очистка временных рядов (Time Series) — это особая задача со своими уникальными вызовами:
- Временная зависимость (Temporal Dependence) - данные не являются независимыми. 
- Сезонность и тренды (Seasonality and Trends) - данные содержат регулярные (сезонные) и долгосрочные (трендовые) компоненты.
- Следствие: Выброс нужно определять не относительно общего среднего, а относительно ожидаемого значения в эту конкретную точку времени с учетом тренда и сезонности.
- Неравномерная частота и пропуски (Irregular Frequency and Missingness) - данные могут поступать с разной частотой (например, сенсорные данные) или иметь неравномерно распределенные пропуски.
- Сложные методы импутации - вместо среднего используется интерполяция, заполнение последним известным значением (forward fill), или более сложные методы.
- Обнаружение аномалий в режиме реального времени - для потоковых данных (например, с IoT-датчиков) нужно обнаруживать аномалии "на лету".
- Согласованность временных меток (Timestamp Alignment) - при объединении нескольких временных рядов из разных источников (например, с разных серверов или датчиков) временные метки могут быть не синхронизированы.

<h4>Процедура очистки данных</h4>

Процедура очистки данных в Big Data обычно состоит из нескольких этапов, которые могут повторяться итеративно. Вот общий подход:
1. Обработка пропущенных значений (Missing Data) - удаление или импутация
2. Обработка выбросов (Outliers) - обнаружение и обработка (удаление или преобразование) выбросов
3. Стандартизация и нормализация - приведение данных к единому формату
4. Дедубликация (Deduplication) - поиск и удаление дубликатов
5. Валидация данных (Data Validation) - проверка данных на соответствие заданным правилам

<h4>Обнаружение и обработка искаженных данных</h4>

Искажённые данные (corrupted data) — это данные, которые были повреждены в процессе сбора, передачи или хранения. Это битые файлы, записи с неверной структурой, данные с ошибками кодирования.

Обнаружение искажённых данных:
- Валидация формата - проверка соответствия ожидаемому формату (например, JSON, XML, CSV). Например, в CSV проверить, что каждая строка имеет одинаковое количество столбцов.
- Использование схемы (Schema Validation) -  в Spark при чтении данных можно задать схему и обрабатывать ошибки (например, режим PERMISSIVE с обработкой плохих записей).
- Контрольные суммы (Checksums) - проверка контрольных сумм (например, MD5, SHA) для обнаружения повреждений при передаче.
- Мониторинг целостности данных - использование инструментов мониторинга (например, Apache Atlas) для отслеживания lineage и качества данных.

Обработка искажённых данных:
- Изоляция (Quarantine) - искажённые данные перемещаются в отдельное место (например, в папку "bad_records") для дальнейшего анализа и ручной обработки.
- Восстановление (Recovery) - если возможно, восстановить данные из резервных копий, попытаться извлечь корректные части.
- Игнорирование (Dropping) - если данные не критичны и их невозможно восстановить, их можно удалить. Но это должно быть осознанным решением.
- Репортинг и оповещение (Reporting and Alerting) - настроить оповещения о появлении искажённых данных, чтобы быстро реагировать.
- Использование отказоустойчивых форматов - использование форматов, устойчивых к повреждениям (например, Parquet, Avro) с встроенными механизмами проверки целостности.

<h3>4. Мониторинг качества данных</h3>

Мониторинг качества данных — это процесс непрерывного отслеживания и управления качеством информации, используемой в организации. Этот процесс важен для поддержания высокого уровня точности, актуальности и надежности данных, что критически важно для аналитических решений, операционной эффективности и стратегического планирования.

Цели мониторинга качества данных:
- Обнаружение проблем - своевременное выявление и устранение ошибок и несоответствий в данных.
- Повышение доверия - укрепление доверия пользователей к данным путем обеспечения их качества и надежности.
- Оптимизация решений - поддержание высокого уровня качества данных для обеспечения точности аналитических решений и бизнес-операций.

Основные компоненты системы мониторинга:
- Определение показателей качества данных (KPIs) - установление количественных показателей для оценки качества данных, таких как точность, полнота, консистентность, актуальность.
- Разработка правил и порогов - создание набора правил и пороговых значений для каждого KPI, при превышении которых инициируются действия по улучшению качества.
- Автоматизированный сбор и анализ данных - использование программных средств для автоматического сбора метрик качества и их анализа на соответствие установленным стандартам.
- Реагирование на проблемы - механизмы уведомления о проблемах качества данных и процессы их устранения.

Зачем нужно отслеживать изменение качества данных:
- Снижение бизнес-рисков - принятие решений на основе некачественных данных ведет к финансовым потерям, неверным стратегиям и репутационным издержкам. Например, некорректные данные о продажах могут привести к неверному прогнозированию спроса и упущенной выгоде.
- Повышение доверия к данным и AI/ML - если стейкхолдеры (аналитики, data scientists, бизнес-пользователи) не доверяют данным, они не будут их использовать. Мониторинг и видимость качества данных строят это доверие. Качественные данные — основа для эффективных ML-моделей.
- Эффективность и скорость разработки - когда инженеры и аналитики уверены в качестве входных данных, они тратят меньше времени на их "очистку" и отладку и больше — на создание ценности.
- Раннее обнаружение проблем - многие проблемы в данных носят "ползучий" характер (например, постепенное увеличение числа пропусков в колонке). Мониторинг позволяет поймать такие аномалии до того, как они нанесут ущерб.

<h4>Инструменты мониторинга</h4>

Инструменты можно разделить на несколько категорий. Выбор зависит от стека технологий, бюджета и сложности требований.

Специализированные Frameworks и Open-Source решения:
- Great Expectations (GX): Позволяет декларативно описывать "ожидания" (expectations) к вашим данным (например, expect_column_values_to_not_be_null). Интегрируется с Pandas, Spark, SQL-базами. Результаты можно визуализировать в Data Docs.
- Soda Core: Использует свой язык проверок (SodaCL), который может быть проще для не-программистов. Легко интегрируется с любым инструментом через YAML-конфиги.
- Deequ (от AWS): Основан на Apache Spark, написан на Scala. Идеален для проверки огромных объемов данных в распределенных кластерах. Использует внутренний язык, похожий на Great Expectations.

Плюсы: Гибкость, бесплатность, глубокие интеграции.
Минусы: Требуют разработки и поддержки кода/конфигов.

Встроенные возможности платформ данных:
- dbt (data build tool): Имеет встроенную систему тестов. Вы можете описывать простые тесты (not_null, unique, accepted_values) прямо в YAML-файлах рядом с моделями, а также создавать кастомные SQL-тесты. Отлично подходит для мониторинга данных внутри трансформационного слоя.
- Apache Atlas / Amundsen (Data Catalogs): Эти инструменты больше про метаданные и обнаружение, но они могут отслеживать профилирование данных и lineage, что косвенно помогает с качеством.

Плюсы: "Из коробки" для пользователей этих платформ, тесная интеграция с пайплайнами.
Минусы: Ограниченный функционал по сравнению со специализированными фреймворками.

Коробочные и Cloud-решения:
- Monte Carlo, Datafold, Anomalo: Современные Data Observability-платформы. Они не только проверяют заведомо известные правила, но и используют ML для обнаружения аномалий (например, "число строк в таблице внезапно упало на 30% без явной причины").
- Cloud-решения (AWS DataBrew, Google Cloud Dataplex, Azure Purview): Предлагают встроенные функции профилирования и проверки качества данных, тесно интегрированные с экосистемой своего облака.

Плюсы: Минимум кода, мощные возможности (особенно ML для аномалий), удобные дашборды.
Минусы: Стоимость.

<h4>Автоматизация мониторинга</h4>

Алгоритм автоматизации:
1. Интеграция в пайплайны данных (Data Pipelines): Проверки качества должны - это шаг в ETL/ELT-процессе. В пайплайне запускаются скрипты Great Expectations или Soda Core. В dbt тесты запускаются автоматически при выполнении dbt run или dbt test.
2. Настройка оповещений (Alerting): Настройка отправки алертов в Slack, Microsoft Teams, Email, PagerDuty (для критичных инцидентов). сегментирование плертов по степени важности.
3. Визуализация и отчетность (Dashboards): Создание дашборда (например, в Grafana, Tableau или используя встроенные Data Docs в GX), который показывает: общее "здоровье" данных (Health Score), историю срабатываний проверок, количество инцидентов за период. Это помогает видеть общую картину и тренды.
4. Создание процессов обработки инцидентов: Для каждой проверки должна быть инструкция: "Если сработал алерт X, первым делом проверить источник Y, затем посмотреть на дашборд Z". Если проблема не решена за N времени, алерт эскалируется на старшего инженера или тимлида. В критичных случаях пайплайн должен не просто слать алерт, а полностью останавливаться, чтобы не испортить витрины и ML-модели невалидными данными.

<h4>Индикаторы и метрики качества данных</h4>

Основные категории и примеры метрик:
1. Полнота (Completeness) - определяет, насколько полны данные. Отсутствующие данные — одна из самых частых проблем. Метрики:
  - Доля пропусков (NULL) в столбце
  - Количество пустых строк
  - Доля заполненных записей в ключевых столбцах
2. Корректность/Точность (Accuracy) - определяет, насколько данные соответствуют реальному положению дел. Это одна из самых сложных для проверки метрик, так как часто требует сверки с эталонным источником. Метрики:
  - Соответствие формату (например, email, номер телефона).
  - Соответствие допустимому диапазону значений (age between 0 and 120).
  - Принадлежность к справочнику (например, country_code должен быть в списке кодов стран).
  - Количество записей, не прошедших бизнес-валидацию.
3. Непротиворечивость (Consistency) - проверяет, что данные не противоречат друг другу в рамках одной системы или между разными системами. Метрики:
  - Согласованность дублирующихся данных в разных источниках (например, баланс счета в OLTP и в витрине данных).
  - Проверка ссылочной целостности (например, у каждого order_id в таблице заказов должен существовать соответствующий user_id в таблице пользователей).
  - Согласованность вычислений (например, total_price = quantity * unit_price).
4. Уникальность (Uniqueness) - определяет отсутствие дубликатов. Метрики:
  - Количество дубликатов по ключевому полю.
  - Количество уникальных значений в столбце, где они должны быть уникальны.
5. Своевременность (Timeliness) - определяет, насколько данные актуальны на момент использования. Метрики:
  - Время задержки данных (latency): разница между временем появления данных в источнике и временем их доступности в целевом хранилище.
  - Фактическое время выполнения пайплайна vs SLA (Service Level Agreement).
  - Возраст самых свежих данных в таблице.
6. Валидность (Validity) - проверяет, что данные соответствуют определенному формату, структуре или типу. Метрики:
  - Соответствие схеме (количество записей, не соответствующих ожидаемой схеме Avro/Protobuf/JSON).
  - Корректность типов данных (например, в колонке amount нет текстовых значений).

<h4>Методы обнаружения аномалий в данных</h4>

Основные методы:
- Правила и пороговые значения (Rule-Based / Thresholding)
- Статистические методы - используют исторические данные для выявления отклонений.
- Машинное обучение - более сложные методы, которые могут обнаруживать неизвестные ранее аномалии без явных правил.
- Профилирование данных и сравнение схем - регулярный автоматический анализ метаданных.

Можно настраивать различные пороги оповещений, например:
1. Статические пороги (Static Thresholds) - жестко заданные значения, основанные на бизнес-требованиях или здравом смысле. Применяется для обнаружения абсолютно некорректных значений, где нет "серой зоны". Примеры:
  - Допустимое количество NULL в колонке user_id: 0 (никогда не должно быть пустым).
  - Допустимый диапазон значения age: от 0 до 120.
  - Допустимое количество дубликатов transaction_id: 0.
2. Статистические пороги (Statistical Thresholds) - пороги, основанные на исторических данных. Они адаптивны и более гибки. Применяются для метрик, которые естественным образом колеблются (объем данных, количество уникальных пользователей и т.д.). Примеры:
  - Предупреждение, если объем данных сегодня упал на 20% и более относительно среднего объема за последние 7 дней.
  - Предупреждение, если значение метрики вышло за пределы 3 сигм от скользящего среднего.
  - Предупреждение, если время выполнения пайплайна превысило 95-й процентиль за последний месяц.
3. Референсные пороги (Reference Thresholds) - сравнение с эталонным набором данных или с другим источником истины. Применяются для проверки согласованности между системами.

<h4>Анализ тенденций изменения качества данных</h4>

Это проактивный подход, который позволяет предсказать проблемы до того, как они достигнут критического порога. Что отслеживать:
- Тренд количества пропусков: Постепенное увеличение NULL в определенной колонке может сигнализировать о проблеме в источнике (например, сломался скрипт на стороне мобильного приложения).
- Тренд объема данных: Резкий рост может указывать на дублирование. Постепенное падение — на потерю данных или проблемы с доставкой.
- Тренд уникальности: Уменьшение количества уникальных значений в колонке, где оно должно быть стабильным.
- Тренд времени выполнения пайплайнов: Постепенное увеличение времени может сигнализировать о неоптимальном коде или нехватке ресурсов.

Данные тренды можно визуализировать на дашбордах и отслеживать в реальном времени

<h4>Стратегия мониторинга качества данных и применение машинного обучения</h4>

Стратегия — это фундамент. Без нее мониторинг превратится в хаотичную проверку случайных показателей.
1. Определение критичных данных: Составить список ключевых таблиц и атрибутов.
2. Выбор метрик качества для каждого объекта: определить каким образом будет измеряться качество данных
3. Установка пороговых значений (Thresholds): Определите, что является "зеленым", "желтым" и "красным" уровнем проблемы.
4. Определение процессов и ответственных: Создание оповещений и действий, направленных на решение конкретного типа проблемы

Традиционный мониторинг работает по принципу "что упало — то упало". ML позволяет перейти к проактивному и предиктивному подходу, предсказывая проблемы до их возникновения.

Основные применения ML в мониторинге качества:
1. Обнаружение аномалий (Anomaly Detection): Это самое распространенное применение. Вместо задания жестких порогов ML-модель учится на исторических данных, что является "нормальным" поведением, и флагует отклонения. Методы: Isolation Forest, Local Outlier Factor (LOF).
2. Прогнозирование "здоровья" данных (Data Health Score): ML-модель может агрегировать множество метрик и предсказывать общий индекс качества данных на будущие периоды. Это позволяет увидеть тренд на ухудшение и принять меры до того, как индекс уйдет в "красную зону".
3. Автоматическая настройка порогов (Adaptive Thresholds): Вместо того чтобы вручную выставлять статические пороги, ML-модель динамически подстраивает их, учитывая сезонность, тренды и корреляции
4. Определение наиболее влияющих факторов (Root Cause Prediction): Когда качество падает, модель может проанализировать тысячи связанных метрик и предложить наиболее вероятную причину.

<h4>Интеграция с другими системами и глубокий анализ</h4>

Мониторинг не должен быть "вещью в себе". Его максимальная ценность раскрывается в экосистеме инструментов компании.

Ключевые интеграции:
- С системой инцидент-менеджмента (PagerDuty, Opsgenie, Jira Service Desk): Автоматическое создание инцидентов при критических падениях качества.
- С каталогом данных (Data Catalog) и Data Discovery инструментами (Alation, DataHub, Amundsen): Метаданные из мониторинга (последний статус проверки, актуальные метрики) передаются в каталог данных.
- С Data Lineage (трассировка данных): Если сломался источник, lineage-граф мгновенно покажет, какие витрины, дашборды и ML-модели затронуты. При срабатывании алерта система мониторинга автоматически дергает API lineage-системы.
- С Orchestration-инструментами (Apache Airflow, Dagster, Prefect): Встраивание проверки качества прямо в пайплайны:
- - Проверка на входе (Pre-check): Задача в Airflow запускает скрипт Great Expectations для проверки "сырых" данных. Если проверка не пройдена — пайплайн не запускается.
- - Проверка на выходе (Post-check): После выполнения трансформаций запускается проверка готовых витрин. Если качество низкое, пайплайн не публикует данные и откатывает транзакцию.
- С BI-системами (Tableau, Power BI, Looker): Предупреждение бизнес-пользователей о проблемах прямо в интерфейсе дашборда.
- С CI/CD (GitLab CI, GitHub Actions): При создании Pull Request для изменения ETL-кода или dbt-модели, пайплайн CI/CD автоматически запускает тесты качества на тестовых данных и блокирует мерж, если новые изменения ухудшают качество.

Процесс глубокого анализа:
1. Быстрая классификация: Определить область проблемы (источник, пайплайн или инфраструктура в целом)
2. Сбор контекста (Data Context): Использовать графы lineage, чтобы отследить поток данных от источника до конечной витрины и найти "точку разрыва", проверить логи выполнения пайплайнов (Airflow и др.) и метрики мониторинга инфраструктуры (CPU, память, диск).
3. Анализ "до и после" (Before/After Analysis): Сравнение снимков данных (Data Profiling), сравнение их распределений, количества строк, уникальных значения. Часто это сразу указывает на природу проблемы (например, появился новый NULL- статус).
4. Использование DQE (Data Quality Dimensions) для фокусировки: анализ полноты, своевременности и непротиворечивости.
5. Формализация и документирование (Blameless Post-Mortem): Задокументировать весь процесс от причины появления инцидента до действий для его устранения.