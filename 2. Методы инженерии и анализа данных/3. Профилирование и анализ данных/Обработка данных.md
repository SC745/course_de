<h2>Обработка данных</h2>

<h3>1. Профилирование данных</h3>

Профилирование данных — это процесс детального анализа исходных данных перед их использованием в аналитических целях. Этот процесс позволяет аналитикам данных оценить качество данных, обнаружить потенциальные проблемы и понять структуру данных.

Цели:
- Понимание структуры данных - идентификация основных атрибутов, типов данных и связей между различными элементами данных.
- Оценка качества данных - выявление проблем с данными, таких как пропущенные значения, дубликаты, неконсистентные или аномальные данные.
- Обнаружение паттернов - анализ распределения данных, частоты значений и выявление возможных взаимосвязей между различными полями данных.
- Подготовка к интеграции данных - профилирование данных помогает оценить совместимость данных из разных источников и разработать правила консолидации.
- Снижение рисков - профилирование данных позволяет выявить потенциальные проблемы до того, как данные будут использованы в критически важных процессах, что снижает риск принятия неправильных решений на основе некорректных данных.

Основные методы:
- Статистический анализ - расчет средних значений, медиан, стандартных отклонений, минимальных и максимальных значений для количественных данных, анализ частоты встречаемости различных значений для качественных данных.
- Анализ уникальности и дубликатов - поиск уникальных записей и идентификация дубликатов, что важно для разработки процедур по очистке данных.
- Проверка правил валидации - применение предварительно определенных правил для проверки соответствия данных требуемым форматам и ограничениям.
- Анализ отсутствующих данных - определение полей с пропущенными значениями и оценка их влияния на анализ.
- Корреляционный анализ - исследование взаимосвязей между различными полями данных для выявления потенциальных зависимостей.

Инструменты:
- Специализированные инструменты профилирования - такие как Informatica Data Quality, Talend, DataCleaner предлагают готовые решения для анализа и оценки качества данных.
- SQL и скриптовые языки - Написание пользовательских запросов и скриптов для специфического анализа данных, используя SQL, Python (pandas), R.
- BI инструменты - Использование инструментов бизнес-аналитики, таких как Tableau или Power BI, для визуализации и анализа данных.

<h4>Типы аномалий в данных</h4>

Аномалии (или отклонения) — это наблюдения, которые значительно отличаются от остальных данных и могут искажать анализ и модели. Их делят на три основных типа:
- Выбросы - точки данных, которые значительно отклоняются от общего распределения:
  - Глобальные - значение аномально по отношению ко всему набору данных.
  - Контекстные - значение аномально в определенном контексте.
  - Коллективные выбросы - группа точек данных, которые вместе являются аномальными.
- Пропуски - отсутствующие значения в наборе данных:
  - MCAR (Missing Completely At Random): Пропуск никак не связан с другими данными (например, случайный сбой датчика).
  - MAR (Missing At Random): Пропуск связан с другими наблюдаемыми переменными. Например, вероятность пропуска зарплаты выше у безработных (значение "безработный" есть в данных).
  - MNAR (Missing Not At Random): Пропуск связан с самим пропущенным значением. Например, люди с высокой зарплатой чаще отказываются её указывать.
- Дубликаты - Полностью или почти полностью повторяющиеся записи.
  - Точные - полностью идентичные записи
  - Неполные - дубликаты по одному или нескольким столбцам
  - Неточные - почти идентичные записи


<h4>Базовые статистические метрики</h4>

Для количественных данных:
- Меры центральной тенденции - среднее, медиана, мода
- Меры изменчивости: минимум / максимум, Дисперсия и стандартное отклонение, 25-й и 75-й процентили

Для качественных данных:
- Количество уникальных значений
- Частота категорий

Общие показатели:
- Количество записей
- Количество пропусков
- Доля пропусков

<h4>Обнаружение пропущенных значений</h4>

Методы обнаружения пропущенных значений:
- Визуальный осмотр с помощью тепловых карт -  визуализация, где цветом кодируется наличие пропуска (например, белый цвет для NaN). Позволяет быстро увидеть паттерны пропусков.
- Статистический анализ - `df.isnull().sum()` для подсчета пропусков в каждом столбце и `df.isnull().mean()` для подсчета доли пропусков.
- Автоматизированные инструменты - библиотеки pandas-profiling (теперь ydata-profiling) автоматически генерируют отчеты с количеством и процентом пропусков.

<h4>Обнаружение дубликатов</h4>

Методы обнаружения различных видов дубликатов:
- Точные дубликаты - поиск полностью идентичных строк во всем наборе данных с помощью `df.duplicated()`.
- Неполные дубликаты - поиск дубликатов по одному или нескольким столбцам, которые должны быть уникальными (например, user_id, email). Пример: `df.duplicated(subset=['email'])`
- Неточные дубликаты - поиск записей, которые почти идентичны, но могут содержать опечатки или небольшие различия с помощью алгоритмов нечеткого сравнения строк, такие как расстояние Левенштейна (Levenshtein distance).

<h4>Оценка полноты данных</h4>

Полнота — это степень отсутствия пропущенных (null, пустых) значений в наборе данных. Низкая полнота напрямую влияет на достоверность любых анализов и моделей, построенных на этих данных.

Основные методы оценки полноты данных:
- Подсчет пропусков (Null Count) - абсолютное количество пропущенных значений для каждого столбца (атрибута).
- Процент заполненности (Completeness Rate) - доля непустых значений в столбце.
- Анализ на уровне записей (Record-Level Completeness) - определяется, сколько записей являются "полностью заполненными" (все обязательные поля присутствуют).
- Анализ "скрытых" пропусков (Analysis of "Hidden" Nulls) - данные могут быть технически не NULL, но по смыслу являться пропусками. Например, строки-заглушки: "N/A", "Unknown", "Not Provided", "0" (если 0 не является валидным значением), пустая строка "". Необходимо составить словарь таких "заглушек" и искать их в данных теми же методами, что и обычные NULL.
- Оценка против бизнес-правил (Completeness vs. Business Rules) - полнота оценивается не абстрактно, а в контексте бизнес-требований. Некоторые поля могут быть обязательными только при определенных условиях.


<h3>2. Картирование данных</h3>

Картирование данных — это процесс создания соответствий между исходными и целевыми данными. Это ключевой шаг в процессе интеграции, миграции, трансформации и подготовки данных, который обеспечивает правильное и эффективное использование данных в аналитических и бизнес-процессах. Для аналитиков данных картирование играет критическую роль в обеспечении точности и целостности аналитических данных. Оно помогает понять структуру исходных данных, обнаружить и устранить несоответствия и проблемы качества, а также гарантировать, что данные адекватно представлены для анализа и отчетности. Эффективное картирование данных требует тесного сотрудничества между аналитиками и инженерами данных. Инженеры обеспечивают техническую реализацию процессов ETL (Extract, Transform, Load), в то время как аналитики определяют бизнес-правила и требования к данным.

Основные этапы картирования данных
- Анализ исходных данных - понимание структуры, типов данных и качества исходных данных.
- Определение целевой структуры - выявление структуры и форматов целевых данных в соответствии с бизнес-требованиями и аналитическими задачами.
- Создание карты соответствий - разработка детального описания того, как каждое поле исходных данных соответствует полю в целевой структуре, включая трансформации и правила обработки.
- Реализация и проверка - техническая реализация картирования с использованием инструментов ETL и последующая проверка корректности передачи данных.

<h3>3. Очистка и подготовка данных</h3>

Очистка и подготовка данных — это критически важные этапы в процессе работы с данными, которые значительно влияют на качество анализа и точность выводов. Этот процесс включает в себя серию действий по трансформации "сырых" данных в формат, пригодный для анализа.

Основные этапы очистки и подготовки данных:
- Оценка качества данных - идентификация и анализ проблем в данных, таких как пропущенные значения, дубликаты, ошибочные данные, аномалии.
- Очистка данных - удаление, замена средним/медианой, интерполяция на основе моделей пустых значений, коррекция явных ошибок и фильтрация аномальных значений, определение и удаление дублирующих записей.
- Трансформация данных - приведение данных к единому формату и масштабу, их агрегация, генерация новых атрибутов из существующих данных для улучшения аналитических моделей.
- Интеграция данных - слияние данных из различных источников для создания комплексного представления информации, обеспечение целостности данных путем сопоставления ключевых атрибутов между разными наборами данных.
- Повышение качества данных - проверка данных на соответствие предварительно определенным правилам и стандартам (валидация) и дополнение данных информацией из внешних источников (обогащение).

<h4>Заполнение пропущенных данных</h4>

Методы:
- Удаление (Deletion) - Удаление строк в зависимости от доли пропусков. Применяется только если пропуски не зависят от других значений (MCAR) и их мало.
- Заполнение константами - замена пропуска на заранее заданное значение. Для числовых данных: 0, -1. Для категориальных данных: "Unknown", "N/A", "Other". Применяется когда сам факт пропуска может быть информативен (MNAR).
- Статистическое заполнение - замена пропуска на центральную тенденцию (среднее или медиану) или наиболее частое значение (моду). Сохраняет общее распределение данных, но не учитывает взаимосвязи между переменными и занижает дисперсию. Может создавать "неестественные" пики в распределении.
- K-ближайших соседей (K-Nearest Neighbors Imputation) - для объекта с пропуском находятся k наиболее похожих объектов (по другим признакам). Пропущенное значение заполняется средним (или модой) значений этих k соседей.
- Множественное заполнение (Multiple Imputation by Chained Equations - MICE) - заполнение на основе интерполяции.


<h3>4. Мониторинг качества данных</h3>

Мониторинг качества данных — это процесс непрерывного отслеживания и управления качеством информации, используемой в организации. Этот процесс важен для поддержания высокого уровня точности, актуальности и надежности данных, что критически важно для аналитических решений, операционной эффективности и стратегического планирования.

Цели мониторинга качества данных:
- Обнаружение проблем - своевременное выявление и устранение ошибок и несоответствий в данных.
- Повышение доверия - укрепление доверия пользователей к данным путем обеспечения их качества и надежности.
- Оптимизация решений - поддержание высокого уровня качества данных для обеспечения точности аналитических решений и бизнес-операций.

Основные компоненты системы мониторинга:
- Определение показателей качества данных (KPIs) - установление количественных показателей для оценки качества данных, таких как точность, полнота, консистентность, актуальность.
- Разработка правил и порогов - создание набора правил и пороговых значений для каждого KPI, при превышении которых инициируются действия по улучшению качества.
- Автоматизированный сбор и анализ данных - использование программных средств для автоматического сбора метрик качества и их анализа на соответствие установленным стандартам.
- Реагирование на проблемы - механизмы уведомления о проблемах качества данных и процессы их устранения.

Инструменты мониторинга:
