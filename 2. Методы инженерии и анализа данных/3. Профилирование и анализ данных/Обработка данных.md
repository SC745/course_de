<h2>Обработка данных</h2>

<h3>1. Профилирование данных</h3>

Профилирование данных — это процесс детального анализа исходных данных перед их использованием в аналитических целях. Этот процесс позволяет аналитикам данных оценить качество данных, обнаружить потенциальные проблемы и понять структуру данных.

Цели:
- Понимание структуры данных - идентификация основных атрибутов, типов данных и связей между различными элементами данных.
- Оценка качества данных - выявление проблем с данными, таких как пропущенные значения, дубликаты, неконсистентные или аномальные данные.
- Обнаружение паттернов - анализ распределения данных, частоты значений и выявление возможных взаимосвязей между различными полями данных.
- Подготовка к интеграции данных - профилирование данных помогает оценить совместимость данных из разных источников и разработать правила консолидации.
- Снижение рисков - профилирование данных позволяет выявить потенциальные проблемы до того, как данные будут использованы в критически важных процессах, что снижает риск принятия неправильных решений на основе некорректных данных.

Основные методы:
- Статистический анализ - расчет средних значений, медиан, стандартных отклонений, минимальных и максимальных значений для количественных данных, анализ частоты встречаемости различных значений для качественных данных.
- Анализ уникальности и дубликатов - поиск уникальных записей и идентификация дубликатов, что важно для разработки процедур по очистке данных.
- Проверка правил валидации - применение предварительно определенных правил для проверки соответствия данных требуемым форматам и ограничениям.
- Анализ отсутствующих данных - определение полей с пропущенными значениями и оценка их влияния на анализ.
- Корреляционный анализ - исследование взаимосвязей между различными полями данных для выявления потенциальных зависимостей.

Инструменты:
- Специализированные инструменты профилирования - такие как Informatica Data Quality, Talend, DataCleaner предлагают готовые решения для анализа и оценки качества данных.
- SQL и скриптовые языки - Написание пользовательских запросов и скриптов для специфического анализа данных, используя SQL, Python (pandas), R.
- BI инструменты - Использование инструментов бизнес-аналитики, таких как Tableau или Power BI, для визуализации и анализа данных.

<h4>Типы аномалий в данных</h4>

Аномалии (или отклонения) — это наблюдения, которые значительно отличаются от остальных данных и могут искажать анализ и модели. Их делят на три основных типа:
- Выбросы - точки данных, которые значительно отклоняются от общего распределения:
  - Глобальные - значение аномально по отношению ко всему набору данных.
  - Контекстные - значение аномально в определенном контексте.
  - Коллективные выбросы - группа точек данных, которые вместе являются аномальными.
- Пропуски - отсутствующие значения в наборе данных:
  - MCAR (Missing Completely At Random): Пропуск никак не связан с другими данными (например, случайный сбой датчика).
  - MAR (Missing At Random): Пропуск связан с другими наблюдаемыми переменными. Например, вероятность пропуска зарплаты выше у безработных (значение "безработный" есть в данных).
  - MNAR (Missing Not At Random): Пропуск связан с самим пропущенным значением. Например, люди с высокой зарплатой чаще отказываются её указывать.
- Дубликаты - Полностью или почти полностью повторяющиеся записи.
  - Точные - полностью идентичные записи
  - Неполные - дубликаты по одному или нескольким столбцам
  - Неточные - почти идентичные записи


<h4>Базовые статистические метрики</h4>

Для количественных данных:
- Меры центральной тенденции - среднее, медиана, мода
- Меры изменчивости: минимум / максимум, Дисперсия и стандартное отклонение, 25-й и 75-й процентили

Для качественных данных:
- Количество уникальных значений
- Частота категорий

Общие показатели:
- Количество записей
- Количество пропусков
- Доля пропусков

<h4>Обнаружение пропущенных значений</h4>

Методы обнаружения пропущенных значений:
- Визуальный осмотр с помощью тепловых карт -  визуализация, где цветом кодируется наличие пропуска (например, белый цвет для NaN). Позволяет быстро увидеть паттерны пропусков.
- Статистический анализ - `df.isnull().sum()` для подсчета пропусков в каждом столбце и `df.isnull().mean()` для подсчета доли пропусков.
- Автоматизированные инструменты - библиотеки pandas-profiling (теперь ydata-profiling) автоматически генерируют отчеты с количеством и процентом пропусков.

<h4>Обнаружение дубликатов</h4>

Методы обнаружения различных видов дубликатов:
- Точные дубликаты - поиск полностью идентичных строк во всем наборе данных с помощью `df.duplicated()`.
- Неполные дубликаты - поиск дубликатов по одному или нескольким столбцам, которые должны быть уникальными (например, user_id, email). Пример: `df.duplicated(subset=['email'])`
- Неточные дубликаты - поиск записей, которые почти идентичны, но могут содержать опечатки или небольшие различия с помощью алгоритмов нечеткого сравнения строк, такие как расстояние Левенштейна (Levenshtein distance).

<h4>Оценка полноты данных</h4>

Полнота — это степень отсутствия пропущенных (null, пустых) значений в наборе данных. Низкая полнота напрямую влияет на достоверность любых анализов и моделей, построенных на этих данных.

Основные методы оценки полноты данных:
- Подсчет пропусков (Null Count) - абсолютное количество пропущенных значений для каждого столбца (атрибута).
- Процент заполненности (Completeness Rate) - доля непустых значений в столбце.
- Анализ на уровне записей (Record-Level Completeness) - определяется, сколько записей являются "полностью заполненными" (все обязательные поля присутствуют).
- Анализ "скрытых" пропусков (Analysis of "Hidden" Nulls) - данные могут быть технически не NULL, но по смыслу являться пропусками. Например, строки-заглушки: "N/A", "Unknown", "Not Provided", "0" (если 0 не является валидным значением), пустая строка "". Необходимо составить словарь таких "заглушек" и искать их в данных теми же методами, что и обычные NULL.
- Оценка против бизнес-правил (Completeness vs. Business Rules) - полнота оценивается не абстрактно, а в контексте бизнес-требований. Некоторые поля могут быть обязательными только при определенных условиях.


<h3>2. Картирование данных</h3>

Картирование данных — это процесс создания соответствий между исходными и целевыми данными. Это ключевой шаг в процессе интеграции, миграции, трансформации и подготовки данных, который обеспечивает правильное и эффективное использование данных в аналитических и бизнес-процессах. Для аналитиков данных картирование играет критическую роль в обеспечении точности и целостности аналитических данных. Оно помогает понять структуру исходных данных, обнаружить и устранить несоответствия и проблемы качества, а также гарантировать, что данные адекватно представлены для анализа и отчетности. Эффективное картирование данных требует тесного сотрудничества между аналитиками и инженерами данных. Инженеры обеспечивают техническую реализацию процессов ETL (Extract, Transform, Load), в то время как аналитики определяют бизнес-правила и требования к данным.

Основные этапы картирования данных
- Анализ исходных данных - понимание структуры, типов данных и качества исходных данных.
- Определение целевой структуры - выявление структуры и форматов целевых данных в соответствии с бизнес-требованиями и аналитическими задачами.
- Создание карты соответствий - разработка детального описания того, как каждое поле исходных данных соответствует полю в целевой структуре, включая трансформации и правила обработки.
- Реализация и проверка - техническая реализация картирования с использованием инструментов ETL и последующая проверка корректности передачи данных.

<h3>3. Очистка и подготовка данных</h3>

Очистка и подготовка данных — это критически важные этапы в процессе работы с данными, которые значительно влияют на качество анализа и точность выводов. Этот процесс включает в себя серию действий по трансформации "сырых" данных в формат, пригодный для анализа.

Основные этапы очистки и подготовки данных:
- Оценка качества данных - идентификация и анализ проблем в данных, таких как пропущенные значения, дубликаты, ошибочные данные, аномалии.
- Очистка данных - удаление, замена средним/медианой, интерполяция на основе моделей пустых значений, коррекция явных ошибок и фильтрация аномальных значений, определение и удаление дублирующих записей.
- Трансформация данных - приведение данных к единому формату и масштабу, их агрегация, генерация новых атрибутов из существующих данных для улучшения аналитических моделей.
- Интеграция данных - слияние данных из различных источников для создания комплексного представления информации, обеспечение целостности данных путем сопоставления ключевых атрибутов между разными наборами данных.
- Повышение качества данных - проверка данных на соответствие предварительно определенным правилам и стандартам (валидация) и дополнение данных информацией из внешних источников (обогащение).

Основные инструменты для очистки данных:
- Python:
  - Pandas: Основная библиотека для работы с данными в памяти. Отлично подходит для прототипирования и очистки небольших (объемом до оперативной памяти) датасетов. Для Big Data часто используется на этапе исследования и создания скриптов для отдельных узлов.
  - PySpark (Spark с Python API): Это главный инструмент для очистки в распределенной среде. Он позволяет использовать знакомый синтаксис Python/Pandas, но выполняет операции на кластере компьютеров, обрабатывая терабайты и петабайты данных.
  - Dask: Еще одна библиотека для параллельных вычислений в Python, которая имитирует API Pandas, но масштабируется на кластеры.
  - Специализированные библиотеки: great_expectations (для проверки качества данных), scikit-learn (для импутации, кодирования), numpy.
- SQL:
  - Distributed SQL Engines: Hive, Impala, PrestoDB, Spark SQL. Позволяют выполнять операции очистки (фильтрация, агрегация, джойны) прямо в распределенной системе, используя знакомый синтаксис SQL. Часто это самый эффективный способ начать очистку.
- Apache Spark: Флагманский инструмент. Предоставляет единую платформу для пакетной обработки, стриминга и сложных ETL-пайплайнов. Его основная сила — в распределенных вычислениях в памяти.
- Apache Flink: Часто используется для обработки данных в реальном времени (streaming). Имеет мощный API для работы с временными рядами и обработки событий.
- Традиционные ETL-инструменты: Informatica, Talend, IBM DataStage. Имеют визуальные интерфейсы для построения пайплайнов очистки и могут интегрироваться с Hadoop/Spark.

<h4>Методы очистки данных</h4>

Ручные:
- Профилирование данных (Data Profiling) - Автоматизированный анализ датасета для сбора статистики: типы данных, количество уникальных значений, частота, минимум/максимум, количество пропусков, распределение. Инструменты: pandas-profiling, great_expectations, встроенные функции в Spark (describe(), summary()).
- Визуальный осмотр и анализ (EDA - Exploratory Data Analysis) - построение гистограмм, боксплотов, scatter-plot'ов для визуального обнаружения выбросов, аномалий и странных распределений. Инструменты: Matplotlib, Seaborn, Plotly в Python.
- Создание правил и скриптов - Data engineer на основе доменных знаний и EDA пишет конкретный код для обработки: "заменить все значения 'N/A' на NaN", "удалить строки, где возраст > 120", "привести все названия городов к верхнему регистру".

Автоматические:
- Обнаружение и импутация пропусков (Missing Data Imputation) - замена на среднее, медиану, моду (в Spark/Pandas это делается одной строкой).
- Обнаружение и обработка выбросов (Outlier Detection) - правило трех сигм (Z-score), метод межквартильного размаха (IQR).
- Алгоритмы ML: Isolation Forest, Local Outlier Factor (LOF). Эффективны для многомерных данных.
- Стандартизация и нормализация - автоматические пайплайны: В scikit-learn или PySpark MLlib можно создать пайплайн, который автоматически применяет StandardScaler, MinMaxScaler ко всем числовым признакам.
- Дедубликация (Deduplication) - Spark имеет встроенные функции dropDuplicates() или более сложные методы на основе приблизительного сопоставления (например, MinHash для LSH - Locality-Sensitive Hashing).
- Валидация данных (Data Validation) - great_expectations позволяет декларативно описать ожидания от данных ("столбец A должен быть уникальным", "значения в столбце B должны быть между 0 и 100"). Эти правила затем автоматически проверяются при поступлении новых данных.

<h4>Заполнение пропущенных данных</h4>

Методы:
- Удаление (Deletion) - Удаление строк в зависимости от доли пропусков. Применяется только если пропуски не зависят от других значений (MCAR) и их мало.
- Заполнение константами - замена пропуска на заранее заданное значение. Для числовых данных: 0, -1. Для категориальных данных: "Unknown", "N/A", "Other". Применяется когда сам факт пропуска может быть информативен (MNAR).
- Статистическое заполнение - замена пропуска на центральную тенденцию (среднее или медиану) или наиболее частое значение (моду). Сохраняет общее распределение данных, но не учитывает взаимосвязи между переменными и занижает дисперсию. Может создавать "неестественные" пики в распределении.
- K-ближайших соседей (K-Nearest Neighbors Imputation) - для объекта с пропуском находятся k наиболее похожих объектов (по другим признакам). Пропущенное значение заполняется средним (или модой) значений этих k соседей.
- Множественное заполнение (Multiple Imputation by Chained Equations - MICE) - заполнение на основе интерполяции.

<h4>Сложности при очистке временных рядов</h4>

Очистка временных рядов (Time Series) — это особая задача со своими уникальными вызовами:
- Временная зависимость (Temporal Dependence) - данные не являются независимыми. 
- Сезонность и тренды (Seasonality and Trends) - данные содержат регулярные (сезонные) и долгосрочные (трендовые) компоненты.
- Следствие: Выброс нужно определять не относительно общего среднего, а относительно ожидаемого значения в эту конкретную точку времени с учетом тренда и сезонности.
- Неравномерная частота и пропуски (Irregular Frequency and Missingness) - данные могут поступать с разной частотой (например, сенсорные данные) или иметь неравномерно распределенные пропуски.
- Сложные методы импутации - вместо среднего используется интерполяция, заполнение последним известным значением (forward fill), или более сложные методы.
- Обнаружение аномалий в режиме реального времени - для потоковых данных (например, с IoT-датчиков) нужно обнаруживать аномалии "на лету".
- Согласованность временных меток (Timestamp Alignment) - при объединении нескольких временных рядов из разных источников (например, с разных серверов или датчиков) временные метки могут быть не синхронизированы.

<h4>Процедура очистки данных</h4>

Процедура очистки данных в Big Data обычно состоит из нескольких этапов, которые могут повторяться итеративно. Вот общий подход:
1. Обработка пропущенных значений (Missing Data) - удаление или импутация
2. Обработка выбросов (Outliers) - обнаружение и обработка (удаление или преобразование) выбросов
3. Стандартизация и нормализация - приведение данных к единому формату
4. Дедубликация (Deduplication) - поиск и удаление дубликатов
5. Валидация данных (Data Validation) - проверка данных на соответствие заданным правилам

<h4>Обнаружение и обработка искаженных данных</h4>

Искажённые данные (corrupted data) — это данные, которые были повреждены в процессе сбора, передачи или хранения. Это битые файлы, записи с неверной структурой, данные с ошибками кодирования.

Обнаружение искажённых данных:
- Валидация формата - проверка соответствия ожидаемому формату (например, JSON, XML, CSV). Например, в CSV проверить, что каждая строка имеет одинаковое количество столбцов.
- Использование схемы (Schema Validation) -  в Spark при чтении данных можно задать схему и обрабатывать ошибки (например, режим PERMISSIVE с обработкой плохих записей).
- Контрольные суммы (Checksums) - проверка контрольных сумм (например, MD5, SHA) для обнаружения повреждений при передаче.
- Мониторинг целостности данных - использование инструментов мониторинга (например, Apache Atlas) для отслеживания lineage и качества данных.

Обработка искажённых данных:
- Изоляция (Quarantine) - искажённые данные перемещаются в отдельное место (например, в папку "bad_records") для дальнейшего анализа и ручной обработки.
- Восстановление (Recovery) - если возможно, восстановить данные из резервных копий, попытаться извлечь корректные части.
- Игнорирование (Dropping) - если данные не критичны и их невозможно восстановить, их можно удалить. Но это должно быть осознанным решением.
- Репортинг и оповещение (Reporting and Alerting) - настроить оповещения о появлении искажённых данных, чтобы быстро реагировать.
- Использование отказоустойчивых форматов - использование форматов, устойчивых к повреждениям (например, Parquet, Avro) с встроенными механизмами проверки целостности.




<h3>4. Мониторинг качества данных</h3>

Мониторинг качества данных — это процесс непрерывного отслеживания и управления качеством информации, используемой в организации. Этот процесс важен для поддержания высокого уровня точности, актуальности и надежности данных, что критически важно для аналитических решений, операционной эффективности и стратегического планирования.

Цели мониторинга качества данных:
- Обнаружение проблем - своевременное выявление и устранение ошибок и несоответствий в данных.
- Повышение доверия - укрепление доверия пользователей к данным путем обеспечения их качества и надежности.
- Оптимизация решений - поддержание высокого уровня качества данных для обеспечения точности аналитических решений и бизнес-операций.

Основные компоненты системы мониторинга:
- Определение показателей качества данных (KPIs) - установление количественных показателей для оценки качества данных, таких как точность, полнота, консистентность, актуальность.
- Разработка правил и порогов - создание набора правил и пороговых значений для каждого KPI, при превышении которых инициируются действия по улучшению качества.
- Автоматизированный сбор и анализ данных - использование программных средств для автоматического сбора метрик качества и их анализа на соответствие установленным стандартам.
- Реагирование на проблемы - механизмы уведомления о проблемах качества данных и процессы их устранения.

Зачем нужно отслеживать изменение качества данных:
- Снижение бизнес-рисков - принятие решений на основе некачественных данных ведет к финансовым потерям, неверным стратегиям и репутационным издержкам. Например, некорректные данные о продажах могут привести к неверному прогнозированию спроса и упущенной выгоде.
- Повышение доверия к данным и AI/ML - если стейкхолдеры (аналитики, data scientists, бизнес-пользователи) не доверяют данным, они не будут их использовать. Мониторинг и видимость качества данных строят это доверие. Качественные данные — основа для эффективных ML-моделей.
- Эффективность и скорость разработки - когда инженеры и аналитики уверены в качестве входных данных, они тратят меньше времени на их "очистку" и отладку и больше — на создание ценности.
- Раннее обнаружение проблем - многие проблемы в данных носят "ползучий" характер (например, постепенное увеличение числа пропусков в колонке). Мониторинг позволяет поймать такие аномалии до того, как они нанесут ущерб.

<h4>Индикаторы и метрики качества данных</h4>

Основные категории и примеры метрик:
1. Полнота (Completeness) - определяет, насколько полны данные. Отсутствующие данные — одна из самых частых проблем. Метрики:
  - Доля пропусков (NULL) в столбце
  - Количество пустых строк
  - Доля заполненных записей в ключевых столбцах
2. Корректность/Точность (Accuracy) - определяет, насколько данные соответствуют реальному положению дел. Это одна из самых сложных для проверки метрик, так как часто требует сверки с эталонным источником. Метрики:
  - Соответствие формату (например, email, номер телефона).
  - Соответствие допустимому диапазону значений (age between 0 and 120).
  - Принадлежность к справочнику (например, country_code должен быть в списке кодов стран).
  - Количество записей, не прошедших бизнес-валидацию.
3. Непротиворечивость (Consistency) - проверяет, что данные не противоречат друг другу в рамках одной системы или между разными системами. Метрики:
  - Согласованность дублирующихся данных в разных источниках (например, баланс счета в OLTP и в витрине данных).
  - Проверка ссылочной целостности (например, у каждого order_id в таблице заказов должен существовать соответствующий user_id в таблице пользователей).
  - Согласованность вычислений (например, total_price = quantity * unit_price).
4. Уникальность (Uniqueness) - определяет отсутствие дубликатов. Метрики:
  - Количество дубликатов по ключевому полю.
  - Количество уникальных значений в столбце, где они должны быть уникальны.
5. Своевременность (Timeliness) - определяет, насколько данные актуальны на момент использования. Метрики:
  - Время задержки данных (latency): разница между временем появления данных в источнике и временем их доступности в целевом хранилище.
  - Фактическое время выполнения пайплайна vs SLA (Service Level Agreement).
  - Возраст самых свежих данных в таблице.
6. Валидность (Validity) - проверяет, что данные соответствуют определенному формату, структуре или типу. Метрики:
  - Соответствие схеме (количество записей, не соответствующих ожидаемой схеме Avro/Protobuf/JSON).
  - Корректность типов данных (например, в колонке amount нет текстовых значений).

<h4>Методы обнаружения аномалий в данных</h4>

Основные методы:
- Правила и пороговые значения (Rule-Based / Thresholding)
- Статистические методы - используют исторические данные для выявления отклонений.
- Машинное обучение - более сложные методы, которые могут обнаруживать неизвестные ранее аномалии без явных правил.
- Профилирование данных и сравнение схем - регулярный автоматический анализ метаданных.

Можно настраивать различные пороги оповещений, например:
1. Статические пороги (Static Thresholds) - жестко заданные значения, основанные на бизнес-требованиях или здравом смысле. Применяется для обнаружения абсолютно некорректных значений, где нет "серой зоны". Примеры:
  - Допустимое количество NULL в колонке user_id: 0 (никогда не должно быть пустым).
  - Допустимый диапазон значения age: от 0 до 120.
  - Допустимое количество дубликатов transaction_id: 0.
2. Статистические пороги (Statistical Thresholds) - пороги, основанные на исторических данных. Они адаптивны и более гибки. Применяются для метрик, которые естественным образом колеблются (объем данных, количество уникальных пользователей и т.д.). Примеры:
  - Предупреждение, если объем данных сегодня упал на 20% и более относительно среднего объема за последние 7 дней.
  - Предупреждение, если значение метрики вышло за пределы 3 сигм от скользящего среднего.
  - Предупреждение, если время выполнения пайплайна превысило 95-й процентиль за последний месяц.
3. Референсные пороги (Reference Thresholds) - сравнение с эталонным набором данных или с другим источником истины. Применяются для проверки согласованности между системами.

<h4>Анализ тенденций изменения качества данных</h4>

Это проактивный подход, который позволяет предсказать проблемы до того, как они достигнут критического порога. Что отслеживать:
- Тренд количества пропусков: Постепенное увеличение NULL в определенной колонке может сигнализировать о проблеме в источнике (например, сломался скрипт на стороне мобильного приложения).
- Тренд объема данных: Резкий рост может указывать на дублирование. Постепенное падение — на потерю данных или проблемы с доставкой.
- Тренд уникальности: Уменьшение количества уникальных значений в колонке, где оно должно быть стабильным.
- Тренд времени выполнения пайплайнов: Постепенное увеличение времени может сигнализировать о неоптимальном коде или нехватке ресурсов.

Данные тренды можно визуализировать на дашбордах и отслеживать в реальном времени

