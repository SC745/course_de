<h2>Apache Airflow</h2>
<h3>1. Основные концепции</h3>
<h4>Архитектура и применение</h4>

Apache Airflow — это открытая платформа для разработки, планирования и мониторинга рабочих процессов, разработанная Airbnb и позже переданная в Apache Software Foundation. Airflow позволяет описывать рабочие процессы как код, что обеспечивает их переносимость, документирование и возможность повторного использования. Основным преимуществом Airflow является его гибкость, позволяющая интегрироваться с множеством облачных и локальных сервисов.

Применение Apache Airflow:
- ETL/ELT процессы: Классический пример — извлечение данных из источников (базы данных, API, файлы), их преобразование (очистка, агрегация, обогащение) и загрузка в целевую систему (хранилище данных, озеро данных).
- Пакетная обработка данных: Запуск регулярных джобов (например, ежедневное или еженедельное обновление моделей машинного обучения, расчет агрегированных отчетов).
- Мониторинг и оповещения: Проверка доступности систем, отслеживание качества данных (Data Quality checks) и отправка уведомлений (по email, в Slack) в случае сбоев или аномалий.
- Обучение ML-моделей: Организация конвейеров для предобработки данных, обучения, валидации и развертывания моделей (MLOps).
- Управление облачной инфраструктурой: Автоматическое создание, изменение или удаление облачных ресурсов (например, в AWS, GCP, Azure) по расписанию или в ответ на события.
- Веб-скрапинг и сбор данных: Плановый обход веб-сайтов для сбора актуальной информации.
- Административные задачи: Регулярное резервное копирование баз данных, ротация логов, очистка временных файлов.

Apache Airflow обладает модульной архитектурой, состоящей из нескольких ключевых компонентов, которые взаимодействуют друг с другом для управления, планирования и мониторинга рабочих процессов:
- Веб-сервер (Web Server): Веб-сервер предоставляет пользовательский интерфейс (UI), через который пользователи могут взаимодействовать с Airflow — просматривать запущенные и запланированные рабочие процессы (DAGs), инициировать запуск рабочих процессов, просматривать логи и многое другое. Веб-сервер использует Flask под капотом для обслуживания веб-интерфейса.
- Планировщик (Scheduler): Планировщик — это сердце Airflow. Он отвечает за планирование задач, определённых в рабочих процессах (DAGs), согласно их зависимостям и графикам выполнения. Планировщик регулярно проверяет наличие новых заданий для запуска и назначает их на выполнение. Планировщик также отслеживает состояние выполнения задач и может перепланировать задачи при необходимости (например, в случае сбоев).
- Исполнители (Executors): Исполнители отвечают за выполнение задач, которые были запланированы и назначены планировщиком. Airflow поддерживает несколько типов исполнителей, которые можно выбирать в зависимости от требований к масштабируемости и окружению выполнения. Примеры исполнителей включают `LocalExecutor`, `SequentialExecutor`, `CeleryExecutor`, `DaskExecutor`, и `KubernetesExecutor`.
- База данных (Metadata Database): Airflow использует реляционную базу данных для хранения метаданных, связанных с выполнением рабочих процессов, состоянием задач, пользовательскими настройками и так далее. Поддерживаемые системы управления базами данных включают PostgreSQL и MySQL. Эта база данных является центральным хранилищем информации о состоянии Airflow и его компонентов.
- Метаданные и очередь сообщений (для CeleryExecutor): Когда используется CeleryExecutor, Airflow также использует брокер сообщений (например, RabbitMQ или Redis) для управления очередями задач и их распределения между рабочими процессами.
- Брокер сообщений и хранилище результатов Celery позволяют масштабировать выполнение задач на множество рабочих узлов.
- Логирование: Airflow предоставляет подробные логи выполнения задач, которые можно настроить для хранения в различных хранилищах, включая локальную файловую систему, удаленные системы (например, S3) или централизованные системы логирования.
- Модульность и Расширяемость: Airflow предлагает высокую степень модульности и расширяемости, позволяя разработчикам добавлять собственные операторы, хуки и интерфейсы. Это делает Airflow гибким инструментом, способным адаптироваться к разнообразным сценариям интеграции и автоматизации.

<h4>Основные понятия и преимущества</h4>

Основные понятия:
- DAG (Directed Acyclic Graph): Описывает всю последовательность задач (работ), которые нужно выполнить, их зависимости друг от друга. Каждый DAG имеет уникальный идентификатор и состоит из одной или нескольких задач.
- Оператор (Operator): Определяет тип задачи, которую необходимо выполнить. В Airflow существует множество встроенных операторов, например, для выполнения Bash команд, работы с Python кодом, обращения к базам данных и многим другим сервисам.
- Задача (Task): Операция или шаг в рабочем процессе, который выполняется. Задачи могут выполняться на различных исполнителях и могут зависеть от результатов других задач.
- Плагины (Plugins): Позволяют расширять функциональность Airflow, добавляя новые операторы, хуки (hooks) и интерфейсы.
- Хуки (Hooks): Предоставляют интерфейсы для взаимодействия с внешними системами и сервисами, например, базами данных, облачными хранилищами и API.

Ключевые преимущества:
- "Как код" (Workflow as Code): Пайплайны описываются на Python. Это дает:
  - Версионирование: DAG-файлы можно хранить в Git, отслеживать изменения, делать code review.
  - Тестируемость: Пайплайны можно тестировать стандартными инструментами для Python (pytest).
  - Динамическая генерация: Можно программно создавать сложные пайплайны, используя циклы и условия.
  - Поддержка сообщества: Огромное количество готовых операторов для интеграции с любыми сервисами (AWS, GCP, Azure, Snowflake, PostgreSQL и т.д.).
- Мощный и гибкий планировщик: Встроенный планировщик постоянно следит за DAG-файлами и запускает задачи по расписанию или по событию.
- Идеальный UI для мониторинга: Веб-интерфейс Airflow предоставляет детальный обзор состояния всех DAG и задач, позволяет просматривать логи, управлять выполнением (запускать, приостанавливать, перезапускать задачи), анализировать производительность.
- Масштабируемость: Архитектура Airflow позволяет легко масштабировать вычислительную мощность, используя различные исполнители (Executors), такие как Celery или Kubernetes. Это позволяет обрабатывать тысячи одновременно работающих задач.
- Параллелизм и повторные попытки: Airflow умеет выполнять множество задач параллельно. Он имеет встроенный механизм повторных попыток при сбоях с настраиваемой задержкой, что повышает отказоустойчивость пайплайнов.
- Четкая параметризация и контекст: С помощью Jinja-шаблононизации и контекста выполнения можно передавать в задачи метаданные (например, дату выполнения, ds), делая пайплайны очень гибкими.
- Экосистема и сообщество: Airflow — проект с открытым исходным кодом под крылом Apache Software Foundation, что гарантирует активное развитие и поддержку.

<h4>Отличие Apache Airflow от других подобных систем</h4>

Apache Airflow — это платформа для программируемого оркестрирования рабочих процессов (workflows) и управления ими. Ключевые отличия:
- Динамические конвейеры: Airflow позволяет определять конвейеры (DAG) как код на Python, что дает гибкость в создании сложных, условных и динамических рабочих процессов.
- Расписание и отслеживание: Встроенный планировщик и богатый пользовательский интерфейс для мониторинга и управления выполнениями.
- Расширяемость: Легко расширяется с помощью операторов, хуков и сенсоров, что позволяет интегрироваться с множеством внешних систем.
- Открытый исходный код: Является проектом Apache с активным сообществом.

Сравнение с другими системами:
- Luigi: Airflow имеет более богатый UI и более развитую экосистему. Оба используют Python, но Airflow более популярен и имеет более активное сообщество.
- Apache Oozie: Oozie тесно связан с Hadoop и использует XML для определения workflow, что менее гибко по сравнению с кодом на Python в Airflow.
- AWS Step Functions: Это облачный сервис, который использует JSON для определения state machine. Airflow же более универсален и может быть развернут где угодно, а также имеет более мощные возможности для обработки данных.
- Prefect: Более современный оркестратор, который пытается решить некоторые проблемы Airflow (например, более простая обработка динамических задач), но Airflow все еще имеет большее распространение.

<h4>Web UI Apache Airflow</h4>

Веб-интерфейс Airflow предоставляет богатый набор функций для управления и мониторинга:
- DAGs View: Список всех DAG с их статусами (включен/выключен), расписанием и последними запусками. Можно включать/выключать DAG, а также просматривать их код.
- Graph View: Визуализация DAG в виде графа задач, показывающая зависимости и статусы задач для конкретного запуска.
- Tree View: Древовидное представление запусков DAG, где можно увидеть статусы задач для нескольких запусков одновременно.
- Task Duration: График продолжительности выполнения задач для выявления аномалий.
- Gantt Chart: Диаграмма Ганта, показывающая, когда какие задачи выполнялись и как долго.
- Code: Просмотр кода DAG (только для чтения).
- Task Instances: Список экземпляров задач с возможностью фильтрации и изменения их состояния (например, пометить как успешное или перезапустить).
- Audit Trail: Журнал событий, показывающий, кто и когда изменял состояния DAG и задач (требуется настройка аутентификации и прав).
- Variables: Просмотр и редактирование переменных Airflow (ключ-значение) через UI.
- Connections: Управление подключениями к внешним системам (базы данных, API и т.д.).
- Pools: Управление пулами ресурсов для ограничения параллельного выполнения определенных типов задач.
- Configuration: Просмотр текущей конфигурации Airflow (только для чтения, так как конфигурация загружается из файла или переменных окружения).
- Plugins: Просмотр зарегистрированных плагинов.

Настройки UI:
- Большинство настроек UI определяются в конфигурации Airflow (`airflow.cfg`) или через переменные окружения.
- Можно настроить аутентификацию (по умолчанию, OAuth, LDAP и др.) и авторизацию (роли пользователей).
- Можно кастомизировать цвета и логотип для корпоративного использования.

<h4>Обработка выполнения задач в распределенной среде</h4>

В распределенной среде Airflow typically состоит из нескольких компонентов:
- Metadata Database: Общая база данных (например, PostgreSQL, MySQL) для хранения состояния задач, DAG, переменных и т.д.
- Scheduler: Один или несколько планировщиков, которые читают DAG, определяют, какие задачи нужно запустить, и ставят их в очередь.
- Executors: Исполнители, которые берут задачи из очереди и запускают их. В распределенной среде обычно используются:
  - CeleryExecutor: Задачи распределяются между несколькими воркерами (worker) через брокер сообщений (например, Redis, RabbitMQ).
  - KubernetesExecutor: Каждая задача запускается в отдельном Pod в кластере Kubernetes.
- Workers: В случае CeleryExecutor, это процессы (или серверы), которые выполняют задачи. В случае KubernetesExecutor, задачи выполняются в Pod, которые создаются и уничтожаются динамически.
- Web Server: Один или несколько веб-серверов, которые предоставляют UI и API.

Процесс выполнения задачи:
1. Планировщик периодически проверяет DAG и создает экземпляры задач, которые должны быть запущены, устанавливая их состояние в "queued" в базе данных.
2. Исполнитель (Executor) забирает задачи из очереди (через базу данных или брокер сообщений) и назначает их на доступного воркера.
3. Воркер получает задачу, меняет ее статус на "running" и выполняет код задачи.
4. После выполнения задача переходит в состояние "success" или "failure", и планировщик может запускать следующие задачи, зависящие от нее.

Особенности распределенного выполнения:
- Масштабируемость: Добавление большего количества воркеров позволяет параллельно выполнять больше задач.
- Отказоустойчивость: Если воркер падает, то задача может быть перезапущена на другом воркере (если используется CeleryExecutor) или перезапущена в новом Pod (в KubernetesExecutor).
- Изоляция: Каждая задача выполняется в своем процессе (Celery) или контейнере (Kubernetes), что обеспечивает изоляцию зависимостей и ресурсов.

Важно правильно настроить бэкенд для хранения результатов (XCom) и логирования, чтобы они были доступны из любого воркера. Обычно для этого используют облачные хранилища или сетевые файловые системы.



<h4>Directed Acyclic Graph (DAG)</h4>

DAG в Airflow — это структура данных, представляющая последовательность операций (задач), которые нужно выполнить, учитывая зависимости между ними. Каждый DAG определяет рабочий процесс, который Airflow будет автоматизировать и планировать. DAG определяется в Python файле, который описывает задачи и их зависимости. Этот файл должен быть помещен в папку DAGs Airflow, где планировщик Airflow сможет его обнаружить. Пример определения DAG:

```python
from datetime import timedelta
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.utils.dates import days_ago

# Определение аргументов DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email': ['example@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Инициализация DAG
dag = DAG(
    'tutorial',
    default_args=default_args,
    description='A simple tutorial DAG',
    schedule_interval=timedelta(days=1),
    start_date=days_ago(2),
    tags=['example'],
)

# Определение задач
t1 = BashOperator(
    task_id='print_date',
    bash_command='date',
    dag=dag,
)

t2 = BashOperator(
    task_id='sleep',
    depends_on_past=False,
    bash_command='sleep 5',
    retries=3,
    dag=dag,
)

# Установка зависимостей между задачами
t1 >> t2
```

<h4>Оператор</h4>

Operator определяет тип задачи, которую нужно выполнить. В Airflow существует множество встроенных операторов для выполнения различных типов задач, например, BashOperator для выполнения Bash команд, PythonOperator для запуска Python скриптов и многие другие. Выбор подходящего оператора зависит от задачи, которую вы хотите автоматизировать. Airflow также позволяет создавать собственные операторы для специфических нужд. Пример использования PythonOperator:

```python
from airflow.operators.python_operator import PythonOperator

def my_python_function():
    print("Hello from Python!")

t3 = PythonOperator(
    task_id='python_task',
    python_callable=my_python_function,
    trigger_rule='all_success'
    dag=dag,
)
```

Триггерные правила (Trigger Rules):
- `all_success`: (по умолчанию) все upstream задачи успешны.
- `all_failed`: все upstream задачи завершились неудачно.
- `one_success`: хотя бы одна upstream задача успешна.
- `one_failed`: хотя бы одна upstream задача неудачна.
- `none_failed`: все upstream задачи не завершились неудачно (т.е. успешны или пропущены).
- `none_skipped`: нет пропущенных upstream задач.
- `dummy`: не проверяет состояния upstream.

<h4>Задача</h4>

Задача — это экземпляр оператора, который выполняет определенную работу. Задача определяется при помощи оператора и добавляется в DAG. Задачи можно настроить на выполнение в определенной последовательности с учетом их зависимостей. Зависимости между задачами задаются с использованием операторов >> и <<. Это определяет порядок выполнения задач в рамках DAG.

Пример установления зависимостей:
```python
t1 >> t3  # Запуск t3 после завершения t1
t2 >> t3  # Запуск t3 после завершения t2
t1 >> [t2, t3] # t1 выполняется первой, затем параллельно запускаются t2 и t3
[t1, t2] >> t3 # И t1, и t2 должны завершиться, чтобы t3 запустилась
```
Использование DAGs, Operators и Tasks вместе позволяет Airflow эффективно управлять сложными рабочими процессами, автоматизируя их планирование и выполнение, а также предоставляя инструменты для мониторинга и отладки.

<h4>Исполнитель</h4>

Исполнитель в Airflow определяет, как и где выполняются задачи. Это компонент, который отвечает за запуск задач в расписании. Существует несколько типов исполнителей, каждый из которых подходит для разных сценариев:
- `SequentialExecutor`: Стандартный исполнитель, который идет по умолчанию в Airflow. Выполняет задачи последовательно, одну за другой. Не поддерживает параллельное выполнение, поэтому не подходит для производственных сред. Использует базу данных SQLite, так как SQLite не поддерживает параллельные записи.
- `LocalExecutor`: Выполняет задачи параллельно на локальной машине, используя многопроцессорность. Подходит для производственных сред, если Airflow развернут на одном сервере (не распределенно). Может использовать базы данных, такие как PostgreSQL или MySQL, которые поддерживают параллельные подключения.
- `CeleryExecutor`: Распределенный исполнитель, который использует очередь задач Celery для распределения задач по нескольким рабочим узлам (worker nodes). Подходит для кластерных развертываний и позволяет масштабировать выполнение задач. Требует настройки брокера сообщений (например, Redis или RabbitMQ) для координации.
- `DaskExecutor`: Использует распределенную вычислительную систему Dask для выполнения задач. Позволяет использовать кластер Dask для распределения задач.
- `KubernetesExecutor`: Запускает каждую задачу в отдельном Pod в кластере Kubernetes. Обеспечивает высокую степень изоляции и масштабируемости. Динамически создает и удаляет Pod'ы для задач, что позволяет эффективно использовать ресурсы.

<h4>Планировщик</h4>

Scheduler (планировщик) — это центральный компонент Airflow, который отвечает за:
- Парсинг DAG файлов и извлечение метаданных (задачи, зависимости, расписания)
- Создание DAG Run (экземпляров DAG) согласно расписанию и условиям (например, catchup)
- Создание Task Instance (экземпляров задач) для каждого DAG Run
- Определение готовности Task Instance к выполнению (учитывая зависимости и состояние upstream задач)
- Отправка готовых к выполнению Task Instance в Executor

Цикл планировщика:
1. Scheduler постоянно выполняет цикл (heartbeat), в котором он: Обновляет список DAGs (парсит файлы), проверяет, нужно ли создать новые DAG Runs (по расписанию или вручную), проверяет состояние Task Instance (уже запущенных и завершенных) и определяет, какие Task Instance готовы к выполнению (все зависимости выполнены, есть свободные слоты)
2. Отправка задач в Executor: Для каждого готового Task Instance планировщик вызывает `executor.queue_task_instance(task_instance)`. Executor помещает задачу в очередь (в случае `LocalExecutor` - в процесс, в `CeleryExecutor` - в очередь сообщений)
3. Мониторинг выполнения: Планировщик периодически проверяет статус задач, которые были отправлены в Executor, обновляя их состояние в мета-БД.
4. Повторное планирование: Если задача завершилась неудачно и есть попытки, планировщик планирует ее повторное выполнение.

<h3>2. Построение и управление Workflow</h3>

<h4>Управление зависимостями</h4>

Управление зависимостями в Airflow обычно задается с помощью операторов `>>` и `<<` или методов `set_upstream` и `set_downstream`. Однако иногда требуется более сложная условная логика, которая определяет, какие задачи должны выполняться в зависимости от результатов предыдущих задач. Для этого используется `BranchPythonOperator`. `BranchPythonOperator` - это специальный оператор, который принимает Python-функцию, возвращающую `task_id` (или список task_id) следующей задачи, которая должна быть выполнена. Все остальные задачи в том же наборе (то есть те, которые не были выбраны) будут пропущены (состояние `SKIPPED`).

Допустим, у нас есть задача, которая проверяет качество данных. Если данные хорошие, мы хотим запустить задачу `process_data`, иначе — задачу `send_alert`:

```python
from airflow.operators.python_operator import BranchPythonOperator
from airflow.operators.dummy_operator import DummyOperator
from airflow import DAG

def decide_branch(**kwargs):
    # Предположим, мы получаем результат из предыдущей задачи через XCom
    if kwargs['ti'].xcom_pull(task_ids='data_quality_check') == 'good':
        return 'process_data'
    else:
        return 'send_alert'

with DAG('branching_dag', default_args=default_args, schedule_interval=None) as dag:
    data_quality_check = DummyOperator(task_id='data_quality_check')
    branch = BranchPythonOperator(
        task_id='branch',
        python_callable=decide_branch,
        provide_context=True
    )
    process_data = DummyOperator(task_id='process_data')
    send_alert = DummyOperator(task_id='send_alert')

    data_quality_check >> branch
    branch >> [process_data, send_alert]
```

В этом примере после `data_quality_check` выполняется branch, который решает, какую ветку выполнить. Обратите внимание, что обе ветки (`process_data` и `send_alert`) должны быть связаны с оператором ветвления, но выполнится только одна из них.

<h4>Условное выполнение задач</h4>

Условное выполнение позволяет динамически выбирать следующие задачи на основе результатов предыдущих, осуществляется с помощью операторов `BranchPythonOperator` и `ShortCircuitOperator`

`BranchPythonOperator` - принимает Python-функцию, которая возвращает `task_id` (или список `task_id`) следующей задачи. Все остальные задачи в downstream помечаются как `SKIPPED`:
```python
from airflow.operators.python import BranchPythonOperator

def choose_branch(**context):
    if context['execution_date'].weekday() < 5:
        return 'weekday_task'
    else:
        return 'weekend_task'

branch_op = BranchPythonOperator(
    task_id='branch_task',
    python_callable=choose_branch,
    provide_context=True
)

weekday_task = DummyOperator(task_id='weekday_task')
weekend_task = DummyOperator(task_id='weekend_task')

branch_op >> [weekday_task, weekend_task]
```

`ShortCircuitOperator` - пропускает downstream задачи, если условие возвращает `False`:
```python
from airflow.operators.python import ShortCircuitOperator

def condition_fn(**context):
    # Если условие вернет False, все downstream задачи будут пропущены
    return context['execution_date'].day % 2 == 0

short_circuit = ShortCircuitOperator(
    task_id='short_circuit',
    python_callable=condition_fn,
    provide_context=True
)

downstream_task = DummyOperator(task_id='downstream_task')

short_circuit >> downstream_task
```

<h4>Передача данных между задачами</h4>

Передача данных между задачами осуществляется с помощью Xcom. XCom (сокращение от Cross-Communication) — это механизм в Airflow, который позволяет задачам обмениваться небольшими объемами данных. XCom хранятся в базе данных Airflow.

Задачи могут отправлять XCom, вызывая `xcom_push` в контексте задачи (например, из `PythonOperator`) или возвращая значение из функции (в `PythonOperator` возвращаемое значение автоматически становится XCom):
```python
def push_function(**kwargs):
    kwargs['ti'].xcom_push(key='my_key', value='my_value')
    return 'This is returned value'  # Это также будет отправлено как XCom с ключом 'return_value'

task1 = PythonOperator(
    task_id='push_task',
    python_callable=push_function,
    provide_context=True,
    dag=dag
)

```
Задачи могут получать XCom, вызывая `xcom_pull`, передавая `task_id` и, опционально, `key` (если ключ не указан, то будет получен XCom с ключом `'return_value'`):
```python
def pull_function(**kwargs):
    value = kwargs['ti'].xcom_pull(task_ids='push_task', key='my_key')
    returned_value = kwargs['ti'].xcom_pull(task_ids='push_task')  # Получит 'return_value'
    # ... что-то делаем с value и returned_value

task2 = PythonOperator(
    task_id='pull_task',
    python_callable=pull_function,
    provide_context=True,
    dag=dag
)
```

XCom предназначены для небольших данных (например, строки, числа, небольшие словари). Не следует использовать их для передачи больших объектов (например, датафреймов), так как это может негативно сказаться на производительности базы данных. Для передачи больших данных между задачами лучше использовать внешние хранилища (например, S3, HDFS, базы данных) и передавать через XCom только ссылку или путь к данным.

<h4>Обработка ошибок, повторы выполнения и уведомления</h4>

Обработка ошибок и повторы (Retries):
- Повторы (Retries): Задачи могут автоматически повторяться при сбое. Количество повторов задается параметром `retries` (на уровне DAG или задачи), а задержка между повторами — `retry_delay`. Можно использовать экспоненциальную задержку (`exponential_backoff`).
- Обработка исключений в задачах: Задачи должны обрабатывать ожидаемые исключения (например, временная недоступность API). Необработанные исключения приводят к провалу задачи.
- Триггерные правила (Trigger Rules): По умолчанию задача запускается, когда все upstream-задачи успешно завершены. Но можно изменить правило (`trigger_rule`), например, на `all_failed` или `one_failed`, чтобы обрабатывать ошибки.
- Callback-функции: DAG и задачи могут иметь callback-функции, которые вызываются при успехе, провале или повторе.

Уведомления:
- Email: Настроить SMTP-сервер в `airflow.cfg` (`smtp_host`, `smtp_port` и т.д.). Задавать `email_on_failure` и `email_on_retry` в `default_args` или на уровне задачи.
- Другие каналы: Использовать on_failure_callback для отправки уведомлений в Slack, Telegram, PagerDuty и т.д.

Пример callback-функции для Slack:
```python
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator

def slack_failure_alert(context):
    # Функция для отправки уведомления в Slack при провале
    slack_msg = f"""
            :red_circle: Task Failed.
            *Task*: {context.get('task_instance').task_id}
            *Dag*: {context.get('task_instance').dag_id}
            *Execution Time*: {context.get('execution_date')}
            *Log Url*: {context.get('task_instance').log_url}
            """
    slack_alert = SlackWebhookOperator(
        task_id='slack_alert',
        http_conn_id='slack_webhook',
        message=slack_msg
    )
    return slack_alert.execute(context=context)

default_args = {
    'on_failure_callback': slack_failure_alert,
    # ... другие аргументы
}
```

<h4>Динамическое изменение параметров задач и Advanced Templating</h4>

Airflow использует Jinja2 в качестве механизма шаблонов, что позволяет динамически устанавливать параметры задач во время выполнения.
- Шаблоны (Templating): Параметры, которые можно шаблонизировать, обозначаются в документации как `«templated»`. Например, параметры `sql`, `bash_command` в операторах `BigQueryOperator` и `BashOperator` соответственно поддерживают шаблоны. Вы можете использовать переменные, такие как `{{ ds }}` (текущая дата в формате `YYYY-MM-DD`) или макросы.
- Расширенные шаблоны: Вы можете создавать собственные макросы и шаблоны, используя Jinja2. Также можно передавать пользовательские параметры через params в операторах, которые также могут быть шаблонизированы.
- Динамическое изменение параметров: Иногда нужно изменить параметры задачи в зависимости от результата предыдущей задачи. Для этого можно использовать XCom (см. ниже) в сочетании с шаблонами.

`task2` использует шаблон для получения значения из XCom задачи `task1`:
```python
task1 = PythonOperator(
    task_id='push_value',
    python_callable=lambda: 'my_value',
    xcom_push=True,
)

task2 = BashOperator(
    task_id='use_value',
    bash_command='echo "{{ task_instance.xcom_pull(task_ids=\'push_value\') }}"',
)
```

<h4>Многоразовые подзадачи и автоматическое масштабирование исполнителей</h4>

Ранее в Airflow были доступны SubDAGs для создания многоразовых групп задач. Однако они были признаны устаревшими начиная с версии 2.0, потому что имели проблемы с производительностью и отладкой. Вместо SubDAGs теперь рекомендуется использовать TaskGroups (доступны с версии 1.10.7 и выше). TaskGroups позволяют логически группировать задачи в UI без создания отдельного DAG, что улучшает производительность и удобство.

Пример `TaskGroup`:
```python
from airflow.utils.task_group import TaskGroup

with TaskGroup("my_group") as group:
    task1 = DummyOperator(task_id="task1")
    task2 = DummyOperator(task_id="task2")
    task1 >> task2
```

Автоматическое масштабирование исполнителей (Auto-scaling executors): Airflow поддерживает несколько типов исполнителей. Для автоматического масштабирования обычно используется `KubernetesExecutor` или `CeleryExecutor` с автоматическим масштабированием воркеров:
`KubernetesExecutor`: Запускает каждый task в отдельном `pod` в Kubernetes. Kubernetes может автоматически масштабировать узлы (если настроен `cluster autoscaler`) и, соответственно, количество одновременно выполняемых задач. Также есть `KubernetesPodOperator`, который позволяет запускать задачи в отдельных `pod` без использования `KubernetesExecutor`.
`CeleryExecutor`: Использует несколько воркеров для выполнения задач. Можно настроить автоматическое масштабирование количества воркеров с помощью систем, таких как Kubernetes Horizontal Pod Autoscaler, или с помощью встроенного в Celery механизма autoscale (но обычно autoscale на уровне Celery не используется в production, а предпочитают масштабирование на уровне инфраструктуры).
- `LocalExecutor`: Не поддерживает автоматическое масштабирование, так как запускает задачи на одном узле.

<h3>3. Производительность, масштабирование и надежность</h3>
<h4>Оптимизация выполнения DAG</h4>

Оптимизация DAG направлена на ускорение их выполнения, снижение потребления ресурсов и повышение надежности. Ключевые параметры и методы:
- Расписание (Schedule Interval): Правильно настроить `schedule_interval` (например, `@daily`, `@hourly`, cron-выражение) в соответствии с потребностями. Не ставить слишком частые интервалы, если данные не требуют этого.
- `start_date` и `end_date`: `start_date` должен быть фиксированной датой (не динамической, например, `datetime(2023, 1, 1)`), чтобы избежать неожиданных запусков.
- `catchup` (Догонка): Определяет, следует ли запускать пропущенные задания за период между `start_date` и текущей датой. В продакшене часто отключают (`catchup=False`), чтобы избежать множественных запусков за прошлые периоды.
- `max_active_runs`: Ограничивает количество одновременно активных запусков одного DAG. Это предотвращает конфликты и избыточную нагрузку.
- `concurrency`: Ограничивает количество одновременно выполняемых задач в рамках одного DAG.
- `dagrun_timeout`: Устанавливает максимальное время для всего запуска DAG. Если выполнение превышает это время, весь DAG помечается как `failed`.
- `task_timeout`: На уровне оператора можно установить `execution_timeout` (например, `datetime.timedelta(minutes=30)`), чтобы задача была прервана, если выполняется дольше.
- Параллелизм и исполнитель: Выбор исполнителя (например, `CeleryExecutor`, `KubernetesExecutor`) и настройка параллелизма (`parallelism` в `airflow.cfg`) влияют на общую производительность.
- Использование пулов (Pools): Пуллы позволяют ограничить количество одновременно выполняемых задач определенного типа. Например, задачи, которые работают с внешним API с ограничениями по RPS, можно поместить в пул с ограниченным количеством слотов.
- Приоритеты задач: Задачам можно назначать приоритеты (`priority_weight`), чтобы планировщик сначала выполнял более важные задачи.
- Оптимизация кода DAG: Избегать тяжелых операций в верхнем уровне кода DAG (вне задач). Весь код на верхнем уровне выполняется планировщиком каждые несколько секунд, что может замедлить его работу. Использовать `@once` для операций, которые нужно выполнить только один раз при запуске DAG.

DAG с оптимизированными параметрами:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2),
}

with DAG(
    'optimized_dag',
    default_args=default_args,
    description='An optimized DAG',
    schedule_interval='0 0 * * *',
    catchup=False,
    max_active_runs=1,
    concurrency=4,
    dagrun_timeout=timedelta(hours=4),
    tags=['example'],
) as dag:

    def my_task():
        pass

    task = PythonOperator(
        task_id='my_task',
        python_callable=my_task,
        # Можно переопределить параметры на уровне задачи
        retries=2,
        execution_timeout=timedelta(hours=1),
    )
```

<h4>Оптимизация производительности Airflow</h4>

Оптимизация производительности Airflow включает настройку конфигурационных параметров и инфраструктуры. Ключевые параметры в `airflow.cfg`:
- Исполнитель (Executor): Для продакшена используйте CeleryExecutor или KubernetesExecutor для распределенной обработки.
- Параллелизм (Parallelism):
  - `parallelism`: максимальное количество задач, которые могут выполняться одновременно во всем Airflow.
  - `dag_concurrency`: максимальное количество задач, которые могут выполняться одновременно в рамках одного DAG (можно переопределить на уровне DAG параметром `concurrency`).
- Максимальное количество активных запусков DAG (`max_active_runs_per_dag`): Глобальное ограничение на количество активных запусков для каждого DAG.
- Интервал планировщика (scheduler_heartbeat_sec): Интервал, с которым планировщик обновляет свои статусы. Обычно оставляют 5 секунд.
- Настройки базы данных: Используовать пул соединений (`sql_alchemy_pool_enabled = True`), чтобы уменьшить нагрузку на БД. Регулярно проводить очистку старых записей в БД (например, с помощью `airflow db clean`).
- Настройки исполнителя Celery:
  - `worker_concurrency:` количество задач, которые один воркер Celery может выполнять одновременно.
  - `celeryd_prefetch_multiplier`: предварительная выборка задач воркерами. Уменьшение этого значения может помочь в равномерном распределении задач.
- Кеширование: Включите кеширование DAG-файлов (`dagbag_import_timeout` и `dag_file_processor_manager_log_location`).
- Метрики и мониторинг: Включить метрики (`metrics`), чтобы отслеживать производительность и вовремя обнаруживать узкие места.

Инфраструктурные оптимизации:
- База данных: Использовать производительную БД (например, PostgreSQL) и настройте индексы.
- Планировщик: Для больших установок может потребоваться несколько планировщиков (с версии 2.0 поддерживается HA для планировщика).
- Воркеры: Масштабировать воркеры (для Celery) в зависимости от нагрузки.
- Хранилище DAG-файлов: Используйте быстрое хранилище (например, AWS S3, GCS) и настройте эффективную синхронизацию.

<h4>Масштабирование и отказоустойчивость</h4>

Airflow — это распределенная система, и ее архитектура напрямую влияет на масштабируемость и устойчивость к сбоям.

Компоненты и их устойчивость:
- Метаданные (Metadata Database): Мозг системы. Хранит состояния DAG, задач, подключений, переменных и т.д. Это единая точка отказа (SPOF). Для повышения отказоустойчивости необходимо настраивать репликацию (например, Master-Slave в MySQL/PostgreSQL) или использовать кластеризованные БД. Производительность БД — ключевой фактор. Необходимо мониторить нагрузку и масштабировать БД (более мощный сервер, оптимизация запросов).
- Планировщик (Scheduler): Определяет, какие задачи нужно запустить, и ставит их в очередь. В Airflow 2.0 можно запускать несколько Scheduler в режиме Active/Active. Они координируют работу через БД. Если один падает, второй немедленно берет на себя его обязанности. Это устраняет главную SPOF старых версий. Добавление более одного планировщика помогает справляться с большим количеством DAG и задач, распределяя нагрузку по парсингу и планированию.
- Исполнитель (Executor): Забирает задачи из очереди и запускает их. Отказоустойчивость и масштабируемость:
- - `LocalExecutor`: Не отказоустойчив и плохо масштабируется (ограничено одной машиной).
- - `CeleryExecutor`: Рабочие (Celery Workers) могут быть развернуты на множестве серверов. Если один worker падает, его задачи будут переназначены другому. Можно динамически добавлять/убирать воркеры.
- - `KubernetesExecutor`: Наиболее эластичный. Каждая задача запускается в отдельном `Pod`. При падении узла Kubernetes сам перезапустит `Pod` на другом узле. Кластер Kubernetes можно автоматически масштабировать.
- Веб-сервер (Web Server): Предоставляет UI для мониторинга и управления. Не имеет состояния (stateless). Можно и нужно запускать несколько экземпляров за балансировщиком нагрузки (Load Balancer). Если один падает, другие продолжают обслуживать запросы.
- Очередь (Queue): Связующее звено между `Scheduler` и `Executor` (при использовании `CeleryExecutor`). Очередь (обычно Redis или RabbitMQ) сама по себе должна быть отказоустойчивой. Для Redis можно использовать кластерный режим (Redis Sentinel), для RabbitMQ — кластер с зеркалированием.

Для построения отказоустойчивого и масштабируемого кластера Airflow необходимо:
- Использовать Airflow 2.0+ с поддержкой нескольких планировщиков.
- Выбрать `KubernetesExecutor` или `CeleryExecutor`.
- Настроить репликацию для БД метаданных.
- Развернуть несколько Web Server за балансировщиком.
- Обеспечить отказоустойчивость брокера сообщений (очереди).

<h4>Мониторинг, логирование и управление версиями DAG</h4>

Инструменты мониторинга:
- Встроенный UI Airflow:
  - DAGs View: Обзор всех DAG, их статусы (успех, провал, выполняется).
  - Graph / Tree View: Визуализация выполнения конкретного DAG Run, позволяет быстро найти упавшую задачу.
  - Alerting: В UI можно настроить оповещения на email при провале DAG или задачи.
- Интеграция с внешними системами мониторинга:
  - Метрики: Airflow предоставляет богатый набор метрик через StatsD (количество успешных/неудачных задач, время выполнения DAG, время ожидания в очереди и т.д.).
  - Prometheus: Можно направить метрики из StatsD в Prometheus через экспортер. После этого строить дашборды в Grafana для визуализации здоровья всего кластера. Пример метрик: `airflow_dag_processing_total`, `airflow_task_failures`, `airflow_scheduler_heartbeat`.
  - Оповещения: Настраиваются в Prometheus/Grafana или системах вроде PagerDuty, OpsGenie при выходе метрик за пороговые значения.

Логирование:
- Хранение логов в Airflow: По умолчанию логи пишутся на локальный диск планировщика/воркеров. Это плохая практика для распределенной системы. Правильный подход — централизованное логирование.
- Бэкенды для логов (Remote Logging):
  - Airflow поддерживает отправку логов в удаленные хранилища:
    - Cloud Storage: S3, GCS, Azure Blob Storage.
    - Elasticsearch: Позволяет не только хранить, но и эффективно искать по логам.
  - В UI Airflow есть вкладка "Log" для каждой задачи. При настройке Remote Logging UI будет автоматически подтягивать логи из удаленного источника, даже если воркер, на котором выполнялась задача, уже уничтожен (что часто бывает с `KubernetesExecutor`).

Управление версиями DAG (DAG Versioning):
1. Git как источник истины (Source of Truth): Все DAG-файлы хранятся в Git-репозитории. Каждое изменение (новая функциональность, багфикс) проходит через Pull/Merge Request с ревью.
2. Процесс развертывания (Deployment):
  - CI/CD (Непрерывная интеграция и доставка): При merge кода в главную ветку (напр., main) CI/CD пайплайн (например, GitHub Actions, GitLab CI) автоматически собирает образ (если используется) и обновляет DAG-файлы на серверах Airflow.
  - Синхронизация папки `dags/`:
    - Spontaneous Sync: Папка `dags` монтируется как Git-репозиторий, и планировщик периодически ее подтягивает (git pull). Просто, но менее надежно.
    - Volume Mount (Kubernetes): В Kubernetes папка `dags` монтируется как Persistent Volume, которая автоматически синхронизируется с Git-репозиторием с помощью sidecar-контейнера (например, `git-sync`). Это лучшая практика для продакшена.
3. Версионность в Airflow: Airflow сам не управляет версиями DAG в классическом понимании (как Git), но он отслеживает код DAG. В базе данных хранится хэш кода каждого DAG. Если хэш изменился (вы задеплоили новую версию), планировщик перезагружает определение DAG. Все запущенные экземпляры DAG Run продолжают работать со старой версией кода до своего завершения. Новые запуски будут использовать новую версию. Это предотвращает несогласованность состояния во время выполнения.

<h4>Настройка для высокой доступности</h4>

Высокая доступность в Airflow достигается за счет развертывания его ключевых компонентов в кластерной конфигурации, чтобы не было единой точки отказа.

Ключевые настройки компонентов для высокой доступности:
- Метаданные Базы Данных (Metadata Database): Использование управляемой облачной СУБД с репликацией (например, Amazon RDS (PostgreSQL/MySQL), Google Cloud SQL или Azure Database for PostgreSQL). Они предоставляют встроенную репликацию и автоматическое переключение при сбое (failover).
- Планировщик (Scheduler): Запуск нескольких экземпляров скедулера одновременно. Начиная с версии 1.10.12 и в версии 2.x, Airflow официально поддерживает HA для скедулера. Несколько планировщик координируют свою работу через базу данных, используя механизм блокировок. Они не будут планировать одну и ту же задачу дважды. Если один скедулер падает, другой мгновенно берет на себя его работу.
- Веб-сервер (Web Server): Запуск нескольких экземпляров веб-сервера за балансировщиком нагрузки (например, nginx, HAProxy или облачным ALB/NLB). Все веб-серверы подключаются к одной и той же базе метаданных и брокеру сообщений (если используется).
- Исполнитель (Executor):: Использование распределенного исполнителя, такого как CeleryExecutor или KubernetesExecutor.
  - CeleryExecutor: Задачи ставятся в очередь (например, Redis или RabbitMQ), а несколько воркеров (Celery Workers) на разных машинах забирают задачи из этой очереди. Отказ одного воркера не остановит систему.
  - KubernetesExecutor: Каждая задача запускается в виде отдельного Pod в кластере Kubernetes. Kubernetes сам управляет жизненным циклом подов и обеспечивает отказоустойчивость.
- Брокер сообщений (для CeleryExecutor): Использование кластеризованного брокера, например, Redis Sentinel или RabbitMQ в кластерном режиме.

<h4>Настройка для работы в нескольких датацентрах</h4>

Развертывание Airflow в нескольких датацентрах (регионах) направлено на обеспечение отказоустойчивости и географической близости к данным или вычислительным ресурсам. Однако важно отметить, что Airflow изначально не предназначен для распределения одного кластера across multiple data centers с задержками в сети. Вместо этого, обычно используют активный-пассивный или активный-активный сценарий, но с осторожностью.

Подходы:
- Активный-пассивный (Active-Passive): В одном датацентре развернут активный кластер Airflow, а в другом — пассивный, который находится в режиме ожидания. Метаданные (база данных) должны реплицироваться синхронно или асинхронно из активного в пассивный датацентр. В случае сбоя в активном датацентре, необходимо переключить трафик на пассивный кластер. Это может быть сделано вручную или с помощью автоматических механизмов (например, DNS). База данных должна быть согласованной. Синхронная репликация может повлиять на производительность из-за задержек между датацентрами.
- Активный-активный (Active-Active): Это сложная конфигурация, так как компоненты Airflow (особенно scheduler) не предназначены для работы в режиме активный-активный в нескольких датацентрах с высокой задержкой. Однако, можно рассмотреть следующие варианты:
  - Запуск независимых кластеров Airflow в каждом датацентре, которые управляют разными наборами DAG. Это не настоящий активный-активный для одного кластера, а скорее разделение workload.
  - Если нужно, чтобы один кластер работал в двух датацентрах, то необходимо обеспечить низкую задержку между датацентрами для базы данных и брокера сообщений (если используется Celery). В противном случае, производительность и стабильность могут сильно пострадать.

Компоненты и их настройка:
- База данных: должна быть развернута в конфигурации с репликацией между датацентрами. Например, PostgreSQL с потоковой репликацией. Однако, если задержка между датацентрами высока, то синхронная репликация может быть невозможна, и придется использовать асинхронную, что несет риск потери данных.
- Брокер (для CeleryExecutor): например, Redis или RabbitMQ. Они могут быть настроены в кластерном режиме across data centers, но это сложно и может привести к проблемам с производительностью. Обычно брокер размещают в одном датацентре, а в другом используют bridge или replica, но это не является настоящим активным-активным.
- Веб-серверы и планировщики: могут быть запущены в обоих датацентрах, но они должны подключаться к единой базе данных и брокеру. Если база данных и брокер находятся в одном датацентре, то компоненты в другом датацентре будут испытывать задержки.

Для обеспечения высокой доступности в нескольких датацентрах, чаще всего используется активный-пассивный режим с асинхронной репликацией базы данных и регулярльными резервными копиями. В случае сбоя, пассивный кластер активируется вручную, так как автоматическое переключение может быть рискованным из-за возможной потери данных при асинхронной репликации.

<h4>Аутентификация и авторизация</h4>

Аутентификация по умолчанию отключена (в версиях до 1.10) или базовая аутентификация (в более новых версиях, но не рекомендуется для продакшена).

Поддерживаемые методы аутентификации:
- Password: хранение паролей в базе Airflow (используя хеши).
- LDAP/Active Directory: интеграция с корпоративными каталогами.
- OAuth: через поставщиков, таких как Google, GitHub, Azure AD и др.
- OpenID Connect: поддержка стандарта OpenID Connect.
- Remote User: аутентификация через веб-сервер (например, с помощью Kerberos или SSO).

Настройка аутентификации осуществляется в `airflow.cfg` путем указания соответствующего бэкенда и его параметров. Пример для OAuth (например, с Google):
```bash
[api]
auth_backends = airflow.api.auth.backend.oauth

[webserver]
authenticate = True
auth_backend = airflow.contrib.auth.backends.google_auth
```
И затем необходимо задать переменные окружения или в конфиге для OAuth клиента.


Airflow имеет встроенную модель ролей для управления доступом. Существует несколько предопределенных ролей с разными уровнями прав:
- Admin: полный доступ.
- User: может создавать и запускать DAG, но не может управлять пользователями и настройками.
- Op: может запускать DAG, но не может их создавать.
- Viewer: только просмотр.
- Public: минимальные права (обычно только просмотр определенных DAG, если разрешено).

Также можно создавать собственные роли.

В Airflow 2.0 и выше появилась более гибкая система управления доступом на основе ролей (RBAC), которая по умолчаванию использует FAB (Flask AppBuilder). Она предоставляет более тонкие настройки прав.

Настройка авторизации:
- Через веб-интерфейс: администратор может назначать роли пользователям.
- Через конфигурационные файлы: можно настроить отображение ролей и прав.
- Для более сложных сценариев можно использовать кастомные политики безопасности, интегрируя Airflow с внешними системами управления доступом.

Пример настройки аутентификации через LDAP:
```bash
[webserver]
authenticate = True
auth_backend = airflow.contrib.auth.backends.ldap_auth

[ldap]
uri = ldap://ldap.example.com
user_filter = objectClass=person
user_name_attr = uid
bind_user = cn=admin,dc=example,dc=com
bind_password = password
basedn = dc=example,dc=com
Важно: при настройке аутентификации и авторизации необходимо обеспечить безопасность передаваемых данных (использовать TLS/SSL) и хранимых учетных данных.
```

Для продакшен-среды обязательно настраивать аутентификацию и авторизацию. Выбор метода аутентификации зависит от инфраструктуры организации. Наиболее популярны OAuth и LDAP. Для авторизации используйте встроенные роли или настраивайте кастомные в соответствии с политиками доступа.