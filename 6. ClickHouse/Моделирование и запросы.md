<h2>Моделирование и запросы</h2>
<h3>1. Моделирование схемы</h3>
<h4>Сжатие данных</h4>

ClickHouse сжимает данные по столбцам отдельно, что значительно эффективнее сжатия по строкам, так как в одном столбце данные однородны. По умолчанию используется кодек `LZ4`. Данные разбиваются на гранулы (например, по 64КБ-1МБ несжатых данных) и сжимаются независимо. Это обеспечивает баланс между скоростью и степенью сжатия.

Кодеки применяются к конкретным столбцам и бывают двух видов: общего назначения и специализированные.

Кодеки общего назначения (указываются при создании таблицы):
- `LZ4` — очень высокая скорость сжатия/распаковки, умеренная степень сжатия. Идеален для большинства случаев.
- `ZSTD` — отличный баланс: степень сжатия значительно выше, чем у `LZ4`, при приемлемой скорости. Рекомендуется для холодных данных или сильно сжимаемых текстовых полей.
- `Brotli`, `LZMAHC` — максимальная степень сжатия, но очень низкая скорость. Только для архивных данных, к которым обращаются редко.

Пример:
```sql
CREATE TABLE logs (
    message String CODEC(ZSTD(3))
) ENGINE = MergeTree ...
```

Специализированные кодеки (для снижения объёма и ускорения запросов):
- `Delta(delta_bytes)` — эффективен для монотонно изменяющихся данных. Хранит разницу между соседними значениями.
- `DoubleDelta` — вычисляет разницу разностей. Идеален для плавных рядов: метрики, временные ряды, DateTime.
- `Gorilla` (или `FPC` для `Float`) — специализирован для чисел с плавающей точкой в временных рядах.
- `T64` — пытается уменьшить разрядность целочисленных данных.
- `GCD` — эффективен для чисел, кратных некому делителю (например, цены в валюте).

Часто используют комбинацию специализированного и общего кодеков: сначала применяется специализированный (например, `DoubleDelta`), который преобразует данные в более сжимаемый вид, а затем общий (например, `ZSTD`).

Пример для метрик:
```sql
CREATE TABLE metrics (
    ts DateTime64(3) CODEC(DoubleDelta, LZ4),
    value Float64 CODEC(Gorilla, ZSTD)
) ENGINE = MergeTree ...
```

<h4>Проектирование схемы данных и денормализация</h4>

Принципы проектирования:
- Лучше широкие таблицы, чем джойны: ClickHouse оптимизирован для денормализованных данных. Объединение при запросах — самая слабая сторона СУБД, особенно для больших таблиц. Данные должны быть подготовлены для вставки.
- Столбцы, а не строки: Запрос должен читать минимальное количество столбцов. Лишние столбцы, не участвующие в запросах, — это бесполезные затраты на ввод-вывод и память.
- Ключ партиционирования (PARTITION BY): Разбивает таблицу на логические части (по дате, региону). Идеальный размер партиции — от 1 до 100+ ГБ. Партиционирование ускоряет удаление данных (`DROP PARTITION`) и управление TTL.
- Ключ сортировки (ORDER BY): Определяет, как данные физически упорядочены на диске. Порядок столбцов должен соответствовать предикатам (`WHERE`) и группировкам (`GROUP BY`) наиболее частых запросов. Первыми идут столбцы с высокой селективностью, затем менее селективные.
- Первичный ключ (PRIMARY KEY): В MergeTree-движках — это префикс ключа сортировки. Он используется для индексации (разметка в `primary.idx`). Не обязан быть уникальным. Это инструмент быстрого поиска по диапазону.
- Индексы (INDEX): Пропускающие индексы (Data Skipping Indexes) — позволяют пропускать гранулы данных при чтении. Типы: `minmax`, `set`, `ngrambf`, `tokenbf_v1` (для текста), `bloom_filter`. Создаются для столбцов, которые часто фигурируют в условиях WHERE, но не входят в начало ключа сортировки.
- Использовать подходящие движки:
  - `MergeTree` — основа для 99% таблиц (ReplicatedMergeTree для кластера).
  - `SummingMergeTree`, `AggregatingMergeTree`, `CollapsingMergeTree` — для предварительно агрегированных или изменяемых данных.
  - `Log`/`TinyLog` — для мелких временных данных.
- TTL (Time To Live):
  - Позволяет автоматически удалять или перемещать устаревшие данные на другой диск/том.
  - Критически важный инструмент управления жизненным циклом данных.

Стратегии денормализации:
- Простое сглаживание (Flattening): Вместо хранения `orders` и `order_items` отдельно, создаётся одна таблица `order_items_denorm`, где каждый товар в заказе — это строка, дополненная всеми полями из `orders`. Запросы на анализ товарооборота не требуют `JOIN`.
- Использование вложенных структур (`Nested` или `Array/Tuple`): Для случаев, когда у одной сущности есть несколько повторяющихся атрибутов. Например, если событие содержит массив тегов, или заказ содержит массив товаров с их ценами и количествами. Работа с массивами требует специальных функций (`arrayMap`, `arrayFilter`), но позволяет избежать взрывного роста строк.
- Использование словарей (С помощью `LowCardinality` или внешних словарей): Не всегда нужно дублировать длинные строки. Можно хранить `region_id UInt8` и иметь отдельную небольшую таблицу-справочник `regions`. ClickHouse умеет встраивать словари в запросы (`region_name` подставляется из `regions` по `region_id`). Или использовать `LowCardinality(String)` — словарь создаётся автоматически внутри столбца.
- Агрегация на вставке (С помощью движков SummingMergeTree/AggregatingMergeTree): Крайняя степень денормализации, когда данные агрегируются сразу при вставке. Например, Вместо обычных записей, в целевой таблице сразу вставляются посчитанные по минутам агрегаты.
- Материализованные представления: Инструмент для автоматической денормализации и агрегации потоков данных. Создаётся представление с целевой структурой и целевым запросом. При вставке в исходную таблицу данные автоматически преобразуются и попадают в целевую.

<h4>Индексы скип-листов</h4>

Индексы скип-листов (Data Skipping Indexes) в ClickHouse — это механизм, позволяющий пропускать чтение блоков данных (гранул), которые гарантированно не содержат искомых значений, что ускоряет выполнение запросов. Данные в таблицах семейства MergeTree разбиты на гранулы (блоки строк, размер которых определяется настройками `index_granularity`, обычно 8192 строки). Для каждой гранулы индекс скип-листа хранит некоторую метаинформацию (в зависимости от типа индекса) о значениях в этой грануле. При выполнении запроса ClickHouse проверяет условия запроса по индексу скип-листа и пропускает гранулы, которые не удовлетворяют условию.

Типы индексов скип-листов:
- `minmax` — хранит минимальное и максимальное значение в грануле. Эффективен для столбцов с коррелированным порядком (например, дата). Позволяет пропускать гранулы, если искомый диапазон не пересекается с `[min, max]` гранулы.
- `set(max_rows)` — хранит множество уникальных значений в грануле (до `max_rows`). Полезен для столбцов с низкой кардинальностью. Если искомое значение не входит в это множество, гранула пропускается.
- `ngrambf_v1(n, size_of_bloom_filter_in_bytes, number_of_hash_functions, random_seed)` — индекс на основе фильтра Блума для поиска подстрок. Параметр n — длина n-граммы (для разбиения строки на токены). Эффективен для текстового поиска.
- `tokenbf_v1(size_of_bloom_filter_in_bytes, number_of_hash_functions, random_seed)` — аналогичен `ngrambf_v1`, но разбивает строку на токены (слова) по пробелам и знакам пунктуации.
- `bloom_filter([false_positive])` — общий фильтр Блума для значений столбца. Эффективен для произвольных условий. Параметр `false_positive` — вероятность ложного срабатывания (от 0 до 1, по умолчанию 0.025).

Индекс определяется при создании таблицы (или добавляется позже) в секции INDEX:
```sql
-- Таблица с несколькими типами индексов
CREATE TABLE logs (
    timestamp DateTime,
    user_id UInt32,
    url String,
    response_code UInt16,
    message String,

    -- minmax индекс для временных меток
    INDEX idx_timestamp timestamp TYPE minmax GRANULARITY 4,

    -- set индекс для кодов ответа (низкая кардинальность)
    INDEX idx_response response_code TYPE set(100) GRANULARITY 2,

    -- bloom фильтр для поиска по user_id
    INDEX idx_user user_id TYPE bloom_filter GRANULARITY 4,

    -- tokenbf индекс для поиска по словам в сообщении
    INDEX idx_message message TYPE tokenbf_v1(32768, 3, 0) GRANULARITY 4
)
ENGINE = MergeTree
ORDER BY (timestamp, user_id)
```

Параметр `GRANULARITY` — сколько гранул объединяется в один индексный блок. Например, если `index_granularity` = 8192 и `GRANULARITY` = 4, то индекс будет строиться на каждые 32768 строк.

Индексы скип-листов не бесплатны: они занимают место на диске и требуют вычислений при запросах. Эффективность зависит от данных и запросов. Например, для высококардинальных столбцов `minmax` может быть бесполезен, а `bloom_filter` может помочь. Индексы скип-листов используются после первичного ключа (если он может отфильтровать гранулы, то скип-лист не нужен).

<h4>Обработка JOIN</h4>

`JOIN` в ClickHouse работает иначе, чем в традиционных реляционных базах данных, и требует особого внимания. Поддерживаемые типы `JOIN`: `INNER`, `LEFT OUTER`, `RIGHT OUTER`, `FULL OUTER`, `CROSS` (но `CROSS` без условия может быть очень тяжелым).

Стратегии выполнения:
- `hash join`: Правая таблица (subquery) читается и строится хэш-таблица в памяти. Затем читается левая таблица (main query) и происходит поиск по хэш-таблице.
- `grace hash join`: Если память недостаточна, ClickHouse может сбросить часть хэш-таблицы на диск и затем использовать дисковую версию.
- `partial merge join`: В некоторых случаях, когда данные отсортированы по ключу соединения, может использоваться слияние.

Ограничения и рекомендации:
- Порядок таблиц в `JOIN`: ClickHouse рекомендует иметь левую таблицу (ту, которая в FROM) как самую большую, а правые таблицы (в `JOIN`) — поменьше, потому что правая таблица полностью загружается в память (для `hash join`). Если правая таблица слишком велика, может не хватить памяти.
- Распределенные `JOIN`: В распределенных запросах `JOIN` может выполняться на каждой шарде отдельно (если данные правильно распределены) или на координаторе. Важно проектировать распределенные запросы так, чтобы минимизировать передачу данных по сети.
- Настройки: Существуют настройки для управления поведением `JOIN`, такие как `join_algorithm`, `max_rows_in_join`, `max_bytes_in_join`, `join_use_nulls` и т.д.

Советы по использованию `JOIN`:
- По возможности избегать `JOIN` в ClickHouse, особенно больших. Часто можно обойтись денормализацией (предварительно соединить таблицы и хранить их в одной широкой таблице).
- Если `JOIN` необходим, убедитесь, что правая таблица небольшая (например, таблица-справочник).
- Используйте необходимый тип `JOIN` (чаще всего `LEFT JOIN` или `INNER JOIN`).
- Для распределенных запросов используйте `GLOBAL JOIN`, если правая таблица находится на координаторе, а левая распределена. Это гарантирует, что правая таблица будет передана на все шарды.

<h4>Создание таблиц</h4>

Создание таблиц в ClickHouse — это важный процесс, определяющий эффективность хранения и выполнения запросов. Основной движок для большинства таблиц — семейство `MergeTree`. Синтаксис:

```sql
CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
(
    column1_name [column1_type] [DEFAULT|MATERIALIZED|ALIAS expr1] [CODEC(codec1)] [TTL expr1],
    column2_name [column2_type] [DEFAULT|MATERIALIZED|ALIAS expr2] [CODEC(codec2)] [TTL expr2],
    ...
) ENGINE = engine_name()
[PARTITION BY expr]
[ORDER BY expr]
[PRIMARY KEY expr]
[SAMPLE BY expr]
[TTL expr [DELETE|TO DISK 'xxx'|TO VOLUME 'xxx'], ...]
[SETTINGS name=value, ...]
```

Ключевые секции для `MergeTree`:
- `ENGINE` — указывает движок таблицы. Основные:
  - `MergeTree` — для нереплицированных данных.
  - `ReplicatedMergeTree` — для реплицированных данных в кластере.
  - `SummingMergeTree` — автоматически суммирует указанные числовые столбцы при слиянии кусков.
  - `AggregatingMergeTree` — для хранения предварительно агрегированных данных.
  - `CollapsingMergeTree` — для хранения изменяемых данных (с использованием специального столбца-флага).
- `PARTITION BY` — выражение для партиционирования. Обычно по дате (например, `toYYYYMM(date)`). Не рекомендуется создавать много мелких партиций.
- `ORDER BY` — ключ сортировки (обязательный параметр). Определяет, как данные будут расположены на диске. Это самый важный параметр для оптимизации запросов.
- `PRIMARY KEY` — первичный ключ (по умолчанию совпадает с ORDER BY, но может быть короче). Используется для индексации гранул.
- `SAMPLE BY` — выражение для семплирования. Если указано, то можно использовать SAMPLE в запросах.
- `TTL` — время жизни данных. Можно задать для строк (на уровне таблицы) или для столбцов.
- `SETTINGS` — дополнительные настройки таблицы, например:
  - `index_granularity` — количество строк между засечками индекса (по умолчанию 8192).
  - `storage_policy` — политика хранения (для многодисковых конфигураций).

Пример:
```sql
CREATE TABLE events
(
    event_id UInt64,
    event_date Date,
    user_id UInt32,
    event_type LowCardinality(String),
    details String CODEC(ZSTD(3))
)
ENGINE = MergeTree
PARTITION BY toYYYYMM(event_date)
ORDER BY (event_date, user_id, event_type)
SETTINGS index_granularity = 8192;
```

<h3>2. MergeTree таблицы</h3>
<h4>Основные концепции и преимущества</h4>

Данные постоянно добавляются в таблицу, а фоновый процесс периодически "мержит" (сливает) маленькие кусочки данных (куски, parts) в более крупные, упорядочивая их по первичному ключу и применяя другие оптимизации.

Как это работает:
- Вставка данных: При вставке данные попадают в оперативную память, затем формируются в новый кусок на диске. Каждый кусок — это отдельная папка с собственными данными (столбцы в сжатых `*.bin` файлах), первичным индексом и метаданными.
- Фоновое слияние (Merge): Асинхронно и автоматически ClickHouse находит соседние куски (по ключу сортировки или партициям) и сливает их в один больший, упорядоченный кусок. При слиянии данные пересортировываются, применяются правила агрегации (для AggregatingMergeTree), удаляются старые версии строк (для ReplacingMergeTree), старые куски впоследствии удаляются.

Ключевые преимущества:
- Хранение данных, отсортированных по первичному ключу: Позволяет эффективно находить данные по диапазонам ключей и быстро выполнять `GROUP BY`, `ORDER BY` по ключу.
- Индексирование: Первичный ключ создает разреженный первичный индекс (запись не на каждую строку, а на каждые N строк — "индексные гранулы"). Это позволяет быстро находить нужные блоки данных без чтения всего файла.
- Разбиение на партиции: Данные можно разделять по партициям. Удаление старой партиции — это мгновенная операция удаления папки.
- Сканирование по столбцам (Column-oriented): Данные каждого столбца хранятся отдельно, что позволяет при запросе читать только нужные столбцы, экономя I/O.
- Сжатие данных: Данные сжимаются по столбцам (обычно `LZ4` или `ZSTD`), что сильно экономит место, так как данные в одном столбце часто однородны.
- Высокая пропускная способность записи: Операции вставки добавляют новые куски, практически не блокируя чтение.

<h4>Роль и настройка первичных ключей</h4>

Первичный ключ в MergeTree не гарантирует уникальности строк (если не используется движок `CollapsingMergeTree` или `ReplacingMergeTree` с явным указанием версии). Его главная роль — определять порядок сортировки данных на диске и строить разреженный первичный индекс.

Ключ задается через `ORDER BY (column1, column2, ...)` при создании таблицы (если не указан `PRIMARY KEY`, то `ORDER BY` становится первичным ключом). Данные в каждом куске отсортированы по этому ключу. Для каждых 8192 строк (индексной гранулы) сохраняется значение ключа первой строки в грануле в индексном файле. При запросе с условием по ключу, ClickHouse бинарным поиском находит нужные гранулы и читает только их, пропуская остальные данные.

Настройка:
```sql
CREATE TABLE logs (
    event_time DateTime,
    user_id UInt32,
    event_type String,
    url String
) ENGINE = MergeTree
ORDER BY (toDate(event_time), user_id) # Первичный ключ и порядок сортировки
# Можно также явно указать PRIMARY KEY, который может быть короче, чем ORDER BY:
# PRIMARY KEY (toDate(event_time)) Индекс будет только по дате
PARTITION BY toYYYYMM(event_time);
```

Правила проектирования ключа:
- Первый столбец ключа должен быть наиболее часто используемым в фильтрах.
- Ключ должен эффективно сокращать объем читаемых данных. Часто это колонки с высокой кардинальностью.
- Не стоит добавлять слишком много столбцов в ключ, так как это увеличит размер индекса и может снизить эффективность.

<h4>Партиционирование и удаление данных в MergeTree таблицах</h4>

Партиционирование разделяет данные на логические части по значению указанного выражения (чаще всего — по дате: `PARTITION BY toYYYYMM(date)`). Каждая партиция хранится отдельно и состоит из одного или нескольких кусков. Партицирование может существенно ускорить запросы, которые фильтруют данные по этому ключу, а также позволяет быстро удалять данные. Слишком мелкие партиции создают много кусков, что ухудшает производительность запросов и замедляет слияния. Размер одной партиции должен быть не менее 10 ГБ.

Способы удаления данных:
- Удаление целых партиций (самый эффективный способ):
```sql
ALTER TABLE logs DROP PARTITION '202401';
```
- Условное удаление строк (менее эффективно, lickHouse помечает строки как удаленные и физически удаляет их при следующем слиянии кусков):
```sql
ALTER TABLE logs DELETE WHERE event_time < '2024-01-01';
```

В ClickHouse для таблиц семейства MergeTree реализован мощный механизм TTL, который удаляет или перемещает устаревшие данные. Основные типы TTL:
- TTL для строк: Удаление целых строк по времени.
- TTL для столбцов: Очистка значений в столбцах (установка в default).
- TTL для партиций: Перемещение партиций на другой носитель (например, с SSD на HDD).

Пример создания таблицы с TTL:
```sql
CREATE TABLE logs (
    timestamp DateTime,
    message String,
    user_id UInt32
) ENGINE = MergeTree
PARTITION BY toYYYYMM(timestamp)
ORDER BY (timestamp, user_id)
TTL timestamp + INTERVAL 3 MONTH -- удаление старых данных
    TO DISK 'hdd' -- сначала переместим на медленный диск
    TO VOLUME 'cold' -- затем на холодное хранилище
SETTINGS storage_policy = 'tiered'; -- политика хранения
```

Настройки для автоматической очистки:
- `ttl_only_drop_parts` - удалять целые партиции, если все данные в них устарели
- `merge_with_ttl_timeout` - периодичность проверки TTL (по умолчанию 14400 секунд)

<h4>Типы данных и ограничения для столбцов</h4>

ClickHouse обладает богатым набором типов данных, оптимизированных для хранения и обработки аналитических данных.

Ключевые группы типов данных:
- Числовые: `Int8`, `Int16`, `Int32`, `Int64`, `Int128`, `Int256`, `Float32`, `Float64`, `Decimal(P, S)`.
- Строковые: `String`, `FixedString(N)`.
- Дата и время: `Date`, `DateTime`, `DateTime64`.

Структурированные и вложенные:
- `Array(T)` — массив элементов типа T.
- `Map(K, V)` — словарь.
- `Tuple(T1, T2, ...)` — кортеж.
- `Nested` — позволяет создавать вложенные структуры. Например, `Nested(ProductID UInt32, Price Decimal(10,2))`.

Специализированные (самые важные для эффективности):
- `LowCardinality(T)` — обёртка для строк или чисел с малым числом уникальных значений.
- `Nullable(T)` — позволяет хранить `NULL`.
- `Enum8`, `Enum16` — хранят строки как числа.
- `IPv4`, `IPv6` — хранятся как числа, но отображаются в привычном строковом формате.
- `UUID` — компактное хранение 128-битного UUID.

Для обычных столбцов нет жестких ограничений схемы, как в PostgreSQL: Столбцы можно спокойно добавлять и удалять. Удаление столбца физически удаляет его файлы с диска.

Ограничения по ключевым столбцам (`ORDER BY`/`PRIMARY KEY`):
- Нельзя использовать столбцы с типом `Nullable`, если движок `MergeTree` не настроен специальным образом (в старых версиях было запрещено).
- Столбцы `Array`, `Nested` и `Tuple` нельзя включать в ключ.
- Рекомендуется использовать типы с фиксированным размером для первых столбцов ключа для более эффективной работы индекса.
- Ограничения по производительности и памяти: Использование `Nullable` увеличивает накладные расходы на хранение. Типы `String` для часто фильтруемых столбцов менее эффективны, чем `LowCardinality(String)` или `Enum`.
- Индексы пропуска данных: Для неключевых столбцов можно создавать дополнительные индексы, чтобы ускорить фильтрацию. Они работают на уровне гранул.

<h4>Автоматическое сжатие данных и оптимальная гранулярность индекса</h4>

ClickHouse автоматически сжимает данные при записи на диск. Каждый столбец в каждом куске данных хранится в собственном файле, который сжимается отдельно. Это позволяет достигать высокой степени сжатия, особенно потому, что данные в столбцах часто однородны. ClickHouse поддерживает несколько алгоритмов сжатия, по умолчанию используется `LZ4`, но можно выбрать и другие, например, `ZSTD`, который обеспечивает лучшее сжатие, но требует больше ресурсов CPU.

Гранулярность индекса — это параметр, который определяет количество строк в одной индексной грануле. По умолчанию он равен 8192. Первичный индекс (который строится по первичному ключу) хранит одну запись на каждую гранулу. Таким образом, индекс является разреженным и позволяет быстро находить гранулы, которые могут содержать искомые данные.

Оптимальный размер гранулы зависит от сценария использования:
- Меньшие гранулы (например, 1024 или 2048 строк) могут ускорить точечные запросы (когда нужно найти несколько строк), потому что индекс будет более плотным, и при запросе будет прочитано меньше лишних данных. Однако это увеличит размер индекса в памяти и может замедлить запросы, которые читают большие диапазоны.
- Большие гранулы (например, 16384 или 32768) уменьшают размер индекса и ускоряют запросы по большим диапазонам, но могут читать больше лишних данных при точечных запросах.

Выбор размера гранулы — это компромисс между точностью индекса и его объемом. Чаще всего оставляют значение по умолчанию, но если у вас есть частые точечные запросы по первичному ключу, возможно, стоит уменьшить размер гранулы. Однако перед изменением этого параметра важно провести тестирование на своих данных и запросах.

Настройка гранулярности индекса выполняется при создании таблицы с помощью параметра `index_granularity` в настройках движка. Например:

```sql
CREATE TABLE my_table
(
    ...
) ENGINE = MergeTree
ORDER BY ...
SETTINGS index_granularity = 1024;
```

Также можно изменить этот параметр для уже существующей таблицы, но это повлечет за собой перестройку всех данных (нужно использовать запрос `ALTER TABLE ... MODIFY SETTING` с версии 20.4, но обычно такие изменения делают на этапе проектирования таблицы).

<h4>Enum и LowCardinality</h4>

Тип данных `Enum` предназначен для хранения строк, которые принимают ограниченный набор значений. Внутри ClickHouse хранит их как числа (`UInt8`, `UInt16` и т.д., в зависимости от количества возможных значений), но при этом позволяет работать с ними как со строками. Преимущества:
- Экономия места на диске и в оперативной памяти, так как хранится число, а не строка.
- Ускорение запросов, потому что сравнение чисел происходит быстрее, чем строк.

Тип данных `LowCardinality` — это более универсальное решение для столбцов с небольшим количеством уникальных значений (но их набор может меняться). `LowCardinality` преобразует столбец в словарь (dictionary) с числовыми ключами, но в отличие от `Enum`, словарь может обновляться при вставке новых значений. Преимущества:
- Также экономит место и ускоряет обработку данных.
- Более гибкий, чем `Enum`, потому что не требует предварительного определения всех возможных значений.
- Особенно эффективен для столбцов, которые используются в группировках и фильтрах, так как работа с числами быстрее.

Оба типа данных позволяют сжимать данные лучше, чем обычные строки, и ускоряют запросы. Однако важно не использовать их для столбцов с высокой кардинальностью (много уникальных значений), потому что это, наоборот, может замедлить работу и занять больше места из-за накладных расходов на словарь.

При вставке данных ClickHouse автоматически строит словарь для `LowCardinality` столбца. Если в столбце много уникальных значений, то словарь становится большим, и эффективность типа `LowCardinality` снижается. Для `LowCardinality` столбцов можно использовать различные кодеки сжатия. По умолчанию используется кодек, который эффективно сжимает числовые данные. При использовании `LowCardinality` в первичном ключе (ORDER BY) или в партиционировании, ClickHouse работает с числовым представлением, что обычно ускоряет сравнения.

<h4>Механизмы дедупликации</h4>

ClickHouse не имеет встроенных ограничений уникальности, но предлагает несколько подходов:
- `ReplacingMergeTree`: Удаляет дубликаты при слиянии партиций по ключу сортировки:
```sql
CREATE TABLE dedup_table (
    id UInt32,
    timestamp DateTime,
    value Float64
) ENGINE = ReplacingMergeTree
PARTITION BY toYYYYMM(timestamp)
ORDER BY (id, timestamp)
PRIMARY KEY id;
```
Дедупликация происходит только при слияниях. Следует Использовать `FINAL` в запросах или `OPTIMIZE TABLE` для принудительной дедупликации.
- `CollapsingMergeTree`: Использует специальный столбец `sign` для отметки удаленных строк:
```sql
CREATE TABLE collapsing_table (
    id UInt32,
    value Float64,
    sign Int8
) ENGINE = CollapsingMergeTree(sign)
ORDER BY id;
```
- `VersionedCollapsingMergeTree`: Улучшенная версия с версионностью:
```sql
CREATE TABLE versioned_table (
    id UInt32,
    value Float64,
    sign Int8,
    version UInt32
) ENGINE = VersionedCollapsingMergeTree(sign, version)
ORDER BY id;
```
- Дедупликация на уровне вставки:
```sql
INSERT INTO table SELECT ... FROM source_table
GROUP BY all_columns; -- агрегация перед вставкой
```

<h4>Особенности и применение различных типов таблиц</h4>

`SummingMergeTree`:
- Особенности: Автоматически суммирует значения числовых столбцов, не входящих в первичный ключ (`ORDER BY`), для всех строк с одинаковым значением ключа сортировки. Остальные нечисловые или не указанные явно столбцы берутся из первой попавшейся строки в группе (значение может быть произвольным).
- Применение: Идеален для предварительной агрегации потоковых данных, где важна сумма показателей. Позволяет значительно уменьшить объем хранимых данных и ускорить итоговые агрегационные запросы. Например, позволяет хранить ежесекундные события с последующей сверткой в минутные или часовые итоги.
- Оптимизация:
  - Гранулярность первичного ключа (`ORDER BY`): Включать в ключ сортировки только те столбцы, по которым нужно гарантировать уникальность строк после суммирования. Обычно это временные интервалы, идентификаторы измерений. Чем выше гранулярность, тем сильнее сжатие.
  - Явное указание суммируемых столбцов: При создании таблицы можно задать `SummingMergeTree([columns])`. Это предотвратит неявное суммирование других числовых полей, которые должны оставаться уникальными (например, хэши).
  - Использование Merge для финальной агрегации: Данные в SummingMergeTree могут быть не полностью свернуты. Следует использовать `sum(column) ... GROUP BY key`. Сам движок гарантирует, что финальная агрегация будет быстрой, так как число строк для обработки уже значительно уменьшено.
  - Комбинирование с партиционированием и TTL: Использовать PARTITION BY для управления данными. Добавлять TTL для автоматического удаления устаревших сырых данных, оставляя только предрассчитанные агрегаты за длительные периоды.
  - Предварительная агрегация на стороне вставки: Вместо вставки каждой микро-записи, можно агрегировать данные в батчи на стороне приложения (или с помощью буферизованных таблиц) и вставлять в `SummingMergeTree` уже частично агрегированные пакеты.
  - Выбор эффективных типов данных: Для суммируемых столбцов использовать наименьший подходящий тип (`UInt32` вместо `UInt64`, `Decimal` с нужной точностью). Это экономит место на диске и в памяти.

`ReplacingMergeTree`:
- Особенности: Оставляет только последнюю версию строки в группе с одинаковым первичным ключом. Понятие "последняя" определяется столбцом/столбцами версии (указаны при создании таблицы), или, если версия не задана, то по времени вставки (последняя вставленная в рамках незавершенного слияния). 
- Применение: Применяется для дедупликации данных "по последнему значению". Классический пример — хранение текущего состояния сущности (последний известный баланс пользователя, последние отправленные данные с датчика). Дедупликация происходит асинхронно во время фоновых слияний, поэтому для получения актуального состояния `SELECT` требует использования модификатора `FINAL` или агрегации `argMax`.
- Оптимизация:
  - Выбор столбца версии: Использовать монотонно возрастающее значение (например, `timestamp`, `version_number`, `binlog_offset`). Это гарантирует корректное определение "последней" записи.
  - Избегание FINAL в продакшене: Не использовать FINAL для больших таблиц. Вместо этого строить запросы с явным `GROUP BY` и `argMax` или проектировать систему так, чтобы запросы работали с учетом асинхронности (допускали небольшую задержку в актуальности).
  - Управление слияниями: Настройка ключа партицирования и TTL помогает своевременно удалять устаревшие данные и управлять фонными операциями.

`AggregatingMergeTree`: 
- Особенности: Не агрегирует простые значения, а хранит и объединяет состояния агрегатных функций (комбинаторы `-State`, `-SimpleState`). Для работы с ним используются материализованные представления с `AggregatingMergeTree` и специальные функции `-Merge`, `-MergeState`. 
- Применение: Применяется для создания инкрементальных материализованных представлений для сложных, многоэтапных агрегаций. Позволяет предрассчитывать и хранить такие метрики, как уникальные значения, перцентили, сложные суммы произведений и т.д., с возможностью их дальнейшего объединения. Например, для создания куба данных с предварительным расчетом количества событий, уникальных пользователей и медианной длительности для каждой комбинации (день, страна, событие).
- Оптимизация:
  - Правильное использование материализованных представлений (MV): `AggregatingMergeTree` почти всегда используется внутри MV. Шаблон:
  - Исходная таблица — любой движок (часто MergeTree). MV создается с движком `AggregatingMergeTree` и `POPULATE` (при первом создании) или данными начинают поступать после создания. В MV вставляются не raw-данные, а состояния агрегатных функций с помощью `toState`-функций.
  - Оптимизация гранулярности: Агрегировать данные до оптимального уровня детализации (например, до минуты или часа, а не до секунды). Это резко уменьшает количество состояний для хранения и объединения. Ключ сортировки (`ORDER BY`) в такой таблице должен соответствовать измерениям куба (например, (`day`, `event_type`, `country`)).
  - Выбор агрегатных функций: Использовать SimpleState-комбинаторы где возможно (например, sumSimpleState, anySimpleState). Они работают быстрее, но поддерживают меньше функций.
  - Эффективные запросы к MV: Для выборки итоговых данных из MV использовать функции `-Merge` (например, `uniqCombinedMerge(state)`). Запрос должен агрегировать по тем же ключам, что и `GROUP BY` в MV.

<h3>3. Обработка и анализ данных</h3>
<h4>Основы SQL</h4>

ClickHouse поддерживает стандартный SQL с некоторыми расширениями и особенностями.

Особенности и расширения SQL в ClickHouse:
- Строгая типизация — все данные имеют явный тип, преобразования не всегда автоматические.
- Богатый набор функций — строковые, математические, функции для работы с массивами, JSON, геоданными, машинного обучения и т.д.
- Работа с массивами и вложенными структурами — есть специальные функции и операторы.
- Модификации данных — хотя ClickHouse оптимизирован для чтения, поддерживаются INSERT, UPDATE, DELETE (последние две — мутации, которые выполняются асинхронно и тяжелы).
- Материализованные представления — позволяют автоматически обрабатывать данные при вставке.

Синтаксис `SELECT`:
```sql
SELECT [DISTINCT] expr_list
[FROM [db.]table | (subquery) | table_function] [FINAL]
[SAMPLE sample_coeff]
[GLOBAL] [ANY|ALL|ASOF] [INNER|LEFT|RIGHT|FULL|CROSS] [OUTER|SEMI|ANTI] JOIN (subquery)|table (ON <expr_list>)|(USING <column_list>)
[PREWHERE expr]
[WHERE expr]
[GROUP BY expr_list] [WITH ROLLUP|WITH CUBE] [WITH TOTALS]
[HAVING expr]
[ORDER BY expr_list] [WITH FILL] [FROM expr] [TO expr] [STEP expr]
[LIMIT [offset_value, ]n BY columns] | [n [WITH TIES]]
[LIMIT [n, ]m] | [OFFSET n ROWS FETCH NEXT m ROWS ONLY]
[SETTINGS ...]
```

Особенности:
- `PREWHERE` — оптимизация, при которой сначала читаются только столбцы, нужные для фильтрации, затем остальные.
- `SAMPLE` — семплирование данных (только для таблиц с движком, поддерживающим семплирование).
- `FINAL` — при запросе к таблице типа `CollapsingMergeTree` или `SummingMergeTree` применяет свёртку данных.

Синтаксис `INSERT`:
```sql
INSERT INTO [db.]table [(column1, column2, ...)] VALUES (v11, v12, ...), (v21, v22, ...), ...
INSERT INTO [db.]table [(column1, column2, ...)] SELECT ...
```

Синтаксис `DELETE` и `UPDATE`:
```sql
ALTER TABLE [db.]table DELETE WHERE filter_expr;
ALTER TABLE [db.]table UPDATE column1 = expr1 [, ...] WHERE filter_expr;
```

Управление таблицами:
```sql
CREATE TABLE ...
ALTER TABLE ... ADD|DROP|CLEAR|COMMENT|MODIFY COLUMN ...
DROP TABLE ...
RENAME TABLE ...
```

Простой запрос с агрегацией:
```sql
SELECT toStartOfHour(event_date) AS hour,
       event_type,
       count() AS events_count,
       uniq(user_id) AS unique_users
  FROM events
 WHERE event_date >= today() - 7
 GROUP BY hour, event_type
 ORDER BY hour, event_type
```

Использование `JOIN`:
```sql
SELECT event_date,
       segment,
       count() AS events
  FROM events
       LEFT JOIN users ON events.user_id = users.id
 GROUP BY event_date, segment
```

Использование оконных функций:
```sql
SELECT user_id,
       event_date,
       rank() OVER (PARTITION BY user_id ORDER BY event_date) AS rank
  FROM events
```

Фильтрация с подзапросом:
```sql
SELECT user_id, name
  FROM users
 WHERE user_id IN (
       SELECT DISTINCT user_id
         FROM purchases
        WHERE amount > 1000);
```

<h4>Агрегатные функции</h4>

ClickHouse имеет огромный набор агрегатных функций, которые можно разделить на несколько категорий:
- Базовые статистические:
  - `count()` — подсчет строк.
  - `sum()` — сумма.
  - `avg()` — среднее.
  - `min()` / `max()` — минимум/максимум.
  - `varPop()` / `stddevPop()` — дисперсия и стандартное отклонение по генеральной совокупности.
- Приблизительные:
  - `uniq()` / `uniqCombined()` — приблизительное количество уникальных значений.
  - `quantile()` / `quantiles()` — приблизительные квантили.
- Точные (могут быть тяжелыми на больших данных):
  - `uniqExact()` — точный подсчет уникальных значений (аналог `COUNT(DISTINCT ...)` в стандартном SQL).
  - `quantileExact()` — точные квантили.
- Агрегаты по массивам:
  - `groupArray()` — собирает значения в массив.
  - `groupUniqArray()` — собирает уникальные значения в массив.
- Условные агрегаты:
  - `sumIf(column, cond)` — сумма значений, удовлетворяющих условию.
  - `countIf(cond)` — подсчет значений, удовлетворяющих условию.
  - `avgIf(column, cond)` — среднее значений, удовлетворяющих условию.
- Комбинаторные модификаторы:
  - `aggFunctionDistinct()` — например, `sumDistinct(column)`, `countDistinct(column)`.
  - `aggFunctionIf()` — например, `avgIf(column, condition)`.
  - `aggFunctionArray()` — например, `sumArray(arr_column)`.

Также можно создавать пользовательские агрегатные функции, например, создадим функцию, рассчитывающую среднее геометрическое:
```sql
CREATE AGGREGATING FUNCTION geometricMean AS (x) -> exp(avg(log(x)));
SELECT geometricMean(value) FROM table;
```

<h4>Оконные функции</h4>

Поддержка оконных функций появилась в релизе 21.3 и активно развивается. Синтаксис близок к стандарту SQL.

Функции смещения:
- `lagInFrame(column, offset [, default])` — значение из строки, отстающей на `offset` в рамках окна.
- `leadInFrame(column, offset [, default])` — значение из строки, опережающей на `offset`.
- `first_value(column)` / `last_value(column)` — первое/последнее значение в окне.

Ранжирующие:
- `row_number()` — порядковый номер строки в окне.
- `rank()` / `dense_rank()` — ранг строки с пропусками/без пропусков.
- `ntile(n)` — разбивает строки окна на n групп.

Агрегатные как оконные: Любую агрегатную функцию можно использовать в окне.
- `sum(column) OVER (PARTITION BY id ORDER BY date)`
- `avg(column) OVER (PARTITION BY group)`
- `uniq(column) OVER (...)`

ClickHouse требует явного указания границ окна (ORDER BY в определении окна) для многих функций. Также есть две формы:
- `lag(column)` — относительно текущей строки всего набора.
- `lagInFrame(column)` — относительно текущей строки в пределах окна (`PARTITION BY` + `ORDER BY`).

Применение оконной функции:
```sql
SELECT date,
       amount,
       sum(amount) OVER (ORDER BY date) AS cumulative_sum
  FROM sales
 ORDER BY date;
```

То же самое с `runningAccumulate()`:
```sql
SELECT date,
       amount,
       runningAccumulate(amount) AS cumulative_sum
  FROM (SELECT date,
              amount
         FROM sales
        ORDER BY date)
 ORDER BY date;
```

<h4>Работа с массивами</h4>

Массивы в ClickHouse — это полноценный и эффективный тип данных `Array(T)`, где `T` — тип элементов (например, `Array(Int32)`, `Array(String)`, даже `Array(Array(String)))`.

Ключевые особенности:
- Производительность: Оптимизированы для хранения и обработки в колоночном формате. Операции над массивами (особенно в агрегатных функциях) работают очень быстро.
- Строгая типизация: Все элементы массива должны быть одного типа.
- Динамический размер: Размер массива не фиксирован при создании таблицы, но есть практические ограничения по объему памяти.
- Вложенность: Поддерживаются многомерные массивы `(Array(Array(T)))`.
- Богатый набор функций: Более 50 функций для работы с массивами: создание, фильтрация, сортировка, поиск, и т.д.
- Использование для денормализации: Частая практика — хранить историю событий или теги объекта не в отдельной нормализованной таблице, а в виде массива в основной таблице. Это резко сокращает количество JOIN-ов и ускоряет многие запросы.

Пример таблицы, в которой для каждого пользователя и дня хранятся массивы ID сессий и просмотренных страниц:
```sql
CREATE TABLE user_sessions
(
    user_id UInt64,
    date Date,
    session_ids Array(UInt32),
    pageviews Array(String)
) ENGINE = MergeTree()
ORDER BY (user_id, date);
```

Функции для работы с массивами:
- Создание массивов:
  - `array(elem1, elem2, ...)` или `[elem1, elem2, ...]` - создание массива
  - `range(end), range(start, end)` - массив чисел
  - `arrayWithConstant(n, value)` - массив из n одинаковых значений
  - `arrayPopFront(array), arrayPopBack(array)` - удаление первого/последнего элемента
  - `arrayPushFront(array, elem)`, `arrayPushBack(array, elem)` - добавление элемента
  - `arrayResize(array, size[, value])` - изменение размера массива
- Преобразование типов:
  - `arrayConcat(arr1, arr2, ...)` - объединение массивов
  - `arraySlice(array, offset[, length])` - получение среза
  - `arrayReverse(array)` - обратный порядок
  - `arrayFlatten(array_of_arrays)` - сглаживание многомерного массива
  - `arrayCompact(array)` - удаление последовательных дубликатов
  - `arrayZip(arr1, arr2, ...)` - создание массива кортежей
- Поиск и проверка:
  - `has(array, elem)` - проверка наличия элемента
  - `hasAny(array1, array2)` - есть ли общие элементы
  - `hasAll(array1, array2)` - содержит ли все элементы
  - `indexOf(array, elem)` - индекс первого вхождения
  - `countEqual(array, value)` - количество элементов равных value
  - `arrayEnumerate(array)` - массив индексов [1,2,3,...]
  - `arrayEnumerateUniq(array)` - нумерация уникальных значений
- Сортировка и фильтрация:
  - `arraySort(array)` - сортировка по возрастанию
  - `arraySort(func, array)` - сортировка с пользовательской функцией
  - `arrayReverseSort(array)` - сортировка по убыванию
  - `arrayDistinct(array)` - уникальные значения
  - `arrayFilter(func, array)` - фильтрация с помощью лямбда-функции
  - `arrayFirst(func, array)` - первый элемент, удовлетворяющий условию
  - `arrayLast(func, array)` - последний элемент, удовлетворяющий условию
- Высшего порядка (лямбда-функции):
  - `arrayMap(func, array1 [, array2, ...])` - применение функции к элементам
  - `arrayFilter(func, array)` - фильтрация
  - `arrayCount(func, array)`- подсчет элементов по условию
  - `arrayExists(func, array)` - существует ли элемент по условию
  - `arrayAll(func, array)` - все ли элементы удовлетворяют условию
  - `arraySum(array)` - сумма элементов
  - `arrayAvg(array)` - среднее значение
  - `arrayCumSum(array)` - кумулятивная сумма
- Агрегатные функции:
  - `groupArray(x)` - агрегация значений в массив
  - `groupArrayArray(arr)` - агрегация массивов в один
  - `groupUniqArray(x)` - агрегация уникальных значений
  - `arrayReduce('agg_func', array)` - применение агрегатной функции к массиву
  - `arrayReduceInRanges('agg_func', ranges, array)` - агрегация по диапазонам
- Работа со строками и массивами:
  - `splitByChar(delimiter, string)` - разбиение строки на массив
  - `splitByString(delimiter, string)` - разбиение по подстроке
  - `extractAll(text, regexp)` - извлечение всех совпадений с regexp
  - `arrayStringConcat(array[, delimiter])` - объединение массива строк
- Математические операции:
  - `arrayProduct(array)` - произведение элементов
  - `arrayDifference(array)` - разности соседних элементов
  - `arrayCumSum(array)` - накопленная сумма
  - `arrayCumSumNonNegative(array)` - накопленная сумма (не отрицательная)
- Специальные функции:
  - `arrayJoin(array)` - разворот массива в строки
  - `arrayIntersect(arr1, arr2, ...)` - пересечение массивов
  - `arrayMax(array)` - максимальный элемент
  - `arrayMin(array)` - минимальный элемент
  - `arrayUniq(array)` - количество уникальных элементов
  - `arrayCount(array)` - количество элементов (аналог length)
- Для многомерных массивов:
  - `arrayFlatten(array_of_arrays)` - преобразование в одномерный
  - `arrayMap(func, arr1, ...)` - работает с несколькими массивами
  - `arrayZip(arr1, arr2, ...)` - создание массива кортежей

Примеры:
```sql
SELECT array(1,2,3) AS arr, [4,5,6] AS arr2;
SELECT arr[1] FROM (SELECT [10,20,30] AS arr);
SELECT has([1,2,3], 2)                    -- возвращает 1.
SELECT slice([1,2,3,4,5], 2, 3)           -- возвращает [2,3,4].
SELECT arrayPushBack([1,2], 3)            -- возвращает [1,2,3].
SELECT arraySort([3,1,2])                 -- возвращает [1,2,3].
SELECT groupArray(number) FROM numbers(5) -- возвращает [0,1,2,3,4].
SELECT arrayMap(x -> x * 2, [1,2,3])      -- возвращает [2,4,6].
SELECT arrayEnumerate([10,20,10,30])      -- возвращает [1,2,3,4].
```

Разворот массива с помощью `arrayJoin()`:
```sql
SELECT
    user_id,
    date,
    arrayJoin(session_ids) AS session_id
FROM user_sessions
WHERE date = today()

┌─user_id─┬─date───────┬─session_id─┐
│     101 │ 2023-10-26 │       5001 │
│     101 │ 2023-10-26 │       5002 │
│     101 │ 2023-10-26 │       5003 │
└─────────┴────────────┴────────────┘
```

Сворачивание в массив с помощью `groupArray()`:
```sql
SELECT
    user_id,
    toDate(event_time) AS date,
    groupArray(page_url) AS pageviews,                -- Массив всех URL
    groupArrayDistinct(page_url) AS unique_pageviews, -- Массив уникальных URL
    count() AS pageview_count
FROM events
GROUP BY user_id, toDate(event_time)
ORDER BY user_id, date

┌─user_id─┬─date───────┬─pageviews────────────────────────────────┬─unique_pageviews─────────────────┬─pageview_count─┐
│     101 │ 2023-10-26 │ ['/home','/product/123','/home','/cart'] │ ['/home','/product/123','/cart'] │              4 │
└─────────┴────────────┴──────────────────────────────────────────┴──────────────────────────────────┴────────────────┘
```

<h4>Работа с географическими данными и лог-файлами</h4>

ClickHouse имеет встроенную поддержку геоданных:
- Типы данных:
  - `Point`, `Ring`, `Polygon`, `MultiPolygon` для хранения геометрий.
  - `Tuple(Float64, Float64)` как простая альтернатива для точки (долгота, широта).
- Функции:
  - Расстояние: `greatCircleDistance(lon1, lat1, lon2, lat2)` (расстояние по ортодромии), `geoDistance` (по формуле Винсенти).
  - Содержание: `pointInPolygon((lon, lat), polygon)` – проверка, попадает ли точка в полигон.

Поиск точек в радиусе:
```sql
SELECT *
FROM geo_events
WHERE greatCircleDistance(lon, lat, 37.617, 55.755) < 1000 -- В метрах
```

ClickHouse идеально подходит для анализа логов (например, nginx, application logs). Структура таблицы: Часто используют партиционирование по дате (`PARTITION BY toYYYYMM(date)`) и порядок по времени и тегу (`ORDER BY (date, timestamp, service)`).

Загрузка логов:
- Прямой парсинг: Можно загружать сырые логи, используя функции для извлечения данных прямо в `INSERT`.
- Pipeline: Filebeat/Logstash → Kafka → ClickHouse (через движок Kafka).
- Встроенный парсинг: Использование формата Regexp или TSKV для полуструктурированных логов.

Анализ:
- Поиск по подстроке: `WHERE message LIKE '%ERROR%'` (использует bloom-фильтры, если включены).
- Парсинг на лету: `extract(message, 'regexp')`, `splitByChar(' ', message)[3]`.
- Агрегация по временным окнам: `GROUP BY toStartOfMinute(timestamp)`, `toStartOfInterval(timestamp, INTERVAL 5 SECOND)`.
- Анализ частот: `count()`, `uniq(ip)`, `quantile(0.99)(response_time_ms)`.
- Движок AsynchronousInsert: Специально для логов — накапливает вставки в буфер и флаширует пачками, снижая нагрузку.
