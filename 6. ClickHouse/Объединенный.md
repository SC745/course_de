<h2>Основы и интеграция</h2>
<h3>1. Основы ClickHouse</h3>
<h4>Предназначение и основные концепции</h4>

ClickHouse — это колоночная система управления базами данных (СУБД) с открытым исходным кодом, разработанная компанией Yandex для сверхбыстрой аналитической обработки запросов (OLAP — Online Analytical Processing). Её главная задача — выполнять сложные агрегирующие запросы за доли секунды или секунды на объемах данных в петабайты.

Ключевые концепции:
- Колоночное хранение: Данные хранятся не по строкам (как в MySQL, PostgreSQL), а по столбцам. Это фундаментальная особенность, определяющая все преимущества.
- Векторизованная обработка запросов: Запросы выполняются не над отдельными значениями, а над целыми массивами данных (векторами), что максимально эффективно использует возможности современных CPU и кешей.
- Работа с "холодными" и "горячими" данными: Система оптимизирована для хранения огромных объемов данных на медленных дисках (HDD) или в облаке (S3), при этом активно используя кеширование и RAM для скорости.
- Данные вставляются, а не изменяются (в основном): ClickHouse предполагает, что данные в основном пишутся и почти никогда не обновляются или удаляются. Это позволяет применять оптимизации, недоступные для OLTP-систем. Однако с недавних версий поддерживаются легкие обновления и удаления (через мутации).
- Shared-nothing архитектура: Система легко масштабируется горизонтально добавлением новых серверов (шардирование). Каждый сервер независим.

Архитектура:
- SLB (Server Load Balancer): SLB или балансировщик нагрузки обычно используется для распределения входящего трафика между несколькими узлами или серверами в кластере ClickHouse. Это обеспечивает равномерное распределение запросов и повышает доступность и отказоустойчивость системы.
- Shard (Шард): В контексте ClickHouse шард представляет собой подмножество данных, хранящееся на отдельном узле или группе узлов. Шардинг позволяет горизонтально масштабировать данные, поскольку запросы могут выполняться параллельно на разных шардах. Это особенно важно для больших объемов данных и высокопроизводительных вычислений.
- Процессоры запросов: Это компоненты, которые обрабатывают SQL-запросы, исполняя операции над данными, хранящимися в базе. Особое внимание в ClickHouse уделяется эффективности обработки больших объемов данных, что достигается через использование векторных запросов.
- ECS (Elastic Compute Service): ECS обычно относится к виртуальным машинам или экземплярам, предоставляемым в облачных сервисах, таких как Alibaba Cloud, AWS или Google Cloud. В контексте ClickHouse, ECS может быть использован для развертывания и управления инстансами ClickHouse.
- Cloud Disk: Это облачное дисковое хранилище, используемое для хранения данных. В архитектуре ClickHouse облачные диски могут использоваться для хранения данных таблиц, журналов и резервных копий.
- Табличные движки: MergeTree - для вставки и выборки, Log - для быстрой записи, Integration Engines - для интеграции с внешними системами, Special Engines = для специализированных задач

Преимущества:
- Чтение только нужных столбцов: Если запросу нужно 5 полей из 100, с диска будет прочитано только 5 столбцов. В строчной базе пришлось бы читать всю строку со всеми 100 полями.
- Эффективное сжатие: Данные в одном столбце обычно однородны (числа, даты, строки), что позволяет сжимать их в десятки-сотни раз лучше, чем разнородные данные в строке.
- Векторизация: Операции над целыми массивами (векторами) данных минимизируют накладные расходы и лучше используют CPU.
- Экономия ресурсов ввода-вывода (I/O) и хранилища: Меньший объем данных, считываемый с диска + лучшее сжатие = меньше нагрузка на диск и сеть (в распределенном кластере).
- Оптимизация под агрегацию и сканирование: Агрегатные функции выполняются последовательно в памяти для целых столбцов, что очень быстро. Легко добавляются новые индексы (пропускающие индексы) для конкретных столбцов без перестройки всей таблицы.
- Поддержка сложных аналитических функций: Оконные функции, приблизительные вычисления (например, `uniq`), вероятностные структуры данных (HyperLogLog), работа с временными рядами "из коробки".

Недостатки (обратная сторона преимуществ):
- Низкая производительность на точечных запросах (`SELECT` по одному ключу) и частых обновлениях (OLTP).
- Сложности с транзакционностью и согласованностью в реальном времени.
- Запросы `JOIN` работают иначе, чем в OLTP-базах, и требуют правильного проектирования данных (часто предпочитается денормализация).

<h4>Столбцовые и строковые БД</h4>

Строковые БД (например, MySQL, PostgreSQL) хранят данные по строкам. Каждая строка содержит все поля записи, и они располагаются на диске последовательно. Это эффективно для операций, затрагивающих всю запись (OLTP), например, вставка, обновление, удаление.

Столбцовые БД (например, ClickHouse, Vertica) хранят данные по столбцам. Значения каждого столбца хранятся отдельно, а в пределах каждого столбца данные сжимаются и хранятся в виде векторов. Это эффективно для операций агрегации и анализа больших объемов данных (OLAP), когда требуется читать только несколько столбцов из множества строк.

Основные различия:
- Структура хранения:
  - Строковые БД: Данные хранятся по строкам
  - Столбцовые БД: Данные хранятся по столбцам
- Сжатие:
  - Строковые БД: Сжатие менее эффективно, так как разнородные данные в строке
  - Столбцовые БД: Высокое сжатие за счет однотипности данных в столбце
- Чтение:
  - Строковые БД: Чтение всей строки, даже если нужен один столбец
  - Столбцовые БД: Чтение только необходимых столбцов
- Запись:
  - Строковые БД: Быстрая вставка/обновление отдельных записей
  - Столбцовые БД: Пакетная вставка, обновления затруднены
- Использование:
  - Строковые БД: OLTP: транзакции, частые обновления
  - Столбцовые БД: OLAP: аналитика, агрегация больших данных
- Индексы:
  - Строковые БД: Индексы по отдельным столбцам, составные индексы
  - Столбцовые БД: Индексы часто не нужны, эффективное сканирование столбцов

<h4>Системные требования и установка</h4>

Поддерживаемые ОС:
- Linux: Наиболее предпочтительная и поддерживаемая платформа. Официально поддерживаются Ubuntu (начиная с 18.04), Debian (начиная с 9), CentOS (7 и 8) и RHEL (7 и 8) Также работает на других дистрибутивах (Fedora, OpenSUSE, ALT Linux и др.)
- macOS: Поддерживается для разработки, но не для production.
- FreeBSD: Экспериментальная поддержка.
- Windows: Не поддерживается нативно, но может быть запущен в Docker или WSL2 (Windows Subsystem for Linux).

Системные требования:
- Память (RAM): Минимум 4 ГБ, но для production-нагрузок рекомендуется от 64 ГБ и более. ClickHouse использует память для обработки запросов, особенно для JOIN и агрегаций.
- Процессор: Современный многоядерный процессор (ClickHouse эффективно использует многопоточность).
- Диск: Предпочтительно SSD или NVMe для высоких нагрузок. HDD подойдут для хранения больших объемов холодных данных. Объем зависит от объема данных и степени сжатия (сжатие может достигать 20-30 раз).
- Файловая система: Рекомендуется ext4 (для Linux) или xfs. Важно отключить atime для ускорения чтения.
- Сеть: Высокая пропускная способность для распределенных кластеров.

Перед установкой ClickHouse необходимо добавить репозиторий ClickHouse в список источников пакетов системы:
```bash
sudo apt-get install apt-transport-https ca-certificates dirmngr
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E0C56BD4    # добавление GPG ключа
echo "deb https://repo.clickhouse.com/deb/stable/ main/" | sudo tee /etc/apt/sources.list.d/clickhouse.list
```

После добавления репозитория нужно обновить информацию о пакетах и установить ClickHouse:
```bash
sudo apt-get update
sudo apt-get install clickhouse-server clickhouse-client
```

После установки ClickHouse сервер можно запустить следующей командой:
```bash
sudo service clickhouse-server start
```

Для проверки успешной установки и работоспособности ClickHouse можно подключиться к серверу с помощью клиента ClickHouse:
```bash
clickhouse-client
```

<h4>Основные команды</h4>

Управление сервером:
```bash
sudo systemctl start clickhouse-server                           # Запуск сервера
sudo systemctl stop clickhouse-server                            # Остановка сервера
sudo systemctl restart clickhouse-server                         # Перезагрузка сервера
sudo systemctl status clickhouse-server                          # Проверка статуса
sudo tail -f /var/log/clickhouse-server/clickhouse-server.log    # Просмотр логов
```

Конфигурация сервера:
- Основной конфигурационный файл: `/etc/clickhouse-server/config.xml`
- Пользовательские настройки: `/etc/clickhouse-server/config.d/`

Конфигурация пользователей и прав доступа: Файлы в `/etc/clickhouse-server/users.xml` и `/etc/clickhouse-server/users.d/`

Команды `clickhouse-client`:
```bash
clickhouse-client               # Запуск клиента:
clickhouse-client --password    # Если установлен пароль
clickhouse-client --host=example.com --port=9000 --user=username --password    # Подключение к удаленному серверу
clickhouse-client --query="SELECT 1"                            # Выполнение запроса из командной строки
clickhouse-client < queries.sql                                 # Выполнение запроса из файла
clickhouse-client --format=Pretty                               # Форматирование вывода
clickhouse-client --multiquery --query="SELECT 1; SELECT 2;"    # Многозапросный режим
clickhouse-client --max_memory_usage=1000000000                 # Настройки в клиенте

```

<h4>Параллелизация запросов</h4>

ClickHouse использует несколько уровней параллелизма для ускорения обработки запросов. Уровни параллелизма:
- Многопоточность на одном узле (на уровне одного запроса): ClickHouse может параллельно читать данные из нескольких кусков (parts) таблицы. Каждый поток может обрабатывать свой кусок. При агрегации данных ClickHouse использует параллельную обработку. Частичные результаты агрегации из разных потоков затем объединяются (merge). Настройки: `max_threads` (максимальное количество потоков для обработки запроса, по умолчанию равно количеству ядер CPU) и `max_block_size` (размер блока для обработки).
- Распределенная обработка (на уровне кластера): При работе с распределенной таблицей (Distributed engine) запрос параллельно выполняется на всех шардах (или на части, в зависимости от условий). Координатор (нода, на которую пришел запрос) отправляет подзапросы на все шарды, дожидается ответов и выполняет финальную агрегацию (если требуется). Если в запросе есть `GROUP BY`, то агрегация сначала выполняется на каждом шарде, а затем на координаторе происходит финальное объединение. Иногда, если данные уже сгруппированы по ключу шардирования, можно использовать настройку `distributed_group_by_no_merge` для отключения финальной агрегации.
- Векторизованное выполнение (на уровне CPU): ClickHouse обрабатывает данные не по одному значению, а по блокам (столбцам) данных, используя SIMD-инструкции процессора. Это позволяет эффективно использовать кэш CPU и выполнять операции над множеством значений за одну инструкцию. При чтении с диска ClickHouse использует асинхронный ввод-вывод и может параллельно читать данные с нескольких дисков.

Пример настройки параллелизма:
sql
```sql
-- Установить максимальное количество потоков для выполнения запроса
SET max_threads = 16;

-- Для распределенных запросов можно управлять параллельностью на уровне шардов
SET distributed_aggregation_memory_efficient = 1;
SET group_by_two_level_threshold = 1000000;

```
Советы по оптимизации параллелизма:
- Не устанавливать `max_threads` слишком высоким, так как это может привести к конкуренции за ресурсы и снижению производительности. Обычно достаточно количества ядер CPU.
- Для распределенных запросов важно, чтобы данные были равномерно распределены по шардам, чтобы избежать ситуации, когда один шард обрабатывает значительно больше данных, чем другие (skew).
- Использовать настройки для управления памятью и параллелизмом при агрегации, такие как `max_bytes_before_external_group_by` (для внешней сортировки на диске при нехватке памяти) и `distributed_aggregation_memory_efficient`.

<h4>Способы вставки данных</h4>

Классический `INSERT`: Подходит для небольших вставок (тысячи строк):
```sql
INSERT INTO table_name (col1, col2) VALUES (v1, v2), (v3, v4), ...
```

Пакетная вставка (`INSERT SELECT`):
```sql
INSERT INTO table_name SELECT ...
```

Вставка данных из файлов (наиболее распространённый способ):
```sql
clickhouse-client --query="INSERT INTO table_name FORMAT CSVWithNames" < data.csv    # Из локального файла
INSERT INTO table_name FROM INFILE '/path/to/data.csv' FORMAT CSV                    # Из файла на сервер
```

Использование специальных форматов для больших объёмов:
```sql
clickhouse-client --query="SELECT * FROM source_table" --format Native | clickhouse-client --query="INSERT INTO dest_table FORMAT Native" # Native формат
INSERT INTO table_name FROM INFILE 'data.parquet' FORMAT Parquet    # Parquet, ORC, Avro
```

Потоковая вставка (для интеграций):
- Нативные протоколы: Библиотеки для Python (clickhouse-driver), Java, Go и др. используют быстрый бинарный протокол.
- Kafka: С помощью движка Kafka или Materialized View данные автоматически консьюмируются из топика и вставляются в таблицу.
- PostgreSQL, MySQL: Используя движки PostgreSQL или MySQL, можно делать INSERT в ClickHouse, который будет читать данные из внешней БД.

<h4>Резервное копирование и восстановление</h4>

ClickHouse не имеет встроенного полноценного механизма бэкапов как коммерческие СУБД, но есть проверенные подходы:
- Логическое резервное копирование (экспорт данных):
```sql
SELECT * FROM my_table
INTO OUTFILE '/backup/my_table.csv'
FORMAT CSVWithNames
```
Плюсы: Переносимость, сжатие, выбор формата (Parquet, Native).
Минусы: Медленно для огромных объемов, блокирующая операция (для больших таблиц).
- Физическое резервное копирование (файлы данных):
  - Ручное копирование: Остановить запись, скопировать директорию /var/lib/clickhouse/data/database/table/ и /metadata/. Подходит только для небольших остановок.
  - Снапшоты ФС: Использование LVM, ZFS или btrfs для создания моментальных снимков папки с данными. Требует осторожности.
  - Replication как бэкап: Наличие 2-3 реплик в кластере (движок ReplicatedMergeTree) — лучшая защита от сбоя железа. Но не защищает от человеческой ошибки (случайного DROP TABLE).
- `clickhouse-backup` — де-факто стандарт. Создает полные/инкрементальные бэкапы (данные + схема), умеет работать с S3, GCS, Azure Blob Storage (идеально для хранения), поддерживает шифрование и сжатие, позволяет делать бэкапы без остановки сервера (`--skip-check-parts`). Основные команды:
```bash
clickhouse-backup create my_backup
clickhouse-backup upload my_backup
clickhouse-backup restore my_backup
```

Восстановление данных:
- Из логического бэкапа: `INSERT ... FROM INFILE`.
- Из физического бэкапа: Остановить ClickHouse, восстановить данные и метаданные в соответствующие директории, запустить `sudo clickhouse start --force-recover`. 
- С помощью clickhouse-backup: Команда restore сделает всё автоматически.
- Восстановление таблицы: Если таблица случайно удалена, но данные на диске остались, можно создать таблицу заново с той же структурой и выполнить `ATTACH PARTITION` или переместить данные из папки `detached`.

<h4>Шардирование и репликация</h4>

Репликация — это механизм копирования данных между серверами для обеспечения отказоустойчивости и доступности данных. В ClickHouse используется мульти-мастер (multi-master) асинхронная репликация на уровне таблиц (семейств движков `ReplicatedMergeTree`). Для работы репликации необходимо использовать движки, начинающиеся с `Replicated` (например, `ReplicatedMergeTree`, `ReplicatedReplacingMergeTree`). Координация репликации (хранение метаданных, очередь операций, выбор лидера) осуществляется через внешний сервис — ZooKeeper или его встроенный аналог ClickHouse Keeper. Сами данные передаются напрямую между репликами (peer-to-peer). Запись можно выполнять на любую реплику, и данные будут асинхронно распространены на другие. Чтение также можно выполнять с любой реплики. Это повышает доступность и распределяет нагрузку на чтение.

Шардирование (сегментирование) — это механизм горизонтального масштабирования, при котором разные части данных (шарды) размещаются на разных серверах. Данные распределяются по шардам в соответствии с ключом шардирования (например, по хэшу от `user_id`). Каждый шард может состоять из одной или нескольких реплик (для отказоустойчивости). Основной инструмент для работы с шардированным кластером — распределенные таблицы (движок `Distributed`). Это «виртуальные» таблицы, которые не хранят данные сами, а лишь прозрачно перенаправляют запросы на нижележащие шарды (локальные таблицы на каждом сервере) и агрегируют результаты. Локальные таблицы - Физические таблицы с данными на каждом узле кластера.

Настройка — это в основном редактирование конфигурационных XML-файлов (`config.xml` и включаемых из папки `config.d/`).

ZooKeeper: Отдельный кластер (минимум 3 ноды для отказоустойчивости). В конфиг ClickHouse (`config.xml` или `config.d/zookeeper.xml`) добавляется:
```xml
<zookeeper>
    <node index="1">
        <host>zk1.example.com</host>
        <port>2181</port>
    </node>
    <node index="2">
        <host>zk2.example.com</host>
        <port>2181</port>
    </node>
    <node index="3">
        <host>zk3.example.com</host>
        <port>2181</port>
    </node>
</zookeeper>
```

ClickHouse Keeper: Встроенное решение, проще в развертывании. Настраивается в `config.xml` аналогично, но с указанием ролей.

В `config.xml` (чаще в `config.d/remote_servers.xml`) описывается топология кластера:
```xml
<clickhouse>
    <remote_servers>
        <my_cluster> <!-- Имя кластера -->
            <shard> <!-- Шард 1 (набор реплик) -->
                <internal_replication>true</internal_replication>
                <replica>
                    <host>ch-s1-r1.example.com</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>ch-s1-r2.example.com</host>
                    <port>9000</port>
                </replica>
            </shard>
            <shard> <!-- Шард 2 -->
                <internal_replication>true</internal_replication>
                <replica>
                    <host>ch-s2-r1.example.com</host>
                    <port>9000</port>
                </replica>
            </shard>
        </my_cluster>
    </remote_servers>
</clickhouse>
```
`internal_replication=true` означает, что ClickHouse будет реплицировать данные между репликами шарда самостоятельно (используя движки `Replicated*`). Если `false`, распределенная таблица будет сама дублировать данные на все реплики (устаревший подход).

Макросы — уникальные идентификаторы для каждого сервера в кластере. Определяются в `config.xml` (`macros.xml`):
```xml
<clickhouse>
    <macros>
        <shard>01</shard>     <!-- Номер шарда -->
        <replica>ch-s1-r1.example.com</replica> <!-- Имя реплики (хоста) -->
    </macros>
</clickhouse>
```
Эти макросы используются при создании реплицируемых таблиц для автоматического определения их места в кластере.

Локальная реплицируемая таблица на каждом узле:
```sql
CREATE TABLE local_data.metrics ON CLUSTER 'my_cluster' (
    event_date Date,
    user_id UInt64,
    value Float64
) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/local_data.metrics', '{replica}')
PARTITION BY toYYYYMM(event_date)
ORDER BY (user_id, event_date);
```
`/clickhouse/tables/{shard}/local_data.metrics` — путь в ZooKeeper. {shard} и {replica} подставятся из макросов.

Распределенная таблица (обычно создается на всех узлах для удобства):
```sql
CREATE TABLE distributed_data.metrics ON CLUSTER 'my_cluster' AS local_data.metrics
ENGINE = Distributed('my_cluster', 'local_data', 'metrics', user_id);
```
- `my_cluster` — имя кластера из конфига.
- `local_data`, `metrics` — база и таблица-исходник на шардах.
- `user_id` — ключ шардирования (необязательно). Если не указан, используется случайное распределение.

Запись идет в `distributed_data.metrics`, чтение — также из нее. Все распределение и сбор данных ClickHouse берет на себя.

<h3>2. Интеграция с внешними источниками</h3>
<h4>Основы интеграции</h4>

ClickHouse может работать с внешними данными через несколько механизмов. Основные из них:
- Табличные движки (Table Engines): Позволяют создать таблицу в ClickHouse, которая является "окном" во внешнюю систему. Запросы к такой таблице преобразуются в запросы к внешнему источнику.
  - ODBC / JDBC: Универсальные драйверы для связи с любыми СУБД: PostgreSQL, MySQL, Microsoft SQL Server, Oracle, Greenplum и многими другими. Требуется установить в системе ClickHouse JDBC-драйвер для целевой СУБД. Обычно драйвер помещается в папку `/usr/share/clickhouse/jdbc-drivers/` (или аналогичную, в зависимости от установки) и настроить конфигурацию JDBC-драйверов в ClickHouse (при необходимости).
  - MySQL: Нативный движок для прямого подключения к MySQL/MariaDB.
  - PostgreSQL: Нативный движок для подключения к PostgreSQL.
  - MongoDB: Прямое чтение данных из коллекций MongoDB.
- Файловые движки и виртуальные столбцы: Позволяют запрашивать данные напрямую из файлов в различных форматах, хранящихся как локально, так и в облаке. File, URL, S3 движки поддерживают множество форматов: Parquet, ORC, Arrow, JSONEachRow, CSV, TSV, Avro и др:
  - S3 / URL: Чтение и запись файлов из/в Amazon S3, Google Cloud Storage, Azure Blob Storage и подобных, а также по HTTP/HTTPS URL.
  - HDFS: Прямая работа с файлами в Hadoop Distributed File System.
- Интеграция с системами обмена сообщениями (Message Queues): Позволяет нативно потреблять данные из потоковых систем:
  - Kafka: Движок Kafka для подписки на топики Apache Kafka. Это основной способ потокового приема данных.
  - RabbitMQ: Поддержка RabbitMQ для аналогичных целей.
- Внешние словари (External Dictionaries): Позволяют загружать справочные данные из внешних источников для JOIN-подобных операций. Поддерживаются источники: локальные файлы, HTTP(s), различные СУБД (через ODBC/JDBC), встроенные в CH источники (ClickHouse, MySQL, MongoDB).

Примеры:
```sql
CREATE TABLE my_external_url ENGINE = URL('http://example.com/data.csv', CSV, 'name String, age UInt32')    -- URL
CREATE TABLE kafka_table (name String, age UInt32) ENGINE = Kafka(...) SETTINGS ...                         -- Kafka
CREATE TABLE hdfs_table (...) ENGINE = HDFS('hdfs://namenode:port/path/to/file', 'Parquet')                 -- HDFS
CREATE TABLE jdbc_table (...) ENGINE = JDBC('jdbc:mysql://host:port/db', 'user', 'password', 'table')       -- JDBC
```

Поддерживаемые типы данных:
- PostgreSQL, MySQL: Поддерживают почти все стандартные SQL-типы с автоматическим преобразованием в типы ClickHouse:
  - Числовые: `Int8-Int256`, `UInt8-UInt256`, `Float32`, `Float64`, `Decimal`
  - Строковые: `String`, `FixedString`, `Enum`
  - Дата/время: `Date`, `Date32`, `DateTime`, `DateTime64`
  - Специальные: `UUID`, `IPv4`, `IPv6`
  - Агрегатные: поддерживаются через движок `AggregateFunction`
- MongoDB:
  - `ObjectId` → `String`
  - `Date` → `DateTime`
  - `Number` → `Float64` или `Int64`
  - `Array` → `Array(String)` или вложенные структуры
  - `Document` → `Nested` или `JSON`
- S3, HDFS, URL:
  - Данные в форматах: CSV, TSV, JSONEachRow, Parquet, ORC, Arrow
  - Типы определяются схемой, указанной в запросе `CREATE TABLE`
- Kafka: Зависит от формата сообщений. Чаще используется String для сырых данных или структурированные форматы

Ограничения при работе с внешними источниками:
- Производительность (Самое главное): Данные не хранятся в ClickHouse и не используют его колоночную структуру и индексы (первичный ключ, засечки и т.д.). Каждый запрос выполняется "на лету" с обращением к внешней системе. Это медленно, особенно для агрегаций и `JOIN`. Не подходит для аналитики на больших объемах в реальном времени.
- Ограниченная поддержка DML: Большинство внешних движков поддерживают только чтение. Запись поддерживается лишь для некоторых движков (например, MySQL, PostgreSQL, S3) и часто с ограничениями. Например, вставка в PostgreSQL таблицу работает, но это неэффективно для больших пачек.
- Отсутствие транзакционности и согласованности: Операции не являются атомарными. При сбое во время длительного INSERT через ODBC часть данных может уже быть записана в целевой системе. Чтение данных из внешних источников (особенно СУБД) может приводить к нагрузке на них и влиять на их работу.
- Ограниченная поддержка сложных запросов: Пушдауны (Pushdown) — способность ClickHouse делегировать часть работы (фильтрацию, агрегацию) внешней системе — реализована не для всех движков и не для всех операций. Например, для MySQL движка условие `WHERE` на столбец-ключ может быть отправлено (pushed down) в MySQL, что ускорит запрос. Но для файла в S3 — нет.
- Сложность управления схемой: Изменение схемы данных во внешнем источнике может привести к ошибкам в запросах ClickHouse. Необходимо вручную обновлять DDL внешней таблицы.

Внешние источники данных в ClickHouse можно использовать в запросах, включая операции объединения (`JOIN`) и агрегации (`GROUP BY`). Однако важно помнить об ограничениях производительности, так как данные находятся вне ClickHouse и могут быть обработаны менее эффективно.

Пример `JOIN` внутренней таблицы ClickHouse с внешней таблицей (например, через JDBC):
```sql
SELECT
    local_table.id,
    local_table.value,
    jdbc_table.name
FROM local_table
LEFT JOIN jdbc_table ON local_table.id = jdbc_table.id
```

Пример агрегации данных из внешней таблицы:
```sql
SELECT
    category,
    COUNT(*) as count
FROM mysql_table
GROUP BY category
```

При выполнении таких запросов ClickHouse может пытаться "протолкнуть" некоторые операции (например, фильтрацию) во внешнюю систему, если это поддерживается движком. Однако это зависит от конкретного движка и версии ClickHouse.

Для улучшения производительности рекомендуется:
- По возможности загружать данные из внешних источников во внутренние таблицы ClickHouse (например, с помощью ETL-процессов).
- Использовать внешние источники для относительно небольших объемов данных, особенно для операций `JOIN`.
- Для справочников, которые используются в JOIN, рассмотреть использование внешних словарей (External Dictionaries), которые загружаются в память и могут быть более эффективны.

<h4>Основные команды для работы со внешними источниками</h4>

ClickHouse предоставляет несколько механизмов для работы с внешними данными:
- Файлы (локальные и по HTTP/S3):
```sql
# file(path, format, structure) — чтение из локального файла сервера.
SELECT * FROM file('data.csv', CSV, 'id UInt32, name String')
# url(url, format, structure) — чтение по HTTP/HTTPS.
SELECT * FROM url('https://example.com/data.tsv', TSV, 'column1 String')
# s3(url, access_key_id, secret_access_key, format, structure) — прямое чтение из S3-совместимых хранилищ.
SELECT * FROM s3('https://storage.example.com/my-bucket/data.parquet', 'AKIA...', 'secret...', Parquet)
```
- Внешние БД (через интеграции):
```sql
# mysql('host:port', 'database', 'table', 'user', 'password') - внешняя БД MySQL
SELECT * FROM mysql('localhost:3306', 'test', 'users', 'user', 'password')
# postgresql('host:port', 'database', 'table', 'user', 'password') - внешняя БД PostgreSQL
SELECT * FROM postgresql('localhost:5432', 'prod', 'orders', 'user', 'password')
```
MongoDB, SQLite, JDBC: Аналогичные табличные функции (`mongodb`, `sqlite`, `jdbc`).
- Виртуальные столбцы и движки таблиц: Создание таблицы на внешний источник (например, MySQL):
```sql
CREATE TABLE mysql_orders (
    id UInt64,
    amount Float64
) ENGINE = MySQL('localhost:3306', 'db', 'orders', 'user', 'password');
SELECT * FROM mysql_orders;
```
- Команды `INSERT` для загрузки данных из ClickHouse во внешние источники
```sql
# Вставка в файл
INSERT INTO FUNCTION file('output.csv', CSV) SELECT * FROM local_table;
# Вставка в S3
INSERT INTO FUNCTION s3('https://.../out.parquet', ...) SELECT * FROM local_table;
# Вставка во внешнюю БД
INSERT INTO TABLE FUNCTION mysql(...) SELECT * FROM local_table;
```

<h4> Параллельная загрузка данных из внешних источников</h4>

Параллельное чтение из одного источника: Настройки `max_threads` и `max_block_size` влияют на параллельную обработку уже загруженных данных. При использовании `file()` или `s3()` параллелизм на чтении одного файла ограничен. Решение: Разбивать большие файлы на части (например, `data_*.csv`) и читать их параллельно с помощью `s3(..., format)` или через `INSERT INTO ... SELECT` из нескольких потоков/задач.

Параллельное чтение из нескольких разделов или таблиц:
```sql
INSERT INTO local_table
SELECT * FROM mysql_table WHERE month='2023-01'
UNION ALL
SELECT * FROM mysql_table WHERE month='2023-02'
```

Параллельная вставка (parallel_distributed_insert_select): При использовании распределённых запросов (к кластеру ClickHouse) можно заставить шарды выполнять `INSERT ... SELECT` параллельно. Для этого нужно создать распределённую таблицу на все шарды, включить настройку `parallel_distributed_insert_select` и  запустить запрос с `INSERT INTO distributed_table SELECT ... FROM external_source`. Каждый шард выполнит свою часть работы.

Практические советы для максимального параллелизма: Использовать `async_insert=1` при вставке больших объёмов из приложений (не для `INSERT ... SELECT`). Выгружать данные партициями (по датам, id) в отдельные файлы и загружать их одновременно разными job.
```bash
cat data_*.csv | parallel -j 8 'clickhouse-client --query "INSERT INTO table FORMAT CSV" < {}'
```
Настроить пул потоков: Увеличьте `background_pool_size`, `background_schedule_pool_size` если много фоновых задач (MV, словари).
Для словарей использовать `layout_cache`, `complex_key_cache` — они загружают данные параллельно в несколько потоков.

<h4>Транзакционная целостность</h4>

ClickHouse не является полноценной транзакционной СУБД (в смысле ACID для произвольных операций). Его сила — в скорости вставки и чтения больших объемов данных. Атомарность на уровне вставки: Вставка данных (`INSERT`) является атомарной на уровне одного запроса. Все строки одного запроса `INSERT` либо будут записаны, либо ни одна. При вставке из нескольких параллельных `INSERT` целостность на уровне таблицы гарантируется.

Изоляция между запросами:
- Чтение: Запрос `SELECT` видит консистентный снимок данных на момент начала своего выполнения. Данные, вставленные во время выполнения `SELECT`, не будут им видны.
- Одновременное чтение и запись: `SELECT` и `INSERT` не блокируют друг друга. Это одно из ключевых преимуществ для аналитики.

Механизмы для "транзакционности":
- Блочная структура данных: Данные вставляются пачками (блоками), и многие операции работают с целыми блоками как с атомарными единицами.
- Мутации (`ALTER ... UPDATE/DELETE`): Тяжелые операции, которые перезаписывают целые партиции (куски данных). Они неатомарны на уровне всей таблицы, но атомарны на уровне партиции. Мутации выполняются асинхронно. Для отслеживания прогресса есть системная таблица `system.mutations`.
- Легковесные транзакции (Lightweight transactions): Используются с фразой `SETTINGS mutations_sync = 1/2`. Это позволяет ждать завершения мутации, но не дает полноценной изоляции как в OLTP.
- Движок `ReplicatedMergeTree`: Обеспечивает консистентность реплик. Данные на разных репликах будут идентичны (в конечном счете, eventual consistency). Запись в несколько реплик синхронизирована.

ClickHouse предлагает "транзакционность для аналитики" — гарантии целостности при вставке больших пачек данных и консистентное чтение. Для частых точечных UPDATE/DELETE (OLTP-сценарии) он не подходит. Стратегия — вставка новых данных и периодическая очистка устаревших.

<h4>Интеграция с облачными хранилищами данных</h4>

ClickHouse имеет превосходную и глубокую интеграцию с S3-совместимыми объектными хранилищами (Amazon S3, Яндекс Object Storage, Google Cloud Storage, MinIO и др.).

Основные возможности интеграции:

- S3 как прямое хранилище для табличных движков: Можно читать и писать данные напрямую в/из S3 с помощью виртуальных файловых систем.
```sql
INSERT INTO TABLE my_table
SELECT * FROM s3('https://bucket.s3.amazonaws.com/data*.csv', 'CSV');
SELECT * FROM s3('https://...', 'Parquet') LIMIT 10;
```
- Движок таблиц S3: Позволяет работать с S3 как с постоянной таблицей (часто для выгрузки данных или промежуточного хранения).
```sql
CREATE TABLE my_s3_table (id UInt32, name String)
ENGINE = S3('https://.../output.csv', 'CSV');
```
- Хранение данных MergeTree в S3 (важнейшая функция):
  - S3 как диск (storage_policy): Можно настроить политику хранения, где "горячие" данные лежат на локальных SSD, а "холодные" автоматически перемещаются в S3. Прозрачно для пользователя: запросы работают со всей историей данных.
  - S3 как основной диск: Можно создать таблицу, данные которой полностью хранятся в S3. Это дешевле, но запросы будут медленнее из-за задержек сети.
- Бэкенд для репликации в S3: Начиная с версии 22.8, ClickHouse может хранить данные для репликации движка `ReplicatedMergeTree` не на локальном диске, а в S3. Это позволяет легко создавать и масштабировать кластеры в облаке, отделяя хранилище от вычислительных узлов.
- Резервное копирование в S3: Утилита clickhouse-backup и встроенные механизмы эффективно работают с S3 для создания и хранения бэкапов.

Практическое применение:
- Холодное хранение (Cold Storage): Архивация старых данных в дешевое S3 с сохранением возможности запроса.
- Data Lake Queries: Прямой запрос к данным, уже лежащим в Data Lake (Parquet, CSV в S3).
- Разделение хранения и вычислений (Storage-Compute Separation): Кластер ClickHouse может динамически подключаться к общему хранилищу в S3, что упрощает масштабирование и повышает отказоустойчивость.
- Экспорт/Импорт: Быстрая выгрузка результатов больших запросов в S3 для передачи в другие системы.

<h4>Интеграция с Apache Kafka</h4>

ClickHouse интегрируется с Kafka через специальный движок таблиц `Kafka`. Этот движок не хранит данные сам, а работает как потребитель (consumer) Kafka, который:
1. Читает сообщения из указанных топиков Kafka.
2. Парсит их в заданном формате (JSONEachRow, CSV, Avro через AvroConfluent и т.д.).
3. Перенаправляет данные в обычную (целевую) таблицу ClickHouse с помощью материализованного представления (Materialized View).

Ключевые преимущества:
- Потоковая (stream) загрузка данных в реальном времени. Данные появляются в ClickHouse с минимальной задержкой (секунды).
- Высокая пропускная способность. ClickHouse может потреблять сотни тысяч сообщений в секунду.
- Отказоустойчивость: ClickHouse хранит смещения (offsets) в системной таблице `kafka` в Zookeeper или самому Kafka (начиная с версии 22.3). При перезапуске потребление продолжится с того места, где остановились.
- Гибкость преобразований. Materialized View позволяет фильтровать, преобразовывать данные и дедуплицировать их перед вставкой в финальную таблицу.
- Поддержка схем. Использование формата AvroConfluent позволяет интегрироваться с Schema Registry Confluent для контроля схемы данных.
- Простота эксплуатации. Не нужны дополнительные ETL-процессы (как, например, Airflow) для пакетной загрузки. Система работает непрерывно.

ClickHouse подключается к Kafka как стандартный потребитель (consumer) из группы (consumer group). Ключевые настройки, которые нужно учесть на стороне Kafka и при конфигурации в ClickHouse:
- Настройки в конфигурационном файле ClickHouse (`config.xml`): `<kafka>` - необязательная секция для общих настроек SSL/аутентификации для всех таблиц Kafka. Чаще настройки задаются прямо при создании таблицы Kafka.
- Ключевые параметры при создании таблицы Kafka в ClickHouse:
```sql
CREATE TABLE queue (
    timestamp DateTime,
    message String
) ENGINE = Kafka
SETTINGS
    kafka_broker_list = 'kafka-host-1:9092,kafka-host-2:9092',
    kafka_topic_list = 'clickhouse_topic',
    kafka_group_name = 'clickhouse_consumer_group',
    kafka_format = 'JSONEachRow',
    kafka_max_block_size = 1048576,
    kafka_skip_broken_messages = 1,
    kafka_num_consumers = 1,
    kafka_handle_error_mode = 'stream';
```

Настройки:
- `kafka_group_name`: Имя группы потребителей, крайне важно для управления оффсетами. Уникальное для каждого потока потребления, позволяет перезапускать потребителя с того же места.
- `kafka_num_consumers`: Количество потребителей в одной таблице. Увеличивать только если одна партиция топика не успевает обрабатываться. Не должно превышать число партиций в топике Kafka.
- `kafka_skip_broken_messages`: Сколько битых сообщений можно пропустить подряд. Защищает от падения потока из-за одиночного некорректного сообщения.
- `kafka_commit_every_batch`: Фиксировать оффсет после каждой пачки (batch). По умолчанию 0 (после блока). 1 увеличивает надежность, но снижает производительность.
- `kafka_poll_timeout_ms`: Таймаут опроса брокера. По умолчанию 5000 (5 сек). Уменьшать для более быстрого ответа на остановку, увеличивать для снижения нагрузки на CPU.
- `kafka_flush_interval_ms`: Интервал сброса данных из внутреннего буфера в MV. По умолчанию 750, Увеличение может немного повысить производительность за счет задержки.

Настройка ClickHouse для приема сообщений:
1. Создание целевой таблицы (куда будут складываться итоговые данные).
```sql
CREATE TABLE target_table (
    id UInt64,
    timestamp DateTime,
    data String,
    metric Float32
) ENGINE = MergeTree
ORDER BY (id, timestamp);
```
2. Создание таблицы-движка Kafka (для чтения потока).
```sql
CREATE TABLE kafka_queue (
    id UInt64,
    timestamp DateTime,
    data String,
    metric Float32
) ENGINE = Kafka
SETTINGS
    kafka_broker_list = 'localhost:9092',
    kafka_topic_list = 'incoming_data',
    kafka_group_name = 'ch_consumers',
    kafka_format = 'JSONEachRow',
    kafka_skip_broken_messages = 10,
    kafka_num_consumers = 4; -- Если в топике 4 партиции
```
3. Создание Materialized View — мост между kafka_queue и target_table.
```sql
CREATE MATERIALIZED VIEW consumer_mv TO target_table AS
SELECT id,
       timestamp,
       data,
       metric
  FROM kafka_queue
 WHERE metric > 0; -- Опциональная фильтрация или трансформация
```

Данные, появляющиеся в топике Kafka `incoming_data`, автоматически читаются таблицей `kafka_queue`, преобразуются MV и вставляются в `target_table`.

Остановка и запуск потока:
```sql
DETACH TABLE kafka_queue; -- Остановить потребление
ATTACH TABLE kafka_queue; -- Возобновить потребление
```

Надежность и отказоустойчивость в интеграции ClickHouse с Kafka достигаются за счет нескольких механизмов:
- Хранение оффсетов: ClickHouse хранит оффсеты (позиции чтения) для каждой партиции Kafka. Это позволяет при перезапуске продолжить чтение с того места, где остановились. До версии 22.3 оффсеты хранились в ZooKeeper, а начиная с 22.3 можно хранить в самом Kafka (как это делают стандартные консьюмеры). Это важно для отказоустойчивости, так как при сбое ClickHouse-сервера другой сервер может подхватить чтение с того же оффсета.
- Репликация в ClickHouse: Для обеспечения отказоустойчивости данных в ClickHouse можно использовать реплицированные таблицы (движок `ReplicatedMergeTree`). При этом каждая реплика может потреблять данные из Kafka, но это требует осторожности, чтобы не дублировать данные. Обычная практика: потреблять данные из Kafka только одним узлом (лидером) и затем реплицировать их на другие узлы через механизм репликации ClickHouse. Но можно настроить потребление на каждой реплике, тогда нужно следить за тем, чтобы оффсеты хранились отдельно для каждой реплики (разные `kafka_group_name`).
- Обработка сбоев в Kafka: Если Kafka-кластер становится недоступным, ClickHouse будет пытаться переподключиться (согласно настройкам таймаутов и повторов). Важно настроить `kafka_skip_broken_messages` и `kafka_handle_error_mode` (в новых версиях), чтобы обработка не прерывалась из-за единичных некорректных сообщений.
- Мониторинг и алертинг: Мониторить отставание по партициям можно через системную таблицу `system.kafka_tables` (столбец `messages_lag`), настроить алерты при большом lag или ошибках потребления. Также важно мониторить нагрузку на ClickHouse и Kafka, чтобы вовремя масштабироваться.
- Гарантии доставки: ClickHouse с движком Kafka обеспечивает гарантию "at least once" (минимум один раз). Это означает, что в случае сбоя могут быть дубликаты. Для исключения дубликатов можно использовать идемпотентные вставки (например, с помощью `ReplacingMergeTree` и версии) или дедупликацию на уровне блока (настройка `merge_tree`), а также транзакции в Kafka (требует тщательной настройки и поддерживается не во всех сценариях).
- Резервное копирование и восстановление: Регулярно делайте бэкапы данных в ClickHouse (например, с помощью `ALTER TABLE ... FREEZE` или инструментов типа clickhouse-backup). Имейте план восстановления данных из бэкапа в случае потери.

<h2>Моделирование и запросы</h2>
<h3>1. Моделирование схемы</h3>
<h4>Сжатие данных</h4>

ClickHouse сжимает данные по столбцам отдельно, что значительно эффективнее сжатия по строкам, так как в одном столбце данные однородны. По умолчанию используется кодек `LZ4`. Данные разбиваются на гранулы (например, по 64КБ-1МБ несжатых данных) и сжимаются независимо. Это обеспечивает баланс между скоростью и степенью сжатия.

Кодеки применяются к конкретным столбцам и бывают двух видов: общего назначения и специализированные.

Кодеки общего назначения (указываются при создании таблицы):
- `LZ4` — очень высокая скорость сжатия/распаковки, умеренная степень сжатия. Идеален для большинства случаев.
- `ZSTD` — отличный баланс: степень сжатия значительно выше, чем у `LZ4`, при приемлемой скорости. Рекомендуется для холодных данных или сильно сжимаемых текстовых полей.
- `Brotli`, `LZMAHC` — максимальная степень сжатия, но очень низкая скорость. Только для архивных данных, к которым обращаются редко.

Пример:
```sql
CREATE TABLE logs (
    message String CODEC(ZSTD(3))
) ENGINE = MergeTree ...
```

Специализированные кодеки (для снижения объёма и ускорения запросов):
- `Delta(delta_bytes)` — эффективен для монотонно изменяющихся данных. Хранит разницу между соседними значениями.
- `DoubleDelta` — вычисляет разницу разностей. Идеален для плавных рядов: метрики, временные ряды, DateTime.
- `Gorilla` (или `FPC` для `Float`) — специализирован для чисел с плавающей точкой в временных рядах.
- `T64` — пытается уменьшить разрядность целочисленных данных.
- `GCD` — эффективен для чисел, кратных некому делителю (например, цены в валюте).

Часто используют комбинацию специализированного и общего кодеков: сначала применяется специализированный (например, `DoubleDelta`), который преобразует данные в более сжимаемый вид, а затем общий (например, `ZSTD`).

Пример для метрик:
```sql
CREATE TABLE metrics (
    ts DateTime64(3) CODEC(DoubleDelta, LZ4),
    value Float64 CODEC(Gorilla, ZSTD)
) ENGINE = MergeTree ...
```

<h4>Проектирование схемы данных и денормализация</h4>

Принципы проектирования:
- Лучше широкие таблицы, чем джойны: ClickHouse оптимизирован для денормализованных данных. Объединение при запросах — самая слабая сторона СУБД, особенно для больших таблиц. Данные должны быть подготовлены для вставки.
- Столбцы, а не строки: Запрос должен читать минимальное количество столбцов. Лишние столбцы, не участвующие в запросах, — это бесполезные затраты на ввод-вывод и память.
- Ключ партиционирования (PARTITION BY): Разбивает таблицу на логические части (по дате, региону). Идеальный размер партиции — от 1 до 100+ ГБ. Партиционирование ускоряет удаление данных (`DROP PARTITION`) и управление TTL.
- Ключ сортировки (ORDER BY): Определяет, как данные физически упорядочены на диске. Порядок столбцов должен соответствовать предикатам (`WHERE`) и группировкам (`GROUP BY`) наиболее частых запросов. Первыми идут столбцы с высокой селективностью, затем менее селективные.
- Первичный ключ (PRIMARY KEY): В MergeTree-движках — это префикс ключа сортировки. Он используется для индексации (разметка в `primary.idx`). Не обязан быть уникальным. Это инструмент быстрого поиска по диапазону.
- Индексы (INDEX): Пропускающие индексы (Data Skipping Indexes) — позволяют пропускать гранулы данных при чтении. Типы: `minmax`, `set`, `ngrambf`, `tokenbf_v1` (для текста), `bloom_filter`. Создаются для столбцов, которые часто фигурируют в условиях WHERE, но не входят в начало ключа сортировки.
- Использовать подходящие движки:
  - `MergeTree` — основа для 99% таблиц (ReplicatedMergeTree для кластера).
  - `SummingMergeTree`, `AggregatingMergeTree`, `CollapsingMergeTree` — для предварительно агрегированных или изменяемых данных.
  - `Log`/`TinyLog` — для мелких временных данных.
- TTL (Time To Live):
  - Позволяет автоматически удалять или перемещать устаревшие данные на другой диск/том.
  - Критически важный инструмент управления жизненным циклом данных.

Стратегии денормализации:
- Простое сглаживание (Flattening): Вместо хранения `orders` и `order_items` отдельно, создаётся одна таблица `order_items_denorm`, где каждый товар в заказе — это строка, дополненная всеми полями из `orders`. Запросы на анализ товарооборота не требуют `JOIN`.
- Использование вложенных структур (`Nested` или `Array/Tuple`): Для случаев, когда у одной сущности есть несколько повторяющихся атрибутов. Например, если событие содержит массив тегов, или заказ содержит массив товаров с их ценами и количествами. Работа с массивами требует специальных функций (`arrayMap`, `arrayFilter`), но позволяет избежать взрывного роста строк.
- Использование словарей (С помощью `LowCardinality` или внешних словарей): Не всегда нужно дублировать длинные строки. Можно хранить `region_id UInt8` и иметь отдельную небольшую таблицу-справочник `regions`. ClickHouse умеет встраивать словари в запросы (`region_name` подставляется из `regions` по `region_id`). Или использовать `LowCardinality(String)` — словарь создаётся автоматически внутри столбца.
- Агрегация на вставке (С помощью движков SummingMergeTree/AggregatingMergeTree): Крайняя степень денормализации, когда данные агрегируются сразу при вставке. Например, Вместо обычных записей, в целевой таблице сразу вставляются посчитанные по минутам агрегаты.
- Материализованные представления: Инструмент для автоматической денормализации и агрегации потоков данных. Создаётся представление с целевой структурой и целевым запросом. При вставке в исходную таблицу данные автоматически преобразуются и попадают в целевую.

<h4>Индексы скип-листов</h4>

Индексы скип-листов (Data Skipping Indexes) в ClickHouse — это механизм, позволяющий пропускать чтение блоков данных (гранул), которые гарантированно не содержат искомых значений, что ускоряет выполнение запросов. Данные в таблицах семейства MergeTree разбиты на гранулы (блоки строк, размер которых определяется настройками `index_granularity`, обычно 8192 строки). Для каждой гранулы индекс скип-листа хранит некоторую метаинформацию (в зависимости от типа индекса) о значениях в этой грануле. При выполнении запроса ClickHouse проверяет условия запроса по индексу скип-листа и пропускает гранулы, которые не удовлетворяют условию.

Типы индексов скип-листов:
- `minmax` — хранит минимальное и максимальное значение в грануле. Эффективен для столбцов с коррелированным порядком (например, дата). Позволяет пропускать гранулы, если искомый диапазон не пересекается с `[min, max]` гранулы.
- `set(max_rows)` — хранит множество уникальных значений в грануле (до `max_rows`). Полезен для столбцов с низкой кардинальностью. Если искомое значение не входит в это множество, гранула пропускается.
- `ngrambf_v1(n, size_of_bloom_filter_in_bytes, number_of_hash_functions, random_seed)` — индекс на основе фильтра Блума для поиска подстрок. Параметр n — длина n-граммы (для разбиения строки на токены). Эффективен для текстового поиска.
- `tokenbf_v1(size_of_bloom_filter_in_bytes, number_of_hash_functions, random_seed)` — аналогичен `ngrambf_v1`, но разбивает строку на токены (слова) по пробелам и знакам пунктуации.
- `bloom_filter([false_positive])` — общий фильтр Блума для значений столбца. Эффективен для произвольных условий. Параметр `false_positive` — вероятность ложного срабатывания (от 0 до 1, по умолчанию 0.025).

Индекс определяется при создании таблицы (или добавляется позже) в секции INDEX:
```sql
-- Таблица с несколькими типами индексов
CREATE TABLE logs (
    timestamp DateTime,
    user_id UInt32,
    url String,
    response_code UInt16,
    message String,

    -- minmax индекс для временных меток
    INDEX idx_timestamp timestamp TYPE minmax GRANULARITY 4,

    -- set индекс для кодов ответа (низкая кардинальность)
    INDEX idx_response response_code TYPE set(100) GRANULARITY 2,

    -- bloom фильтр для поиска по user_id
    INDEX idx_user user_id TYPE bloom_filter GRANULARITY 4,

    -- tokenbf индекс для поиска по словам в сообщении
    INDEX idx_message message TYPE tokenbf_v1(32768, 3, 0) GRANULARITY 4
)
ENGINE = MergeTree
ORDER BY (timestamp, user_id)
```

Параметр `GRANULARITY` — сколько гранул объединяется в один индексный блок. Например, если `index_granularity` = 8192 и `GRANULARITY` = 4, то индекс будет строиться на каждые 32768 строк.

Индексы скип-листов не бесплатны: они занимают место на диске и требуют вычислений при запросах. Эффективность зависит от данных и запросов. Например, для высококардинальных столбцов `minmax` может быть бесполезен, а `bloom_filter` может помочь. Индексы скип-листов используются после первичного ключа (если он может отфильтровать гранулы, то скип-лист не нужен).

<h4>Обработка JOIN</h4>

`JOIN` в ClickHouse работает иначе, чем в традиционных реляционных базах данных, и требует особого внимания. Поддерживаемые типы `JOIN`: `INNER`, `LEFT OUTER`, `RIGHT OUTER`, `FULL OUTER`, `CROSS` (но `CROSS` без условия может быть очень тяжелым).

Стратегии выполнения:
- `hash join`: Правая таблица (subquery) читается и строится хэш-таблица в памяти. Затем читается левая таблица (main query) и происходит поиск по хэш-таблице.
- `grace hash join`: Если память недостаточна, ClickHouse может сбросить часть хэш-таблицы на диск и затем использовать дисковую версию.
- `partial merge join`: В некоторых случаях, когда данные отсортированы по ключу соединения, может использоваться слияние.

Ограничения и рекомендации:
- Порядок таблиц в `JOIN`: ClickHouse рекомендует иметь левую таблицу (ту, которая в FROM) как самую большую, а правые таблицы (в `JOIN`) — поменьше, потому что правая таблица полностью загружается в память (для `hash join`). Если правая таблица слишком велика, может не хватить памяти.
- Распределенные `JOIN`: В распределенных запросах `JOIN` может выполняться на каждой шарде отдельно (если данные правильно распределены) или на координаторе. Важно проектировать распределенные запросы так, чтобы минимизировать передачу данных по сети.
- Настройки: Существуют настройки для управления поведением `JOIN`, такие как `join_algorithm`, `max_rows_in_join`, `max_bytes_in_join`, `join_use_nulls` и т.д.

Советы по использованию `JOIN`:
- По возможности избегать `JOIN` в ClickHouse, особенно больших. Часто можно обойтись денормализацией (предварительно соединить таблицы и хранить их в одной широкой таблице).
- Если `JOIN` необходим, убедитесь, что правая таблица небольшая (например, таблица-справочник).
- Используйте необходимый тип `JOIN` (чаще всего `LEFT JOIN` или `INNER JOIN`).
- Для распределенных запросов используйте `GLOBAL JOIN`, если правая таблица находится на координаторе, а левая распределена. Это гарантирует, что правая таблица будет передана на все шарды.

<h4>Создание таблиц</h4>

Создание таблиц в ClickHouse — это важный процесс, определяющий эффективность хранения и выполнения запросов. Основной движок для большинства таблиц — семейство `MergeTree`. Синтаксис:

```sql
CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
(
    column1_name [column1_type] [DEFAULT|MATERIALIZED|ALIAS expr1] [CODEC(codec1)] [TTL expr1],
    column2_name [column2_type] [DEFAULT|MATERIALIZED|ALIAS expr2] [CODEC(codec2)] [TTL expr2],
    ...
) ENGINE = engine_name()
[PARTITION BY expr]
[ORDER BY expr]
[PRIMARY KEY expr]
[SAMPLE BY expr]
[TTL expr [DELETE|TO DISK 'xxx'|TO VOLUME 'xxx'], ...]
[SETTINGS name=value, ...]
```

Ключевые секции для `MergeTree`:
- `ENGINE` — указывает движок таблицы. Основные:
  - `MergeTree` — для нереплицированных данных.
  - `ReplicatedMergeTree` — для реплицированных данных в кластере.
  - `SummingMergeTree` — автоматически суммирует указанные числовые столбцы при слиянии кусков.
  - `AggregatingMergeTree` — для хранения предварительно агрегированных данных.
  - `CollapsingMergeTree` — для хранения изменяемых данных (с использованием специального столбца-флага).
- `PARTITION BY` — выражение для партиционирования. Обычно по дате (например, `toYYYYMM(date)`). Не рекомендуется создавать много мелких партиций.
- `ORDER BY` — ключ сортировки (обязательный параметр). Определяет, как данные будут расположены на диске. Это самый важный параметр для оптимизации запросов.
- `PRIMARY KEY` — первичный ключ (по умолчанию совпадает с ORDER BY, но может быть короче). Используется для индексации гранул.
- `SAMPLE BY` — выражение для семплирования. Если указано, то можно использовать SAMPLE в запросах.
- `TTL` — время жизни данных. Можно задать для строк (на уровне таблицы) или для столбцов.
- `SETTINGS` — дополнительные настройки таблицы, например:
  - `index_granularity` — количество строк между засечками индекса (по умолчанию 8192).
  - `storage_policy` — политика хранения (для многодисковых конфигураций).

Пример:
```sql
CREATE TABLE events
(
    event_id UInt64,
    event_date Date,
    user_id UInt32,
    event_type LowCardinality(String),
    details String CODEC(ZSTD(3))
)
ENGINE = MergeTree
PARTITION BY toYYYYMM(event_date)
ORDER BY (event_date, user_id, event_type)
SETTINGS index_granularity = 8192;
```

<h3>2. MergeTree таблицы</h3>
<h4>Основные концепции и преимущества</h4>

Данные постоянно добавляются в таблицу, а фоновый процесс периодически "мержит" (сливает) маленькие кусочки данных (куски, parts) в более крупные, упорядочивая их по первичному ключу и применяя другие оптимизации.

Как это работает:
- Вставка данных: При вставке данные попадают в оперативную память, затем формируются в новый кусок на диске. Каждый кусок — это отдельная папка с собственными данными (столбцы в сжатых `*.bin` файлах), первичным индексом и метаданными.
- Фоновое слияние (Merge): Асинхронно и автоматически ClickHouse находит соседние куски (по ключу сортировки или партициям) и сливает их в один больший, упорядоченный кусок. При слиянии данные пересортировываются, применяются правила агрегации (для AggregatingMergeTree), удаляются старые версии строк (для ReplacingMergeTree), старые куски впоследствии удаляются.

Ключевые преимущества:
- Хранение данных, отсортированных по первичному ключу: Позволяет эффективно находить данные по диапазонам ключей и быстро выполнять `GROUP BY`, `ORDER BY` по ключу.
- Индексирование: Первичный ключ создает разреженный первичный индекс (запись не на каждую строку, а на каждые N строк — "индексные гранулы"). Это позволяет быстро находить нужные блоки данных без чтения всего файла.
- Разбиение на партиции: Данные можно разделять по партициям. Удаление старой партиции — это мгновенная операция удаления папки.
- Сканирование по столбцам (Column-oriented): Данные каждого столбца хранятся отдельно, что позволяет при запросе читать только нужные столбцы, экономя I/O.
- Сжатие данных: Данные сжимаются по столбцам (обычно `LZ4` или `ZSTD`), что сильно экономит место, так как данные в одном столбце часто однородны.
- Высокая пропускная способность записи: Операции вставки добавляют новые куски, практически не блокируя чтение.

<h4>MergeTree и распределенные таблицы</h4>

MergeTree — это семейство движков таблиц в ClickHouse, которое обеспечивает хранение данных, индексацию по первичному ключу и поддержку партиционирования. Это самый мощный и гибкий движок для работы с большими данными. Основные особенности MergeTree:
- Партиционирование: Данные можно разделять по партициям (например, по дате). Это ускоряет выборки по ключу партиционирования и позволяет управлять данными (удалять целые партиции).
- Первичный ключ (Primary Key): Определяет, как данные будут отсортированы на диске. Это не уникальный идентификатор, а индекс для быстрого поиска.
- Индекс по первичному ключу: Разреженный индекс (каждая N-ая строка), который позволяет быстро находить данные без полного сканирования.
- Секции (куски данных): Данные хранятся в виде неизменяемых секций (кусков). При вставке новых данных создаются новые секции, а затем периодически происходит слияние (merge) секций для оптимизации.

Пример создания таблицы MergeTree:

```sql
CREATE TABLE logs (
    event_date Date,
    user_id Int32,
    event_type String,
    details String
) ENGINE = MergeTree
PARTITION BY toYYYYMM(event_date)
ORDER BY (user_id, event_date)
SETTINGS index_granularity = 8192;
```

Распределенные таблицы — это виртуальные таблицы, которые не хранят данные, а распределяют запросы по нескольким серверам (шардам). Они используются для горизонтального масштабирования. Как работают распределенные таблицы:
- Локальная таблица: Создается на каждом шарде и хранит часть данных.
- Распределенная таблица: Создается на каждом сервере (или на специальном сервере-координаторе) и знает, как перенаправить запрос на локальные таблицы шардов.

Пример создания распределенной таблицы:
```sql
-- Предположим, у нас есть кластер 'my_cluster' с двумя шардами, каждый из которых имеет локальную таблицу 'logs_local'
CREATE TABLE logs_distributed AS logs_local
ENGINE = Distributed('my_cluster', 'default', 'logs_local', rand());
```

Запрос к `logs_distributed` будет распределен по всем шардам, а результаты агрегированы.

<h4>Роль и настройка первичных ключей</h4>

Первичный ключ в MergeTree не гарантирует уникальности строк (если не используется движок `CollapsingMergeTree` или `ReplacingMergeTree` с явным указанием версии). Его главная роль — определять порядок сортировки данных на диске и строить разреженный первичный индекс.

Ключ задается через `ORDER BY (column1, column2, ...)` при создании таблицы (если не указан `PRIMARY KEY`, то `ORDER BY` становится первичным ключом). Данные в каждом куске отсортированы по этому ключу. Для каждых 8192 строк (индексной гранулы) сохраняется значение ключа первой строки в грануле в индексном файле. При запросе с условием по ключу, ClickHouse бинарным поиском находит нужные гранулы и читает только их, пропуская остальные данные.

Настройка:
```sql
CREATE TABLE logs (
    event_time DateTime,
    user_id UInt32,
    event_type String,
    url String
) ENGINE = MergeTree
ORDER BY (toDate(event_time), user_id) # Первичный ключ и порядок сортировки
# Можно также явно указать PRIMARY KEY, который может быть короче, чем ORDER BY:
# PRIMARY KEY (toDate(event_time)) Индекс будет только по дате
PARTITION BY toYYYYMM(event_time);
```

Правила проектирования ключа:
- Первый столбец ключа должен быть наиболее часто используемым в фильтрах.
- Ключ должен эффективно сокращать объем читаемых данных. Часто это колонки с высокой кардинальностью.
- Не стоит добавлять слишком много столбцов в ключ, так как это увеличит размер индекса и может снизить эффективность.

<h4>Партиционирование и удаление данных в MergeTree таблицах</h4>

Партиционирование разделяет данные на логические части по значению указанного выражения (чаще всего — по дате: `PARTITION BY toYYYYMM(date)`). Каждая партиция хранится отдельно и состоит из одного или нескольких кусков. Партицирование может существенно ускорить запросы, которые фильтруют данные по этому ключу, а также позволяет быстро удалять данные. Слишком мелкие партиции создают много кусков, что ухудшает производительность запросов и замедляет слияния. Размер одной партиции должен быть не менее 10 ГБ.

Способы удаления данных:
- Удаление целых партиций (самый эффективный способ):
```sql
ALTER TABLE logs DROP PARTITION '202401';
```
- Условное удаление строк (менее эффективно, lickHouse помечает строки как удаленные и физически удаляет их при следующем слиянии кусков):
```sql
ALTER TABLE logs DELETE WHERE event_time < '2024-01-01';
```

В ClickHouse для таблиц семейства MergeTree реализован мощный механизм TTL, который удаляет или перемещает устаревшие данные. Основные типы TTL:
- TTL для строк: Удаление целых строк по времени.
- TTL для столбцов: Очистка значений в столбцах (установка в default).
- TTL для партиций: Перемещение партиций на другой носитель (например, с SSD на HDD).

Пример создания таблицы с TTL:
```sql
CREATE TABLE logs (
    timestamp DateTime,
    message String,
    user_id UInt32
) ENGINE = MergeTree
PARTITION BY toYYYYMM(timestamp)
ORDER BY (timestamp, user_id)
TTL timestamp + INTERVAL 3 MONTH -- удаление старых данных
    TO DISK 'hdd' -- сначала переместим на медленный диск
    TO VOLUME 'cold' -- затем на холодное хранилище
SETTINGS storage_policy = 'tiered'; -- политика хранения
```

Настройки для автоматической очистки:
- `ttl_only_drop_parts` - удалять целые партиции, если все данные в них устарели
- `merge_with_ttl_timeout` - периодичность проверки TTL (по умолчанию 14400 секунд)

<h4>Типы данных и ограничения для столбцов</h4>

ClickHouse обладает богатым набором типов данных, оптимизированных для хранения и обработки аналитических данных.

Ключевые группы типов данных:
- Числовые: `Int8`, `Int16`, `Int32`, `Int64`, `Int128`, `Int256`, `Float32`, `Float64`, `Decimal(P, S)`.
- Строковые: `String`, `FixedString(N)`.
- Дата и время: `Date`, `DateTime`, `DateTime64`.

Структурированные и вложенные:
- `Array(T)` — массив элементов типа T.
- `Map(K, V)` — словарь.
- `Tuple(T1, T2, ...)` — кортеж.
- `Nested` — позволяет создавать вложенные структуры. Например, `Nested(ProductID UInt32, Price Decimal(10,2))`.

Специализированные (самые важные для эффективности):
- `LowCardinality(T)` — обёртка для строк или чисел с малым числом уникальных значений.
- `Nullable(T)` — позволяет хранить `NULL`.
- `Enum8`, `Enum16` — хранят строки как числа.
- `IPv4`, `IPv6` — хранятся как числа, но отображаются в привычном строковом формате.
- `UUID` — компактное хранение 128-битного UUID.

Для обычных столбцов нет жестких ограничений схемы, как в PostgreSQL: Столбцы можно спокойно добавлять и удалять. Удаление столбца физически удаляет его файлы с диска.

Ограничения по ключевым столбцам (`ORDER BY`/`PRIMARY KEY`):
- Нельзя использовать столбцы с типом `Nullable`, если движок `MergeTree` не настроен специальным образом (в старых версиях было запрещено).
- Столбцы `Array`, `Nested` и `Tuple` нельзя включать в ключ.
- Рекомендуется использовать типы с фиксированным размером для первых столбцов ключа для более эффективной работы индекса.
- Ограничения по производительности и памяти: Использование `Nullable` увеличивает накладные расходы на хранение. Типы `String` для часто фильтруемых столбцов менее эффективны, чем `LowCardinality(String)` или `Enum`.
- Индексы пропуска данных: Для неключевых столбцов можно создавать дополнительные индексы, чтобы ускорить фильтрацию. Они работают на уровне гранул.

<h4>Автоматическое сжатие данных и оптимальная гранулярность индекса</h4>

ClickHouse автоматически сжимает данные при записи на диск. Каждый столбец в каждом куске данных хранится в собственном файле, который сжимается отдельно. Это позволяет достигать высокой степени сжатия, особенно потому, что данные в столбцах часто однородны. ClickHouse поддерживает несколько алгоритмов сжатия, по умолчанию используется `LZ4`, но можно выбрать и другие, например, `ZSTD`, который обеспечивает лучшее сжатие, но требует больше ресурсов CPU.

Гранулярность индекса — это параметр, который определяет количество строк в одной индексной грануле. По умолчанию он равен 8192. Первичный индекс (который строится по первичному ключу) хранит одну запись на каждую гранулу. Таким образом, индекс является разреженным и позволяет быстро находить гранулы, которые могут содержать искомые данные.

Оптимальный размер гранулы зависит от сценария использования:
- Меньшие гранулы (например, 1024 или 2048 строк) могут ускорить точечные запросы (когда нужно найти несколько строк), потому что индекс будет более плотным, и при запросе будет прочитано меньше лишних данных. Однако это увеличит размер индекса в памяти и может замедлить запросы, которые читают большие диапазоны.
- Большие гранулы (например, 16384 или 32768) уменьшают размер индекса и ускоряют запросы по большим диапазонам, но могут читать больше лишних данных при точечных запросах.

Выбор размера гранулы — это компромисс между точностью индекса и его объемом. Чаще всего оставляют значение по умолчанию, но если у вас есть частые точечные запросы по первичному ключу, возможно, стоит уменьшить размер гранулы. Однако перед изменением этого параметра важно провести тестирование на своих данных и запросах.

Настройка гранулярности индекса выполняется при создании таблицы с помощью параметра `index_granularity` в настройках движка. Например:

```sql
CREATE TABLE my_table
(
    ...
) ENGINE = MergeTree
ORDER BY ...
SETTINGS index_granularity = 1024;
```

Также можно изменить этот параметр для уже существующей таблицы, но это повлечет за собой перестройку всех данных (нужно использовать запрос `ALTER TABLE ... MODIFY SETTING` с версии 20.4, но обычно такие изменения делают на этапе проектирования таблицы).

<h4>Enum и LowCardinality</h4>

Тип данных `Enum` предназначен для хранения строк, которые принимают ограниченный набор значений. Внутри ClickHouse хранит их как числа (`UInt8`, `UInt16` и т.д., в зависимости от количества возможных значений), но при этом позволяет работать с ними как со строками. Преимущества:
- Экономия места на диске и в оперативной памяти, так как хранится число, а не строка.
- Ускорение запросов, потому что сравнение чисел происходит быстрее, чем строк.

Тип данных `LowCardinality` — это более универсальное решение для столбцов с небольшим количеством уникальных значений (но их набор может меняться). `LowCardinality` преобразует столбец в словарь (dictionary) с числовыми ключами, но в отличие от `Enum`, словарь может обновляться при вставке новых значений. Преимущества:
- Также экономит место и ускоряет обработку данных.
- Более гибкий, чем `Enum`, потому что не требует предварительного определения всех возможных значений.
- Особенно эффективен для столбцов, которые используются в группировках и фильтрах, так как работа с числами быстрее.

Оба типа данных позволяют сжимать данные лучше, чем обычные строки, и ускоряют запросы. Однако важно не использовать их для столбцов с высокой кардинальностью (много уникальных значений), потому что это, наоборот, может замедлить работу и занять больше места из-за накладных расходов на словарь.

При вставке данных ClickHouse автоматически строит словарь для `LowCardinality` столбца. Если в столбце много уникальных значений, то словарь становится большим, и эффективность типа `LowCardinality` снижается. Для `LowCardinality` столбцов можно использовать различные кодеки сжатия. По умолчанию используется кодек, который эффективно сжимает числовые данные. При использовании `LowCardinality` в первичном ключе (ORDER BY) или в партиционировании, ClickHouse работает с числовым представлением, что обычно ускоряет сравнения.

<h4>Механизмы дедупликации</h4>

ClickHouse не имеет встроенных ограничений уникальности, но предлагает несколько подходов:
- `ReplacingMergeTree`: Удаляет дубликаты при слиянии партиций по ключу сортировки:
```sql
CREATE TABLE dedup_table (
    id UInt32,
    timestamp DateTime,
    value Float64
) ENGINE = ReplacingMergeTree
PARTITION BY toYYYYMM(timestamp)
ORDER BY (id, timestamp)
PRIMARY KEY id;
```
Дедупликация происходит только при слияниях. Следует Использовать `FINAL` в запросах или `OPTIMIZE TABLE` для принудительной дедупликации.
- `CollapsingMergeTree`: Использует специальный столбец `sign` для отметки удаленных строк:
```sql
CREATE TABLE collapsing_table (
    id UInt32,
    value Float64,
    sign Int8
) ENGINE = CollapsingMergeTree(sign)
ORDER BY id;
```
- `VersionedCollapsingMergeTree`: Улучшенная версия с версионностью:
```sql
CREATE TABLE versioned_table (
    id UInt32,
    value Float64,
    sign Int8,
    version UInt32
) ENGINE = VersionedCollapsingMergeTree(sign, version)
ORDER BY id;
```
- Дедупликация на уровне вставки:
```sql
INSERT INTO table SELECT ... FROM source_table
GROUP BY all_columns; -- агрегация перед вставкой
```

<h4>Особенности и применение различных типов таблиц</h4>

`SummingMergeTree`:
- Особенности: Автоматически суммирует значения числовых столбцов, не входящих в первичный ключ (`ORDER BY`), для всех строк с одинаковым значением ключа сортировки. Остальные нечисловые или не указанные явно столбцы берутся из первой попавшейся строки в группе (значение может быть произвольным).
- Применение: Идеален для предварительной агрегации потоковых данных, где важна сумма показателей. Позволяет значительно уменьшить объем хранимых данных и ускорить итоговые агрегационные запросы. Например, позволяет хранить ежесекундные события с последующей сверткой в минутные или часовые итоги.
- Оптимизация:
  - Гранулярность первичного ключа (`ORDER BY`): Включать в ключ сортировки только те столбцы, по которым нужно гарантировать уникальность строк после суммирования. Обычно это временные интервалы, идентификаторы измерений. Чем выше гранулярность, тем сильнее сжатие.
  - Явное указание суммируемых столбцов: При создании таблицы можно задать `SummingMergeTree([columns])`. Это предотвратит неявное суммирование других числовых полей, которые должны оставаться уникальными (например, хэши).
  - Использование Merge для финальной агрегации: Данные в SummingMergeTree могут быть не полностью свернуты. Следует использовать `sum(column) ... GROUP BY key`. Сам движок гарантирует, что финальная агрегация будет быстрой, так как число строк для обработки уже значительно уменьшено.
  - Комбинирование с партиционированием и TTL: Использовать PARTITION BY для управления данными. Добавлять TTL для автоматического удаления устаревших сырых данных, оставляя только предрассчитанные агрегаты за длительные периоды.
  - Предварительная агрегация на стороне вставки: Вместо вставки каждой микро-записи, можно агрегировать данные в батчи на стороне приложения (или с помощью буферизованных таблиц) и вставлять в `SummingMergeTree` уже частично агрегированные пакеты.
  - Выбор эффективных типов данных: Для суммируемых столбцов использовать наименьший подходящий тип (`UInt32` вместо `UInt64`, `Decimal` с нужной точностью). Это экономит место на диске и в памяти.

`ReplacingMergeTree`:
- Особенности: Оставляет только последнюю версию строки в группе с одинаковым первичным ключом. Понятие "последняя" определяется столбцом/столбцами версии (указаны при создании таблицы), или, если версия не задана, то по времени вставки (последняя вставленная в рамках незавершенного слияния). 
- Применение: Применяется для дедупликации данных "по последнему значению". Классический пример — хранение текущего состояния сущности (последний известный баланс пользователя, последние отправленные данные с датчика). Дедупликация происходит асинхронно во время фоновых слияний, поэтому для получения актуального состояния `SELECT` требует использования модификатора `FINAL` или агрегации `argMax`.
- Оптимизация:
  - Выбор столбца версии: Использовать монотонно возрастающее значение (например, `timestamp`, `version_number`, `binlog_offset`). Это гарантирует корректное определение "последней" записи.
  - Избегание FINAL в продакшене: Не использовать FINAL для больших таблиц. Вместо этого строить запросы с явным `GROUP BY` и `argMax` или проектировать систему так, чтобы запросы работали с учетом асинхронности (допускали небольшую задержку в актуальности).
  - Управление слияниями: Настройка ключа партицирования и TTL помогает своевременно удалять устаревшие данные и управлять фонными операциями.

`AggregatingMergeTree`: 
- Особенности: Не агрегирует простые значения, а хранит и объединяет состояния агрегатных функций (комбинаторы `-State`, `-SimpleState`). Для работы с ним используются материализованные представления с `AggregatingMergeTree` и специальные функции `-Merge`, `-MergeState`. 
- Применение: Применяется для создания инкрементальных материализованных представлений для сложных, многоэтапных агрегаций. Позволяет предрассчитывать и хранить такие метрики, как уникальные значения, перцентили, сложные суммы произведений и т.д., с возможностью их дальнейшего объединения. Например, для создания куба данных с предварительным расчетом количества событий, уникальных пользователей и медианной длительности для каждой комбинации (день, страна, событие).
- Оптимизация:
  - Правильное использование материализованных представлений (MV): `AggregatingMergeTree` почти всегда используется внутри MV. Шаблон:
  - Исходная таблица — любой движок (часто MergeTree). MV создается с движком `AggregatingMergeTree` и `POPULATE` (при первом создании) или данными начинают поступать после создания. В MV вставляются не raw-данные, а состояния агрегатных функций с помощью `toState`-функций.
  - Оптимизация гранулярности: Агрегировать данные до оптимального уровня детализации (например, до минуты или часа, а не до секунды). Это резко уменьшает количество состояний для хранения и объединения. Ключ сортировки (`ORDER BY`) в такой таблице должен соответствовать измерениям куба (например, (`day`, `event_type`, `country`)).
  - Выбор агрегатных функций: Использовать SimpleState-комбинаторы где возможно (например, sumSimpleState, anySimpleState). Они работают быстрее, но поддерживают меньше функций.
  - Эффективные запросы к MV: Для выборки итоговых данных из MV использовать функции `-Merge` (например, `uniqCombinedMerge(state)`). Запрос должен агрегировать по тем же ключам, что и `GROUP BY` в MV.

<h3>3. Обработка и анализ данных</h3>
<h4>Основы SQL</h4>

ClickHouse поддерживает стандартный SQL с некоторыми расширениями и особенностями.

Особенности и расширения SQL в ClickHouse:
- Строгая типизация — все данные имеют явный тип, преобразования не всегда автоматические.
- Богатый набор функций — строковые, математические, функции для работы с массивами, JSON, геоданными, машинного обучения и т.д.
- Работа с массивами и вложенными структурами — есть специальные функции и операторы.
- Модификации данных — хотя ClickHouse оптимизирован для чтения, поддерживаются INSERT, UPDATE, DELETE (последние две — мутации, которые выполняются асинхронно и тяжелы).
- Материализованные представления — позволяют автоматически обрабатывать данные при вставке.

Синтаксис `SELECT`:
```sql
SELECT [DISTINCT] expr_list
[FROM [db.]table | (subquery) | table_function] [FINAL]
[SAMPLE sample_coeff]
[GLOBAL] [ANY|ALL|ASOF] [INNER|LEFT|RIGHT|FULL|CROSS] [OUTER|SEMI|ANTI] JOIN (subquery)|table (ON <expr_list>)|(USING <column_list>)
[PREWHERE expr]
[WHERE expr]
[GROUP BY expr_list] [WITH ROLLUP|WITH CUBE] [WITH TOTALS]
[HAVING expr]
[ORDER BY expr_list] [WITH FILL] [FROM expr] [TO expr] [STEP expr]
[LIMIT [offset_value, ]n BY columns] | [n [WITH TIES]]
[LIMIT [n, ]m] | [OFFSET n ROWS FETCH NEXT m ROWS ONLY]
[SETTINGS ...]
```

Особенности:
- `PREWHERE` — оптимизация, при которой сначала читаются только столбцы, нужные для фильтрации, затем остальные.
- `SAMPLE` — семплирование данных (только для таблиц с движком, поддерживающим семплирование).
- `FINAL` — при запросе к таблице типа `CollapsingMergeTree` или `SummingMergeTree` применяет свёртку данных.

Синтаксис `INSERT`:
```sql
INSERT INTO [db.]table [(column1, column2, ...)] VALUES (v11, v12, ...), (v21, v22, ...), ...
INSERT INTO [db.]table [(column1, column2, ...)] SELECT ...
```

Синтаксис `DELETE` и `UPDATE`:
```sql
ALTER TABLE [db.]table DELETE WHERE filter_expr;
ALTER TABLE [db.]table UPDATE column1 = expr1 [, ...] WHERE filter_expr;
```

Управление таблицами:
```sql
CREATE TABLE ...
ALTER TABLE ... ADD|DROP|CLEAR|COMMENT|MODIFY COLUMN ...
DROP TABLE ...
RENAME TABLE ...
```

Простой запрос с агрегацией:
```sql
SELECT toStartOfHour(event_date) AS hour,
       event_type,
       count() AS events_count,
       uniq(user_id) AS unique_users
  FROM events
 WHERE event_date >= today() - 7
 GROUP BY hour, event_type
 ORDER BY hour, event_type
```

Использование `JOIN`:
```sql
SELECT event_date,
       segment,
       count() AS events
  FROM events
       LEFT JOIN users ON events.user_id = users.id
 GROUP BY event_date, segment
```

Использование оконных функций:
```sql
SELECT user_id,
       event_date,
       rank() OVER (PARTITION BY user_id ORDER BY event_date) AS rank
  FROM events
```

Фильтрация с подзапросом:
```sql
SELECT user_id, name
  FROM users
 WHERE user_id IN (
       SELECT DISTINCT user_id
         FROM purchases
        WHERE amount > 1000);
```

<h4>Агрегатные функции</h4>

ClickHouse имеет огромный набор агрегатных функций, которые можно разделить на несколько категорий:
- Базовые статистические:
  - `count()` — подсчет строк.
  - `sum()` — сумма.
  - `avg()` — среднее.
  - `min()` / `max()` — минимум/максимум.
  - `varPop()` / `stddevPop()` — дисперсия и стандартное отклонение по генеральной совокупности.
- Приблизительные:
  - `uniq()` / `uniqCombined()` — приблизительное количество уникальных значений.
  - `quantile()` / `quantiles()` — приблизительные квантили.
- Точные (могут быть тяжелыми на больших данных):
  - `uniqExact()` — точный подсчет уникальных значений (аналог `COUNT(DISTINCT ...)` в стандартном SQL).
  - `quantileExact()` — точные квантили.
- Агрегаты по массивам:
  - `groupArray()` — собирает значения в массив.
  - `groupUniqArray()` — собирает уникальные значения в массив.
- Условные агрегаты:
  - `sumIf(column, cond)` — сумма значений, удовлетворяющих условию.
  - `countIf(cond)` — подсчет значений, удовлетворяющих условию.
  - `avgIf(column, cond)` — среднее значений, удовлетворяющих условию.
- Комбинаторные модификаторы:
  - `aggFunctionDistinct()` — например, `sumDistinct(column)`, `countDistinct(column)`.
  - `aggFunctionIf()` — например, `avgIf(column, condition)`.
  - `aggFunctionArray()` — например, `sumArray(arr_column)`.

Также можно создавать пользовательские агрегатные функции, например, создадим функцию, рассчитывающую среднее геометрическое:
```sql
CREATE AGGREGATING FUNCTION geometricMean AS (x) -> exp(avg(log(x)));
SELECT geometricMean(value) FROM table;
```

<h4>Оконные функции</h4>

Поддержка оконных функций появилась в релизе 21.3 и активно развивается. Синтаксис близок к стандарту SQL.

Функции смещения:
- `lagInFrame(column, offset [, default])` — значение из строки, отстающей на `offset` в рамках окна.
- `leadInFrame(column, offset [, default])` — значение из строки, опережающей на `offset`.
- `first_value(column)` / `last_value(column)` — первое/последнее значение в окне.

Ранжирующие:
- `row_number()` — порядковый номер строки в окне.
- `rank()` / `dense_rank()` — ранг строки с пропусками/без пропусков.
- `ntile(n)` — разбивает строки окна на n групп.

Агрегатные как оконные: Любую агрегатную функцию можно использовать в окне.
- `sum(column) OVER (PARTITION BY id ORDER BY date)`
- `avg(column) OVER (PARTITION BY group)`
- `uniq(column) OVER (...)`

ClickHouse требует явного указания границ окна (ORDER BY в определении окна) для многих функций. Также есть две формы:
- `lag(column)` — относительно текущей строки всего набора.
- `lagInFrame(column)` — относительно текущей строки в пределах окна (`PARTITION BY` + `ORDER BY`).

Применение оконной функции:
```sql
SELECT date,
       amount,
       sum(amount) OVER (ORDER BY date) AS cumulative_sum
  FROM sales
 ORDER BY date;
```

То же самое с `runningAccumulate()`:
```sql
SELECT date,
       amount,
       runningAccumulate(amount) AS cumulative_sum
  FROM (SELECT date,
              amount
         FROM sales
        ORDER BY date)
 ORDER BY date;
```

<h4>Работа с массивами</h4>

Массивы в ClickHouse — это полноценный и эффективный тип данных `Array(T)`, где `T` — тип элементов (например, `Array(Int32)`, `Array(String)`, даже `Array(Array(String)))`.

Ключевые особенности:
- Производительность: Оптимизированы для хранения и обработки в колоночном формате. Операции над массивами (особенно в агрегатных функциях) работают очень быстро.
- Строгая типизация: Все элементы массива должны быть одного типа.
- Динамический размер: Размер массива не фиксирован при создании таблицы, но есть практические ограничения по объему памяти.
- Вложенность: Поддерживаются многомерные массивы `(Array(Array(T)))`.
- Богатый набор функций: Более 50 функций для работы с массивами: создание, фильтрация, сортировка, поиск, и т.д.
- Использование для денормализации: Частая практика — хранить историю событий или теги объекта не в отдельной нормализованной таблице, а в виде массива в основной таблице. Это резко сокращает количество JOIN-ов и ускоряет многие запросы.

Пример таблицы, в которой для каждого пользователя и дня хранятся массивы ID сессий и просмотренных страниц:
```sql
CREATE TABLE user_sessions
(
    user_id UInt64,
    date Date,
    session_ids Array(UInt32),
    pageviews Array(String)
) ENGINE = MergeTree()
ORDER BY (user_id, date);
```

Функции для работы с массивами:
- Создание массивов:
  - `array(elem1, elem2, ...)` или `[elem1, elem2, ...]` - создание массива
  - `range(end), range(start, end)` - массив чисел
  - `arrayWithConstant(n, value)` - массив из n одинаковых значений
  - `arrayPopFront(array), arrayPopBack(array)` - удаление первого/последнего элемента
  - `arrayPushFront(array, elem)`, `arrayPushBack(array, elem)` - добавление элемента
  - `arrayResize(array, size[, value])` - изменение размера массива
- Преобразование типов:
  - `arrayConcat(arr1, arr2, ...)` - объединение массивов
  - `arraySlice(array, offset[, length])` - получение среза
  - `arrayReverse(array)` - обратный порядок
  - `arrayFlatten(array_of_arrays)` - сглаживание многомерного массива
  - `arrayCompact(array)` - удаление последовательных дубликатов
  - `arrayZip(arr1, arr2, ...)` - создание массива кортежей
- Поиск и проверка:
  - `has(array, elem)` - проверка наличия элемента
  - `hasAny(array1, array2)` - есть ли общие элементы
  - `hasAll(array1, array2)` - содержит ли все элементы
  - `indexOf(array, elem)` - индекс первого вхождения
  - `countEqual(array, value)` - количество элементов равных value
  - `arrayEnumerate(array)` - массив индексов [1,2,3,...]
  - `arrayEnumerateUniq(array)` - нумерация уникальных значений
- Сортировка и фильтрация:
  - `arraySort(array)` - сортировка по возрастанию
  - `arraySort(func, array)` - сортировка с пользовательской функцией
  - `arrayReverseSort(array)` - сортировка по убыванию
  - `arrayDistinct(array)` - уникальные значения
  - `arrayFilter(func, array)` - фильтрация с помощью лямбда-функции
  - `arrayFirst(func, array)` - первый элемент, удовлетворяющий условию
  - `arrayLast(func, array)` - последний элемент, удовлетворяющий условию
- Высшего порядка (лямбда-функции):
  - `arrayMap(func, array1 [, array2, ...])` - применение функции к элементам
  - `arrayFilter(func, array)` - фильтрация
  - `arrayCount(func, array)`- подсчет элементов по условию
  - `arrayExists(func, array)` - существует ли элемент по условию
  - `arrayAll(func, array)` - все ли элементы удовлетворяют условию
  - `arraySum(array)` - сумма элементов
  - `arrayAvg(array)` - среднее значение
  - `arrayCumSum(array)` - кумулятивная сумма
- Агрегатные функции:
  - `groupArray(x)` - агрегация значений в массив
  - `groupArrayArray(arr)` - агрегация массивов в один
  - `groupUniqArray(x)` - агрегация уникальных значений
  - `arrayReduce('agg_func', array)` - применение агрегатной функции к массиву
  - `arrayReduceInRanges('agg_func', ranges, array)` - агрегация по диапазонам
- Работа со строками и массивами:
  - `splitByChar(delimiter, string)` - разбиение строки на массив
  - `splitByString(delimiter, string)` - разбиение по подстроке
  - `extractAll(text, regexp)` - извлечение всех совпадений с regexp
  - `arrayStringConcat(array[, delimiter])` - объединение массива строк
- Математические операции:
  - `arrayProduct(array)` - произведение элементов
  - `arrayDifference(array)` - разности соседних элементов
  - `arrayCumSum(array)` - накопленная сумма
  - `arrayCumSumNonNegative(array)` - накопленная сумма (не отрицательная)
- Специальные функции:
  - `arrayJoin(array)` - разворот массива в строки
  - `arrayIntersect(arr1, arr2, ...)` - пересечение массивов
  - `arrayMax(array)` - максимальный элемент
  - `arrayMin(array)` - минимальный элемент
  - `arrayUniq(array)` - количество уникальных элементов
  - `arrayCount(array)` - количество элементов (аналог length)
- Для многомерных массивов:
  - `arrayFlatten(array_of_arrays)` - преобразование в одномерный
  - `arrayMap(func, arr1, ...)` - работает с несколькими массивами
  - `arrayZip(arr1, arr2, ...)` - создание массива кортежей

Примеры:
```sql
SELECT array(1,2,3) AS arr, [4,5,6] AS arr2;
SELECT arr[1] FROM (SELECT [10,20,30] AS arr);
SELECT has([1,2,3], 2)                    -- возвращает 1.
SELECT slice([1,2,3,4,5], 2, 3)           -- возвращает [2,3,4].
SELECT arrayPushBack([1,2], 3)            -- возвращает [1,2,3].
SELECT arraySort([3,1,2])                 -- возвращает [1,2,3].
SELECT groupArray(number) FROM numbers(5) -- возвращает [0,1,2,3,4].
SELECT arrayMap(x -> x * 2, [1,2,3])      -- возвращает [2,4,6].
SELECT arrayEnumerate([10,20,10,30])      -- возвращает [1,2,3,4].
```

Разворот массива с помощью `arrayJoin()`:
```sql
SELECT
    user_id,
    date,
    arrayJoin(session_ids) AS session_id
FROM user_sessions
WHERE date = today()

┌─user_id─┬─date───────┬─session_id─┐
│     101 │ 2023-10-26 │       5001 │
│     101 │ 2023-10-26 │       5002 │
│     101 │ 2023-10-26 │       5003 │
└─────────┴────────────┴────────────┘
```

Сворачивание в массив с помощью `groupArray()`:
```sql
SELECT
    user_id,
    toDate(event_time) AS date,
    groupArray(page_url) AS pageviews,                -- Массив всех URL
    groupArrayDistinct(page_url) AS unique_pageviews, -- Массив уникальных URL
    count() AS pageview_count
FROM events
GROUP BY user_id, toDate(event_time)
ORDER BY user_id, date

┌─user_id─┬─date───────┬─pageviews────────────────────────────────┬─unique_pageviews─────────────────┬─pageview_count─┐
│     101 │ 2023-10-26 │ ['/home','/product/123','/home','/cart'] │ ['/home','/product/123','/cart'] │              4 │
└─────────┴────────────┴──────────────────────────────────────────┴──────────────────────────────────┴────────────────┘
```

<h4>Работа с географическими данными и лог-файлами</h4>

ClickHouse имеет встроенную поддержку геоданных:
- Типы данных:
  - `Point`, `Ring`, `Polygon`, `MultiPolygon` для хранения геометрий.
  - `Tuple(Float64, Float64)` как простая альтернатива для точки (долгота, широта).
- Функции:
  - Расстояние: `greatCircleDistance(lon1, lat1, lon2, lat2)` (расстояние по ортодромии), `geoDistance` (по формуле Винсенти).
  - Содержание: `pointInPolygon((lon, lat), polygon)` – проверка, попадает ли точка в полигон.

Поиск точек в радиусе:
```sql
SELECT *
FROM geo_events
WHERE greatCircleDistance(lon, lat, 37.617, 55.755) < 1000 -- В метрах
```

ClickHouse идеально подходит для анализа логов (например, nginx, application logs). Структура таблицы: Часто используют партиционирование по дате (`PARTITION BY toYYYYMM(date)`) и порядок по времени и тегу (`ORDER BY (date, timestamp, service)`).

Загрузка логов:
- Прямой парсинг: Можно загружать сырые логи, используя функции для извлечения данных прямо в `INSERT`.
- Pipeline: Filebeat/Logstash → Kafka → ClickHouse (через движок Kafka).
- Встроенный парсинг: Использование формата Regexp или TSKV для полуструктурированных логов.

Анализ:
- Поиск по подстроке: `WHERE message LIKE '%ERROR%'` (использует bloom-фильтры, если включены).
- Парсинг на лету: `extract(message, 'regexp')`, `splitByChar(' ', message)[3]`.
- Агрегация по временным окнам: `GROUP BY toStartOfMinute(timestamp)`, `toStartOfInterval(timestamp, INTERVAL 5 SECOND)`.
- Анализ частот: `count()`, `uniq(ip)`, `quantile(0.99)(response_time_ms)`.
- Движок AsynchronousInsert: Специально для логов — накапливает вставки в буфер и флаширует пачками, снижая нагрузку.

<h4>Создание и управление БД и таблицами</h4>

ClickHouse поддерживает несколько движков баз данных, но чаще всего используется обычная база данных (движок Atomic по умолчанию, начиная с версии 20.5).

Создание базы данных:
```sql
CREATE DATABASE mydb;
```
По умолчанию будет использован движок Atomic.

Удаление базы данных:
```sql
DROP DATABASE mydb;
```

Создание таблицы: Указываем имя, столбцы с типами, движок и параметры (партиционирование, сортировка и т.д.).
```sql
CREATE TABLE mydb.mytable (
    id UInt32,
    timestamp DateTime,
    value Float64
) ENGINE = MergeTree
PARTITION BY toYYYYMM(timestamp)
ORDER BY (id, timestamp);
```

Удаление таблицы:
```sql
DROP TABLE mydb.mytable;
```

Изменение таблицы: ClickHouse поддерживает ограниченный набор операций ALTER (добавление/удаление столбцов, изменение комментариев, сброс настроек).
```sql
ALTER TABLE mydb.mytable ADD COLUMN comment String;
ALTER TABLE mydb.mytable DROP COLUMN comment;
```

Клонирование структуры таблицы:
```sql
CREATE TABLE mydb.new_table AS mydb.mytable;
```

Переименование таблицы:
```sql
RENAME TABLE mydb.mytable TO mydb.mytable_old;
```

Вставка данных:
```sql
INSERT INTO mydb.mytable (id, timestamp, value) VALUES (1, now(), 42.5);
```

Обычно вставка выполняется большими пачками (тысячами или миллионами строк) для эффективности.

Обновление и удаление данных: ClickHouse не оптимизирован для точечных обновлений и удалений, но поддерживает их (с использованием мутаций).
```sql
ALTER TABLE mydb.mytable UPDATE value = 100 WHERE id = 1;
ALTER TABLE mydb.mytable DELETE WHERE id = 1;
```
Мутации выполняются асинхронно и могут быть тяжелыми для производительности.

ClickHouse предоставляет системные таблицы для мониторинга и управления, например:
- `system.tables` — информация о таблицах.
- `system.databases` — информация о базах данных.
- `system.parts` — информация о кусках данных таблиц семейства MergeTree.

Пример запроса к системной таблице:
```sql
SELECT name, total_rows FROM system.tables WHERE database = 'mydb';
```

<h2>Оптимизация и мониторинг</h2>
<h3>1. Производительность и оптимизация</h3>
<h4>Основы производительности</h4>

Производительность ClickHouse базируется на нескольких архитектурных принципах:
- Колоночное хранение: Данные хранятся не по строкам, а по столбцам. Это позволяет при запросе читать только необходимые столбцы, а не всю строку (снижение I/O), эффективно сжимать данные (однотипные данные в колонке сжимаются лучше).
- Векторизованная обработка запросов: Операции выполняются не над отдельными значениями, а над целыми блоками данных (векторами, обычно 65536 строк). Это максимально эффективно использует возможности CPU (кеш, конвейер инструкций SIMD).
- Индексные структуры (первичный ключ): Это не B-tree индекс как в OLAP. ClickHouse использует разреженный индекс. Для каждой N-ой строки (где N — `index_granularity`, обычно 8192) запоминается значение первичного ключа и адрес начала этого куска (гранулы) данных на диске. При запросе с `WHERE` по первичному ключу система быстро находит нужные гранулы и считывает только их.
- Data Skipping Индексы: Дополнительные легковесные индексы позволяют "пропускать" гранулы данных, которые гарантированно не содержат искомых значений.
- Материализованные представления и агрегатные функции: Возможность предварительно вычислять и хранить агрегированные данные, что ускоряет выполнение типовых отчетов в тысячи раз.
- Работа с данными партиями (batch): ClickHouse оптимизирован под вставку больших пачек данных (тысячи-миллионы строк за раз), а не одиночных `INSERT`. Мелкие частые вставки крайне неэффективны.
- Локальность данных: Высокая производительность достигается при работе с данными на локальных SSD/NVMe дисках. Распределенность (кластеры) — это надстройка для масштабирования и отказоустойчивости.

<h4>Настройки производительности</h4>

Настройки можно разделить на несколько групп. Основной конфигурационный файл — `config.xml` и пользовательские настройки в `users.xml`.

Настройки, влияющие на слияния (MergeTree):
- `max_bytes_to_merge_at_max_space_in_pool` (по умолчанию ~150 ГБ) — максимальный суммарный размер данных в одном слиянии. Слишком большие значения могут привести к долгим слияниям и нехватке места на диске.
- `number_of_free_entries_in_pool_to_lower_max_size_of_merge` — управляет приоритизацией слияний. Увеличение значения заставляет ClickHouse делать более мелкие слияния, что полезно при нехватке места.

Настройки для вставки данных:
- `max_insert_block_size` (по умолчанию 1,048,576) — размер блока для вставки. Должен быть согласован с настройками клиента.
- `min_insert_block_size_rows` — минимальный размер блока для вставки. Если блок меньше, он будет аккумулироваться в памяти до достижения этого порога. Важно для мелких вставок.
- `async_insert` (рекомендуется 1) — включает асинхронную вставку, когда данные сначала буферизуются, а потом записываются пачкой. Ключевая настройка для работы с мелкими вставками.
- `wait_for_async_insert` (по умолчанию 1) — ждать ли подтверждения записи при асинхронной вставке.

Настройки для чтения (`SELECT`):
- `max_threads` — максимальное количество потоков для обработки одного запроса (см. ниже).
- `max_block_size` — размер блока, который используется при обработке и возврате данных.
- `preferred_block_size_bytes` — система будет стараться формировать блоки указанного размера в байтах для оптимизации работы кэша CPU.
- `use_uncompressed_cache` (по умолчанию 0) — использовать ли кэш разжатых данных. Включение (1) может сильно ускорить повторяющиеся запросы к одним и тем же данным, если они влезают в память.

Глобальные настройки ресурсов:
- `background_pool_size` / `background_schedule_pool_size` — количество потоков для фоновых операций (слияния, перемещения). По умолчанию равно количеству ядер CPU.
- `background_buffer_flush_schedule_pool_size` — пул для сброса буферных таблиц.

<h4>Настройки памяти и процессора</h4>

ClickHouse использует память для обработки запросов, фоновых операций и кэшей. Важно разделить общий объем памяти между этими потребителями.

Ограничение памяти на запрос:
- `max_memory_usage` (по умолчанию 10 ГБ) — абсолютный максимум RAM на один запрос. Самая важная настройка для предотвращения OOM (Out Of Memory). Устанавливается в байтах (например, 20GiB).
- `max_memory_usage_for_user` — лимит на все запросы одного пользователя.
- `max_server_memory_usage` — глобальный лимит на всю RAM сервера (не рекомендуется, лучше настраивать через cgroups).

Память для JOIN:
- `join_algorithm` — предпочитаемый алгоритм (hash, parallel_hash, partial_merge, auto). hash требует хранения правой таблицы в памяти, но быстрее.
- `max_bytes_in_join` — ограничение памяти для хеш-таблицы JOIN.
- `join_use_nulls` — влияет на использование памяти.

Память для агрегации:
- `max_bytes_before_external_group_by` / `max_bytes_before_external_sort` — если агрегация или сортировка превышает этот порог, данные сбрасываются на диск (внешняя агрегация/сортировка). Критически важная настройка для тяжелых запросов, предотвращающая OOM. Типичное значение — 50-80% от max_memory_usage.

Кэши:
- `uncompressed_cache_size` (по умолчанию 8 GiB) — объем кэша разжатых данных. Увеличивайте, если горячие данные не помещаются в RAM.
- `mark_cache_size` (по умолчанию 5 GiB) — кэш для файлов индексов (.mrk). Должен быть достаточно большим, чтобы хранить индексы всех активно используемых таблиц.

Процессор
- `max_threads` (по умолчанию равно количеству ядер CPU) — определяет параллелизм на уровне данных. Для запросов с GROUP BY, ORDER BY данные разбиваются на потоки. Установка значения больше числа физических ядер редко дает выигрыш и может навредить из-за переключения контекста. Для легких запросов его можно уменьшить.
- `parallelize_output_from_storages` — распараллеливать ли чтение с диска. Для быстрых SSD часто имеет смысл включить.
- `max_streams_to_max_threads_ratio` (по умолчанию 1) — множитель для создания потоков при чтении с удаленных источников (например, при чтении из S3 или HDFS).

Настройки для операций с диском:
- `local_filesystem_read_method` / `..._write_method` — методы ввода-вывода (`pread`, `read`, `mmap`). Для NVMe SSD часто pread или read показывают лучшую производительность, чем mmap.
- `min_bytes_to_use_direct_io` (по умолчанию 0) — порог для использования прямого ввода-вывода (O_DIRECT), минуя кэш ОС. При использовании быстрых NVMe налету и наличии большого объема RAM в самом ClickHouse, установка значения (например, 1GiB) может освободить кэш ОС для полезной работы.

<h4>План выполнения запроса</h4>

Пример запроса:
```sql
EXPLAIN
SELECT
    user_id,
    COUNT() as page_views,
    SUM(revenue) as total_revenue
FROM user_events
WHERE
    event_date >= '2024-01-01'
    AND event_type = 'page_view'
    AND country IN ('US', 'UK', 'DE')
GROUP BY user_id
HAVING total_revenue > 100
ORDER BY total_revenue DESC
LIMIT 100
```

План выполнения:
```bash
Expression (Projection)
  Limit
    Expression (Before ORDER BY)
      Sorting (Merge)
        Aggregating
          Expression (Before GROUP BY)
            Filter (HAVING)
              Expression (Before GROUP BY)
                Filter (WHERE)
                  ReadFromMergeTree (user_events)
```

Для получения детального плана:
```sql
EXPLAIN
    header = 1,
    description = 1,
    actions = 1,
    json = 0
SELECT ... -- тот же запрос
```

Разбор плана запроса (выполнение снизу вверх):
1. ReadFromMergeTree (user_events) - чтение данных:
```bash
ReadFromMergeTree
Read Type: Default
Parts: 12 parts (по числу партиций)
Granules: 2450 granules (гранулы для чтения)
Indexes:
  PrimaryKey: (event_date, event_type)
  MinMax: event_date: ['2024-01-01', +Inf)
  Partition: event_date
```
2. Filter (WHERE) - применение условий WHERE:
```bash
Filter
Predicate:
  (event_date >= '2024-01-01')
  AND (event_type = 'page_view')
  AND (country IN ('US', 'UK', 'DE'))
Rows Before: 10,000,000 (оценка)
Rows After: 500,000 (оценка после фильтрации)
```
3. Expression (Before GROUP BY) - вычисление выражений:
```bash
Expression
Actions: PROJECT user_id, revenue
```
4. Aggregating - агрегация:
```bash
Aggregating
Keys: user_id
Aggregates: count(), sum(revenue)
Memory Usage: ~256 MB (оценка)
```
5. Filter (HAVING) - применение HAVING:
```bash
Filter
Predicate: total_revenue > 100
Rows Before: 50,000
Rows After: 10,000
```
6. Sorting (Merge) - сортировка:
```bash
Sorting
Keys: total_revenue DESC
Limit: 100
Memory Usage: ~16 MB
```
7. Limit - применение LIMIT
8. Expression (Projection) - финальная проекция столбцов

В последних версиях ClickHouse появилась возможность использовать подсказки для влияния на план выполнения. Они задаются в виде комментариев в SQL-запросе:
```sql
-- Указание предпочтительного алгоритма JOIN
SELECT /*+ JOIN_DEFAULT_STRATEGY('hash') */ 
    a.*, b.name
FROM table_a AS a
JOIN table_b AS b ON a.id = b.id;

-- Указание направления JOIN (чтобы не переставлять таблицы местами)
SELECT /*+ NO_REORDER_JOINS() */ 
    a.*, b.name
FROM table_a AS a
JOIN table_b AS b ON a.id = b.id;

-- Принудительное использование определенного индекса (если есть несколько)
SELECT /*+ INDEX(my_table my_index) */ 
    *
FROM my_table
WHERE column = 'value';
```

<h4>Оптимизация запросов в MergeTree таблицах</h4>

Общие методы оптимизации:
- Партиционирование (`PARTITION BY`): Управление жизненным циклом данных (TTL, удаление партиций DROP PARTITION).  Партиция должна содержать от 1 до ~100 гигабайт данных. Слишком мелкие партиции -> много мелких кусков -> проблемы с мерджем и запросами. Слишком крупные партиции -> неэффективное удаление данных. Типичный ключ партицирования — по дате (`toYYYYMM(event_date)`, `toMonday(event_date)`).
- Ключ сортировки/Первичный ключ (ORDER BY / PRIMARY KEY): Определяет, как данные физически упорядочены на диске. Первый столбец в ключе должен быть самым часто используемым в фильтрах (`WHERE`, `GROUP BY`). Использовать кардинальность от высокой к низкой (например, user_id, date). Ключ работает по принципу пропуска индекса. Он указывает на "гранулы" данных (по умолчанию 8192 строки). Чем длиннее ключ, тем больше индексный файл, но точнее поиск. Не рекмендуется делать ключ слишком "широким", 2-4 столбца — часто оптимально.
- Индексация (Пропускающие индексы - Skipping Indexes): Дополнение к первичному индексу для столбцов, не входящих в `ORDER BY`. Пример (`bloom_filter` для поиска по подстроке):
```sql
ALTER TABLE logs ADD INDEX idx_message message TYPE bloom_filter GRANULARITY 1;
```
Индексы работают на уровне гранул. Они не гарантируют пропуск гранулы, а только позволяют это сделать. Эффективность зависит от данных.

Оптимизация чтения:
- Гранулярность: Настройка `index_granularity` (по умолч. 8192). Меньшее значение -> точнее индекс -> больше чтений. Увеличивать для очень больших таблиц.
- Кодеки сжатия столбцов: CODEC(Delta, ZSTD). Delta эффективен для монотонно возрастающих данных (timestamp, id). ZSTD дает хороший баланс скорости/сжатия.

Оптимизация вставки:
- Вставлять пачками (batch). Одна вставка 100к-1м строк гораздо эффективнее 1000 вставок по 100 строк.
- Использовать асинхронные вставки: через Buffer таблицу или `async_insert=1`.
```sql
CREATE TABLE logs_buffer AS logs ENGINE = Buffer(default, logs, 16, 10, 100, 10000, 1000000, 10000000, 100000000);
```
- Контролировать размер кусков: Настройки `min_bytes_for_wide_part`, `min_rows_for_wide_part` решают, хранить ли часть в "широком" (каждый столбец отдельно) или "компактном" (все в одном файле) формате. Для больших вставок важен "широкий" формат.
Параллельные вставки в разные партиции не блокируют друг друга.

<h4>Использование Materialized View для оптимизации</h4>

`MATERIALIZED VIEW` (MV) в ClickHouse — это не просто кэш запроса, а триггер на вставку в исходную таблицу. При вставке данных в исходную таблицу, кусок данных проходит через движок MV и записывается в целевую таблицу, где хранятся материализованные данные.

```sql
-- 1. Исходная детальная таблица
CREATE TABLE sales_detail (
    dt DateTime,
    product_id UInt32,
    revenue Decimal(10,2)
) ENGINE = MergeTree PARTITION BY toYYYYMM(dt) ORDER BY (dt, product_id);

-- 2. Целевая таблица для агрегированных данных
CREATE TABLE sales_daily (
    date Date,
    product_id UInt32,
    sum_revenue AggregateFunction(sum, Decimal(10,2)),
    count_revenue AggregateFunction(count, Decimal(10,2))
) ENGINE = AggregatingMergeTree()
PARTITION BY toYYYYMM(date) ORDER BY (date, product_id);

-- 3. MV, которая пересчитывает агрегаты при каждой вставке
CREATE MATERIALIZED VIEW sales_daily_mv TO sales_daily
AS
SELECT toDate(dt) AS date,
       product_id,
       sumState(revenue) AS sum_revenue,
       countState(revenue) AS count_revenue
  FROM sales_detail
 GROUP BY date, product_id;
```

Теперь запрос к агрегированным данным быстрый:
```sql
SELECT date,
       product_id,
       sumMerge(sum_revenue),
       countMerge(count_revenue)
  FROM sales_daily
 GROUP BY date, product_id;
```

Важные нюансы:
- Данные в MV появляются только после вставки. Исторические данные в неё не попадут автоматически. Их нужно подтянуть: 
```sql
INSERT INTO sales_daily SELECT ... FROM sales_detail WHERE dt < ...
```
- MV увеличивает нагрузку на вставку, так как данные обрабатываются дважды.
- Для агрегаций используется движок `AggregatingMergeTree` и функции `*State`/`*Merge`. Это "правильный" способ в ClickHouse.
- Можно создавать несколько MV на одну таблицу для разных агрегаций (по минутам, по часам, по разным измерениям).

<h4>Стратегии оптимизации запросов</h4>

Оптимизация в ClickHouse начинается на уровне проектирования таблиц, но эти стратегии касаются написания самих запросов:
- Использовать движок MergeTree и его производные.
- Минимизировать обрабатываемый объем данных(prewhere vs where): `PREWHERE` — это фильтрация, которая происходит до чтения столбцов, на уровне чтения кусков (chunks). Используется для условий, которые отфильтровывают много строк, особенно если задействованы столбцы, не входящие в первичный ключ. ClickHouse часто применяет `PREWHERE` автоматически, но можно указать вручную.
```sql
SELECT * FROM logs WHERE data LIKE '%error%'       -- Плохо (для большого столбца data)
SELECT * FROM logs PREWHERE data LIKE '%error%'    -- Лучше
```
- Избегать `SELECT *`. Читать только необходимые столбцы. В колоночной СУБД чтение лишнего столбца = чтение отдельного файла на диске.
- Сортировать данные по ключевым фильтрам. Если `ORDER BY` в запросе совпадает с ключом сортировки таблицы, ClickHouse сможет эффективно использовать первичный индекс для прыжков по данным.
- Избегать или аккуратно используйте `JOIN`. ClickHouse не реляционная СУБД, `JOIN` — не её сильная сторона. Для этого использовать денормализацию, движки Join или словари (DICTIONARY). Если `JOIN` необходим, правая таблица должна быть в памяти. Используйте движок Join для маленьких справочников.
```sql
CREATE TABLE small_dict (...) ENGINE = Join(ANY, LEFT, id);    -- Создание таблицы для JOIN в оперативке
```
- Агрегировать данные насколько возможно. Для этого использовать приближённые алгоритмы (`uniqCombined`, `quantileTDigest`), если точность не критична — они в разы быстрее.
- Использовать подзапросы вместо `JOIN` там, где это уместно. Часто фильтрация подзапросом эффективнее.
Контролировать память и параллелизм: настройки `max_memory_usage`, `max_threads`.

<h4>Оптимизация запросов к внешним источникам</h4>

ClickHouse может запрашивать данные из внешних источников через табличные функции (такие как `mysql`, `postgresql`, `s3`, `url`, `hdfs`) или через движки таблиц (например, MySQL, PostgreSQL, ODBC). Оптимизация таких запросов критична, поскольку внешние источники могут быть медленными. Стратегии оптимизации:
- Фильтрация и агрегация на стороне ClickHouse: ClickHouse старается передать часть запроса (предикаты, агрегации) во внешкую БД, но это зависит от возможностей внешней системы. Всегда старайтесь явно указать фильтры, чтобы уменьшить объем получаемых данных.
- Использование кэширования: Для повторяющихся запросов к внешним источникам можно использовать кэш. Например, для S3 есть настройки кэширования.
- Параллельное чтение: Если внешний источник поддерживает, можно читать данные параллельно. Например, для S3 можно использовать `s3_parallel_read=1`.
- Прямое соединение с внешними БД: При использовании движков таблиц (например, MySQL) запросы преобразуются в SQL запросы к внешней БД. Убедитесь, что на внешней БД есть индексы для ускорения.
- Буферизация и материализация: Если внешние данные используются часто, рассмотрите возможность загрузки их в ClickHouse (например, в таблицу с движком Memory или MergeTree). Можно использовать `CREATE TABLE AS SELECT ...` или `MATERIALIZED VIEW` для синхронизации.
- Настройки времени ожидания и повторов: Увеличьте `connect_timeout` и `receive_timeout` при работе с медленными сетями.

Пример запроса к MySQL с фильтрацией:
```sql
SELECT *
  FROM mysql('host:port', 'database', 'table', 'user', 'password')
 WHERE id = 1;
```
ClickHouse попытается отправить условие `id=1` в MySQL, чтобы тот вернул только одну строку.

<h4>Оптимизация запросов с использованием шардирования</h4>

Шардирование в ClickHouse — это горизонтальное разделение данных между несколькими серверами. Оно используется для масштабирования. Стратегии оптимизации:
- Распределенные запросы: Использовать Distributed движок для запросов ко всем шардам. ClickHouse может выполнять запросы параллельно на каждом шарде и агрегировать результаты.
- Локальность данных: При использовании Distributed движок может хранить локальную таблицу на каждом шарде. Убедиться, что запросы используют локальные таблицы, а не удаленные.
- Распределенные агрегации: Для агрегаций можно использовать двухуровневую агрегацию. Сначала каждый шард выполняет агрегацию, затем результаты отправляются на инициализирующий сервер для финальной агрегации. Это уменьшает объем передаваемых данных. Включить можно настройками:
```sql
SET distributed_aggregation_memory_efficient = 1;
SET group_by_two_level_threshold = 1000;
SET group_by_two_level_threshold_bytes = 100000000;
```
- Распределенные подзапросы: Избегать распределенных подзапросов, которые могут привести к передаче больших объемов данных. Вместо этого использовать `GLOBAL IN` или `GLOBAL JOIN`.
- Балансировка и топология: Правильно настраивать топологию кластера. Использовать веса реплик, если шарды неоднородны. Убедиться, что сетевые задержки минимальны. Пример создания распределенной таблицы:
```sql
CREATE TABLE distributed_table ON CLUSTER 'my_cluster' AS local_table
ENGINE = Distributed('my_cluster', 'default', 'local_table', rand());
```
Запросы к `distributed_table` будут распределяться по шардам.
- Распределенные DDL: Использовать `ON CLUSTER` для управления схемой на всех шардах.

<h4>Оптимизация запросов с использованием шардирования</h4>

Работа с большими данными требует особого подхода к проектированию таблиц, запросов и кластера. Стратегии оптимизации:
- Горизонтальное масштабирование (шардирование): Распределить данные по нескольким серверам. Это позволяет обрабатывать больше данных, чем помещается на один сервер.
- Вертикальное масштабирование: Увеличить ресурсы сервера (память, CPU, быстрые диски). ClickHouse эффективно использует многоядерные процессоры и оперативную память.
- Сжатие данных: ClickHouse сжимает данные по умолчанию. Выбор кодека (например, ZSTD) может значительно уменьшить объем хранилища и ускорить чтение.
- Архивация и TTL: Использовать TTL для автоматического удаления или перемещения старых данных на более медленные диски (или в S3). Это помогает управлять объемом активных данных. Пример:
```sql
CREATE TABLE logs (
  event_date DateTime,
  message String
) ENGINE = MergeTree
PARTITION BY toYYYYMM(event_date)
ORDER BY event_date
TTL event_date + INTERVAL 1 MONTH TO DISK 'slow_disk',
     event_date + INTERVAL 1 YEAR DELETE;
```
- Использование движка `ReplicatedMergeTree` для отказоустойчивости: Репликация не увеличивает производительность запросов, но обеспечивает доступность и целостность данных.
- Оптимизация памяти: Настроить параметры памяти для обработки больших запросов (`max_memory_usage`, `max_bytes_before_external_group_by`, `max_bytes_before_external_sort`). При нехватке памяти ClickHouse может сбрасывать промежуточные данные на диск.
- Проектирование таблиц: Правильный первичный ключ (для пропуска данных), партицирование для управления жизненным циклом, использование разных движков для разных задач (например, `MergeTree` для детальных данных, `AggregatingMergeTree` для агрегатов).
- Batch processing: Вставлять данные крупными пачками (от 10 000 до 1 000 000 строк за раз) и尽量避免 частые мелкие вставки.
- Мониторинг и профилирование: Использовать системные таблицы (`system.query_log`, `system.part_log`, `system.metrics`) для анализа производительности. Включить профилировку запросов (`SET send_profiler_end = 1`).
- Использование приближённых вычислений: Для больших данных точность может быть не так важна. Можно использовать приближённые функции (`uniqCombined`, `approxQuantile`).
- Векторизованное выполнение запросов: ClickHouse использует векторную обработку данных. Убедиться, что данные хранятся в колоночном формате и кодек сжатия не мешает векторной обработке.
- Работа с дисками: Использовать быстрые SSD. Разделить данные и журналы на разные диски. Настройка `storage_policy` позволяет использовать несколько дисков с разными скоростями. Пример настройки политики хранения:
```xml
<storage_configuration>
    <disks>
        <fast_disk>
            <path>/var/lib/clickhouse/fast/</path>
        </fast_disk>
        <slow_disk>
            <path>/var/lib/clickhouse/slow/</path>
        </slow_disk>
    </disks>
    <policies>
        <moving_from_fast_to_slow>
            <volumes>
                <fast>
                    <disk>fast_disk</disk>
                </fast>
                <slow>
                    <disk>slow_disk</disk>
                </slow>
            </volumes>
        </moving_from_fast_to_slow>
    </policies>
</storage_configuration>
```
Затем в таблице:
```sql
CREATE TABLE logs (...) ENGINE = MergeTree
PARTITION BY ...
ORDER BY ...
TTL ... TO VOLUME 'slow'
SETTINGS storage_policy = 'moving_from_fast_to_slow';
```

<h4>Оптимизация дисковых операций</h4>

Дисковые операции часто являются узким местом в производительности. Вот ключевые техники их оптимизации:
- Использование быстрых накопителей: NVMe SSD предпочтительнее SATA SSD и HDD. ClickHouse активно использует дисковый ввод-вывод, поэтому скорость диска напрямую влияет на производительность.
- Разделение данных на разные диски (Storage Policies): Можно настроить политики хранения, чтобы, например, горячие данные находились на быстрых NVMe, а холодные — на более медленных SATA или даже в объектном хранилище (S3). Также можно распределять данные по нескольким дискам (JBOD) для увеличения пропускной способности.
- Оптимизация слияний (Merge): Слияния (merge) — это фоновая операция объединения данных, которая может создавать высокую нагрузку на диск. Нужно настраивать параметры слияний (см. ниже) в соответствии с вашим оборудованием и нагрузкой.
- Использование TTL для управления жизненным циклом данных: Автоматическое удаление или перемещение старых данных на более медленные диски (или в S3) с помощью TTL (Time To Live) помогает поддерживать объем горячих данных на оптимальном уровне.
- Настройка методов ввода-вывода:
  - `local_filesystem_read_method` и `local_filesystem_write_method` (например, pread, read, mmap). Для NVMe SSD pread или read могут быть лучше, чем mmap.
  - `min_bytes_to_use_direct_io` — использование прямого ввода-вывода (O_DIRECT) позволяет обойти кэш ОС, что может быть полезно при наличии собственного кэша в ClickHouse (uncompressed cache) и быстрых дисках.
- Кэширование: Настройка кэша разжатых данных (`uncompressed_cache_size`) и кэша засечек (`mark_cache_size`) уменьшает количество обращений к диску.
- Оптимизация структуры таблиц: Правильный выбор порядка столбцов в первичном ключе и партиционирования может значительно уменьшить объем читаемых с диска данных.
- Использование эффективных кодеков сжатия (например, Delta, DoubleDelta, Gorilla для временных рядов, ZSTD для общего случая) уменьшает объем данных на диске, что снижает IO.

<h4>Оптимизация дисковых операций</h4>

TTL (Time To Live) — мощный инструмент управления жизненным циклом данных. Виды TTL:
- TTL для строк: Удаление строк по истечении времени:
```sql
CREATE TABLE example_table
(
    date Date,
    value Int32
)
ENGINE = MergeTree
ORDER BY date
TTL date + INTERVAL 1 MONTH;
```
- TTL для столбцов: Установка значений по умолчанию для столбцов через определенное время (например, обнуление конфиденциальных данных):
```sql
CREATE TABLE example_table
(
    date Date,
    secret String TTL date + INTERVAL 1 DAY
)
ENGINE = MergeTree
ORDER BY date;
```
- TTL для перемещения данных между дисками/томами: Например, перемещение старых данных на более медленный диск или в S3:
```sql
CREATE TABLE example_table
(
    date Date,
    value Int32
)
ENGINE = MergeTree
ORDER BY date
TTL date + INTERVAL 1 WEEK TO DISK 'slow_disk',
    date + INTERVAL 1 MONTH TO VOLUME 'cold_volume';
```

Стратегии оптимизации хранения данных:
- Партиционирование: Разбиение таблицы на части по ключу (например, по месяцу). Упрощает управление данными (удаление, перемещение) и может ускорить запросы, если они затрагивают только определенные партиции. Не стоит делать слишком мелкие партиции (например, по дням), если данных не много. Это может привести к большому количеству файлов и снижению производительности.
- Использование различных движков таблиц:
  - `ReplicatedMergeTree` для отказоустойчивости и распределения нагрузки на чтение.
  - `Distributed` для шардирования данных по кластеру.
  - `MergeTree` для локальных таблиц.
- Архивация старых данных: С помощью TTL перемещать старые данные в объектное хранилище (S3) или на холодные диски. ClickHouse поддерживает хранение данных в S3 и может запрашивать их оттуда при необходимости (хотя и с большей задержкой).
- Использование материализованных представлений: Для предварительного агрегирования данных и ускорения часто используемых запросов.
- Управление индексными гранулами: Параметр `index_granularity` (по умолчанию 8192) определяет, сколько строк в одной грануле индекса. Уменьшение этого значения может улучшить точность индекса (меньше лишних данных читается), но увеличит размер индекса и может замедлить запросы из-за большего количества гранул.

<h4>Настройка аутентификации и авторизации</h4>

ClickHouse поддерживает два основных способа управления доступом: устаревший XML-конфиги и современный SQL-управляемый RBAC (ролевая модель). Рекомендуется использовать RBAC.

Аутентификация: Определяется в `users.xml` или (лучше) в отдельных файлах в `users.d/.`

Основные методы:
- Пароль: Может быть храниться в plaintext, sha256, sha256 в hex, double_sha1 (для совместимости с MySQL).
- LDAP: Интеграция с внешним LDAP-сервером.
- Kerberos.

Пример создания пользователя через SQL (RBAC):
```sql
CREATE USER alice IDENTIFIED WITH sha256_password BY 'strong_password';
CREATE USER bob IDENTIFIED WITH ldap SERVER 'my_ldap_server';
```

Авторизация — RBAC модель
Роли: Группируют привилегии.
```sql
CREATE ROLE analyst;
GRANT SELECT ON database1.* TO analyst;
GRANT SELECT ON database2.table2 TO analyst;
```

Привязка ролей к пользователям:
```sql
GRANT analyst TO alice;
```

Привилегии: Можно выдавать напрямую.
```sql
GRANT INSERT ON db.table TO bob;
```

Профили настроек: Ограничивают ресурсы (макс. память, таймауты).
```sql
CREATE PROFILE readonly_profile SETTINGS max_memory_usage = 10000000000, read_only = 1;
CREATE USER viewer IDENTIFIED BY 'view' PROFILE readonly_profile;
```

<h3>2. Масштабирование и мониторинг</h3>
<h3>Масштабирование</h3>

Вертикальное масштабирование:
- Колоночное хранение: Данные хранятся не по строкам, а по столбцам. Это позволяет читать только те столбцы, которые нужны для запроса (меньший объем I/O), эффективно сжимать данные (поскольку значения в одном столбце часто похожи), что уменьшает объем хранилища в десятки раз.
- Векторизованная обработка запросов: Данные обрабатываются не по одной строке, а большими блоками (векторами, часто по 8192 строки). Это позволяет максимально эффективно использовать возможности CPU (кэш, инструкции SIMD).
- Распределение нагрузки на железо:
  - Множественные ядра CPU: ClickHouse активно использует все доступные ядра для параллельного выполнения частей одного запроса (параллелизм на уровне данных).
  - Большая пропускная способность дисков (IOPS, throughput): Оптимизирован под быстрые последовательные чтения (HDD/SSD, а лучше NVMe). Рекомендуется использовать RAID или несколько отдельных дисков для данных и журналов.
  - Большой объем RAM: Используется для хранения индексов, промежуточных данных, состояния агрегатных функций, кэширования.
- Отсутствие транзакционности (в классическом OLAP-смысле): Отказ от ACID для отдельных вставок/обновлений позволяет достичь предельной скорости записи и чтения.
- Эффективные алгоритмы и структуры данных:
  - MergeTree-движки: Фундамент ClickHouse. Данные записываются в мелкие куски (parts), которые затем фоново сливаются в более крупные, упорядоченные. Это обеспечивает быструю вставку (просто добавление нового куска на диск), предварительную сортировку по первичному ключу, что ускоряет диапазонные выборки (WHERE, GROUP BY).
  - Пропускающие индексы (Data Skipping Indexes): Статистические индексы (minmax, set, bloom_filter и др.), которые позволяют "пропускать" целые блоки данных при чтении, если известно, что нужных данных там нет.
  - Параллелизм и конкуренция: Поддерживает большое количество одновременных SELECT-запросов, эффективно распределяя ресурсы между ними.

Горизонтальное масштабирование:
- Шардирование (Sharding) — для масштабирования объема/производительности: Разделение данных между разными серверами (шардами) по какому-либо ключу (например, `user_id`). При вставке данных распределительный движок (например, Distributed) решает, на какой шард отправить каждую порцию данных, основываясь на ключе шардирования. Запрос, отправленный на любую ноду через Distributed-таблицу, выполняется параллельно на всех шардах, результаты агрегируются на ноде-инициаторе. Цель - Увеличить общую пропускную способность записи и чтения, а также общий объем данных сверх возможностей одного сервера.
- Репликация (Replication) — для отказоустойчивости и балансировки чтения: Создание полных копий данных (реплик) на нескольких серверах. Используются семейства движков `ReplicatedMergeTree`. При вставке в одну реплику данные через ZooKeeper синхронизируются со всеми другими репликами этого шарда. Репликация асинхронная на уровне кусков данных. Цель - отказоустойчивость и Балансировка чтения

<h4>Мониторинг метрик производительности</h4>

ClickHouse предоставляет несколько способов мониторинга:
- Системные таблицы (`system.metrics`, `system.events`, `system.asynchronous_metrics`, `system.query_log`, `system.part_log` и др.)
- HTTP-интерфейс для Prometheus (метрики экспортируются в формате Prometheus)
- Собственный профилировщик (`query profiler`) и трассировка (`trace_log`)

Основные метрики, которые важно отслеживать:
- Запросы: Количество запросов в секунду, медленные запросы (длительность запросов, можно настраивать пороги), ошибки запросов (количество исключений)
- Использование ресурсов: Загрузка CPU (системная и пользовательская)
- Использование памяти:
  - RAM: общее, свободное, используемое ClickHouse (включая разбивку по типам памяти: например, для кэшей, для запросов и т.д.)
  - Память процессов: RSS, виртуальная память
- Дисковое использование:
  - Свободное место на дисках (особенно важно, если данные растут быстро)
  - Скорость чтения/записи (IOPS, пропускная способность)
  - Использование inode
- Метрики ClickHouse (из `system.metrics` и `system.events`):
  - `Query` - количество запросов (разные типы)
  - `Merge` - активные слияния (мержи) в MergeTree
  - ReplicatedFetch - для репликации
  - TCPConnection - количество соединений
  - HTTPConnection - количество HTTP соединений
  - DelayedInserts - количество отложенных вставок (если есть проблемы с производительностью при вставке)
- Логи запросов (`system.query_log`): Позволяет анализировать историю запросов: кто, когда, какой запрос, как долго выполнялся, сколько строк прочитал, сколько памяти использовал.
- Логи партиций (`system.part_log`): Информация о создании, удалении, слиянии кусков данных в MergeTree.

Мониторинг внешних источников: ClickHouse может работать с внешними источниками данных (внешние словари, табличные функции, интеграции с Kafka, MySQL, PostgreSQL и др.). Мониторинг этих компонентов включает:
- Внешние словари (`system.dictionaries`):
  - Статус загрузки словарей (загружены ли, время последней загрузки, количество строк, объем памяти)
  - Ошибки загрузки словарей
- Интеграция с Kafka (движок Kafka):
  - Отставание потребителей (lag) по партициям
  - Количество сообщений, обработанных за период
  - Ошибки потребления
- Интеграция с MySQL/PostgreSQL (табличные функции, движок MySQL, движок PostgreSQL):
  - Задержка подключения
  - Количество переподключений
  - Задержка репликации (если используется)
- HDFS/S3 (табличные функции, движок S3):
  - Скорость чтения/записи
  - Количество ошибок при работе с объектами
- Также важно мониторить сетевые соединения между ClickHouse и этими внешними системами (задержки, разрывы).

<h4>Настройка мониторинга с помощью Graphana и Prometheus</h4>

Этапы настройки:
1. Настройка экспорта метрик из ClickHouse в Prometheus: ClickHouse имеет встроенную поддержку Prometheus. В конфигурационном файле `config.xml` (или в `prometheus.xml` внутри папки `config.d`) нужно добавить:
 ```xml
 <prometheus>
     <endpoint>/metrics</endpoint>
     <port>9363</port>
     <metrics>true</metrics>
     <events>true</events>
     <asynchronous_metrics>true</asynchronous_metrics>
     <status_info>true</status_info>
 </prometheus>
 ```
 После перезагрузки конфигурации или перезапуска ClickHouse метрики будут доступны по адресу `http://<clickhouse-server>:9363/metrics`.

2. Настройка Prometheus для сбора метрик: В конфигурации Prometheus (`prometheus.yml`) нужно добавить job:
```yaml
scrape_configs:
job_name: 'clickhouse'
static_configs:
targets: ['<clickhouse-server>:9363']
scrape_interval: 5s
```
3. Установка и настройка Grafana: Установить Grafana и добавьте источник данных (data source) типа Prometheus, указав URL до Prometheus.

4. Импорт дашбордов:В Grafana есть несколько готовых дашбордов для ClickHouse:
  - Официальный дашборд от ClickHouse: https://github.com/ClickHouse/clickhouse-presentations/tree/master/grafana
  - Дашборд от Percona: https://grafana.com/grafana/dashboards/882

Или можно создать свой дашборд, используя следующие ключевые метрики:
- Общее состояние кластера (количество нод, их версии, uptime)
- Запросы в секунду (разбивка по типу)
- Задержка запросов (средняя, 95-й и 99-й перцентили)
- Использование памяти (разбивка по типам: резидентная, для запросов, для кэшей)
- Активность слияний (мержей) и мутаций
- Количество реплик и их отставание (для реплицируемых таблиц)
- Использование дискового пространства
- Сетевой трафик (ввод/вывод)
- Количество открытых файлов

Пример запроса в Grafana для количества SELECT-запросов в секунду:
```
rate(ClickHouseProfileEvents_Query[5m])
```
или, если используется метрика из `system.events`:
```
rate(ClickHouseMetrics_Query[5m])
```
Для мониторинга репликации можно использовать метрики из system.replicas, например, отставание реплики:
```
ClickHouseMetrics_ReplicasMaxAbsoluteDelay
```

5. Настройка алертинга
В Prometheus или Grafana можно настроить алерты на критические состояния:
- Отсутствие метрик от ClickHouse (нода недоступна)
- Слишком много ошибок запросов
- Высокая загрузка CPU или памяти
- Заканчивается место на диске
- Отставание репликации выше порога
- Медленные запросы (перцентиль 95 выше порога)