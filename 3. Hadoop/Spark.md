<h2>Apache Spark</h2>
<h3>1. Введение</h3>
<h4>Определение и основные компоненты</h4>

Apache Spark — это универсальная вычислительная система для обработки больших данных, предоставляющая комплексные API на Java, Scala, Python и R, а также поддерживающая разнообразные задачи обработки данных: пакетную обработку, потоковую передачу данных, обработку графов, и машинное обучение. Spark разработан для работы с большими объемами данных в распределенной среде, предлагая более быструю и гибкую альтернативу MapReduce Hadoop. Spark позволяет проводить обработку данных в памяти, что значительно увеличивает скорость выполнения приложений по сравнению с MapReduce. Поддержка чтения и записи данных в HDFS, а также совместимость с Hadoop YARN для управления ресурсами кластера, обеспечивает легкую интеграцию Spark в существующую инфраструктуру Hadoop.

Компоненты экосистемы:
- Spark Core: Основная часть фреймворка, предоставляющая основные функции распределенной обработки данных, в том числе управление памятью, отказоустойчивость, распределение задач и базовый API.
- Spark SQL: Модуль для работы с структурированными данными. Позволяет выполнять SQL-запросы, используя традиционный синтаксис SQL, а также HiveQL для интеграции с Apache Hive.
- Spark Streaming: Предназначен для обработки потоковых данных в реальном времени. Данные могут поступать из различных источников, таких как Kafka, Flume или Kinesis.
- MLlib (Machine Learning Library): Библиотека машинного обучения, предоставляющая множество алгоритмов машинного обучения и утилит для обработки данных.
- GraphX: Библиотека для обработки графов и выполнения графовых вычислений, предоставляющая API для создания и трансформации неизменяемых графов, а также выполнения параллельных операций над ними.

Компоненты архитектуры:
- Driver Program: Программа, которая создает SparkContext, инициирующий Spark-приложение. Это центральный узел управления, который конвертирует пользовательскую программу в задачи и распределяет их между исполнителями (Executors). Драйвер также отвечает за планирование задач и восстановление от сбоев.
- SparkContext: Контекст выполнения приложения, управляющий доступом к кластеру через Cluster Manager.
- Cluster Manager: Spark может работать на различных менеджерах кластеров, включая Spark Standalone, YARN (Hadoop), Mesos и Kubernetes. Менеджер кластера отвечает за выделение ресурсов драйверу и исполнителям.
- Executors: Процессы, выполняющиеся на узлах рабочего кластера, которые обрабатывают задачи вычислений и хранят данные приложения в памяти или на диске. Executors взаимодействуют с программой "Driver Program" для выполнения кода.
- RDD (Resilient Distributed Dataset): Это основная абстракция данных в Spark, представляющая собой неизменяемую коллекцию элементов, распределенных по кластеру, которую можно обрабатывать в параллельном режиме. Именно RDD является основным вычислительным примитивом Spark, над которым можно делать параллельные вычисления и преобразования с помощью встроенных и произвольных функций, в том числе с помощью временных окон.

Рабочий процесс Spark:
1. Инициализация: Пользовательская программа, написанная на одном из поддерживаемых Spark языков (Scala, Java, Python, R), запускается на драйвере. SparkSession создается как точка входа в функциональность Spark.
2. Логическое планирование: Драйвер преобразует программу в логическое план выполнения, определяя операции на RDD или DataFrame.
3. Физическое планирование: Логический план преобразуется в физический план выполнения, определяющий, какие операции будут выполняться на каждом исполнителе.
4. Выполнение: Драйвер отправляет задачи исполнителям через менеджер кластера. Исполнители выполняют задачи и возвращают результаты драйверу, если это необходимо.
5. Восстановление от сбоев: Spark обеспечивает отказоустойчивость через механизм линеаризации RDD. В случае потери данных или сбоя задачи могут быть повторно выполнены.

<h4>Основные типы данных и операции</h4>

Основные типы данных (Абстракции):
- RDD (Resilient Distributed Dataset): Низкоуровневая абстракция, основа всего Spark, неизменяемая и отказоустойчивая. Может содержать объекты любого типа (Python, Java, Scala). Создается из коллекций в памяти или из внешних источников (HDFS, S3, локальная ФС).
- DataFrame: Высокоуровневая абстракция, построенная на RDD. Представляет данные в виде таблицы со схемой (имена и типы столбцов). Оптимизируется с помощью Catalyst Optimizer и Tungsten (высокопроизводительный движок сериализации и исполнения), что делает операции быстрее, чем на чистом RDD.
- Dataset (только для Scala и Java): Комбинация преимуществ RDD (статическая типизация, лямбда-функции) и DataFrame (оптимизация Catalyst).

Операции в Spark делятся на два типа:
- Трансформации (Transformations): Это "ленивые" (lazy) операции. Они не выполняются немедленно, а лишь создают план вычислений. Выполнение происходит только при вызове действия.
  - Узкие трансформации (Narrow): Каждая партиция входного RDD используется не более чем в одной партиции выходного RDD (не требуют перемешивания данных между узлами):
    - `map(func)`: Применяет функцию к каждому элементу.
    - `filter(func)`: Возвращает элементы, для которых функция возвращает true.
    - `flatMap(func)`: Как map, но возвращает плоскую структуру (например, разбивает строку на слова).
    - `union(otherRDD)`: Объединяет два RDD.
  - Широкие трансформации (Wide): Требуют перемешивания данных (shuffle) между узлами кластера, так как входные данные для одной партиции выходного RDD могут приходить из многих партиций входного RDD:
    - `groupByKey()`: Группирует значения по ключу (только для парных RDD).
    - `reduceByKey(func)`: Агрегирует значения по ключу с помощью функции.
    - `join(otherRDD)`: Объединяет два RDD по ключу.
    - `distinct()`: Возвращает уникальные элементы.
- Действия (Actions): Запускают выполнение всего плана трансформаций для получения результата:
  - `collect()`: Возвращает все элементы набора данных в виде массива на драйвере.
  - `count()`: Возвращает общее количество элементов.
  - `first()`: Возвращает первый элемент (аналогично take(1)).
  - `take(n)`: Возвращает первые n элементов.
  - `saveAsTextFile(path)`: Сохраняет RDD в текстовый файл.
  - `foreach(func)`: Применяет функцию к каждому элементу (часто используется для side-эффектов, например, записи в базу данных).

<h4>Основы RDD и операции</h4>

RDD (Resilient Distributed Dataset) — это основополагающая структура данных в Apache Spark, представляющая собой неизменяемую, распределённую коллекцию объектов, которую можно обрабатывать параллельно. RDD предоставляют мощную абстракцию для работы с данными, что позволяет Spark обеспечивать высокую производительность при обработке больших объёмов данных. Вот детальный обзор RDD, включая их ключевые характеристики, методы создания, операции и преимущества.

Ключевые характеристики RDD:
- Неизменяемость и разбиение: RDD неизменяемы, то есть однажды созданные, они не могут быть изменены. Это упрощает программирование и способствует достижению консистентности в вычислениях. RDD распределены по узлам кластера, что обеспечивает параллельную обработку.
- Устойчивость к ошибкам: RDD разработаны таким образом, чтобы быть устойчивыми к сбоям за счёт информации о происхождении. В случае потери части RDD из-за сбоя узла, Spark может восстановить её, используя историю операций, которые изначально создали эту часть.
- Постоянство: Пользователи могут сохранять или кэшировать RDD в памяти или на диске узлов кластера. Это особенно полезно для итеративных алгоритмов, которым требуется многократный доступ к одному и тому же набору данных.
- Ленивые вычисления: Операции с RDD являются ленивыми, то есть они не выполняются немедленно. Spark начинает вычисления RDD только когда вызывается действие, требующее возврата результата в программу-драйвер.
- Прозрачность размещения: RDD предоставляют абстрактный интерфейс, скрывая сложности распределения данных и позволяя вычислительным процессам получать непосредственный доступ к данным.

Преимущества использования RDD:
- Гибкость: RDD позволяют обрабатывать как пакетные, так и потоковые данные.
- Эффективность: Кэширование или сохранение RDD в памяти между операциями значительно ускоряет обработку данных.
- Устойчивость к отказам: Благодаря информации об истории RDD можно восстанавливать после сбоев, что обеспечивает надежность.

Создать RDD в Spark можно двумя основными способами:
- Параллелизация существующей коллекции: Существующие коллекции в программе-драйвере могут быть преобразованы в RDD с использованием метода `parallelize` объекта `SparkContext`.
```python
data = [1, 2, 3, 4, 5]
rdd = sparkContext.parallelize(data)
```
- Ссылка на датасет во внешней системе хранения: RDD можно создать, сославшись на датасеты во внешних системах хранения, таких как HDFS, HBase, общие файловые системы, используя методы вроде `textFile` или `wholeTextFiles` объекта `SparkContext`.
```python
rdd = sparkContext.textFile("hdfs://namenode:8020/path/to/file.txt")
```

Над RDD поддерживаются два типа операций:
- Трансформации: Операции, которые возвращают новый RDD из существующего, например, `map`, `filter`, `flatMap`, `union` и `reduceByKey`. Трансформации являются отложенным типом операции и выполняются только тогда, когда запрашивается действие.
- Свертки: Операции, которые возвращают результат в программу-драйвер или записывают его во внешнее хранилище. Примеры включают `collect`, `count`, `reduce`, `take`, и `saveAsTextFile`.

Пример использования RDD в PySpark для анализа текста:
```python
from pyspark import SparkContext
sc = SparkContext()

# Чтение текстового файла в RDD
textFile = sc.textFile("hdfs://namenode:8020/path/to/text.txt")

# Разделение текста на слова и подсчет каждого уникального слова
wordCounts = textFile.flatMap(lambda line: line.split(" ")) \
             .map(lambda word: (word, 1)) \
             .reduceByKey(lambda a, b: a + b)

# Вывод результатов
wordCounts.collect()
```

<h4>Создание приложений</h4>

Программирование приложений Spark обычно включает создание контекста (SparkContext для RDD или SparkSession для DataFrame) и определение операций.

Шаги для запуска приложения Spark:
1. Написание кода (на Scala, Java, Python или R).
2. Сборка (для Scala/Java с помощью Maven или SBT) и создание JAR-файла.
3. Запуск приложения с помощью `spark-submit`.

Структура приложения:
```python
from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession

# 1. Конфигурация приложения
conf = SparkConf().setAppName("MySparkApp") \
                  .setMaster("local[4]") \
                  .set("spark.executor.memory", "2g")

# 2. Создание SparkContext и/или SparkSession
sc = SparkContext(conf=conf)
spark = SparkSession.builder.config(conf=conf).getOrCreate()

try:
    # 3. Бизнес-логика приложения
    data = sc.parallelize(range(1000))
    result = data.filter(lambda x: x % 2 == 0) \
                 .map(lambda x: x * x) \
                 .reduce(lambda a, b: a + b)
    print(f"Результат: {result}")
finally:
    # 4. Завершение работы
    sc.stop()
```

<h4>Запуск приложений</h4>

Специальная консольная утилита `spark-submit` позволяет управлять выполнением Spark-приложений на кластерах и в локальной среде. Она поддерживает широкий спектр опций, позволяющих настроить выполнение приложения, включая выбор кластера, выделение ресурсов, настройку конфигураций и многое другое.

Основные параметры команды `spark-submit`:
- `--class`: Имя главного класса приложения (для Java и Scala приложений).
- `--master`: URL мастера Spark-кластера (например, `spark://master:7077`, `yarn`, `local[4]`).
- `--deploy-mode`: Режим развертывания приложения (`client` или `cluster`).
- `--conf`: Передача дополнительных конфигурационных параметров и их значений.
- `--packages`: Список Maven координат зависимостей, которые требуются приложению.
- `--py-files`: Список файлов Python, необходимых для выполнения приложения (например, дополнительные модули, зависимости).

Для запуска Python-скрипта `my_spark_app.py`, который принимает в качестве входного параметра `input` файлы, хранящиеся в HDFS, и сохраняет результат анализа обратно в HDFS, с использованием `spark-submit` необходимо выполнить следующую команду:
```bash
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --executor-memory 2G \
  --num-executors 3 \
  my_spark_app.py \
  --input hdfs:///path/to/input/data \
  --output hdfs:///path/to/save/results
```

В этом примере:
- `--master yarn` указывает, что приложение будет запущено на YARN.
- `--deploy-mode cluster` означает, что драйвер программы будет запущен на узле кластера, а не на локальной машине.
- `--executor-memory 2G` выделяет 2 GB оперативной памяти для каждого исполнителя.
- `--num-executors 3` запрашивает три исполнителя для обработки данных.
- `my_spark_app.py` – путь к вашему Python-скрипту.
- `--input` и `--output` – пользовательские аргументы, определенные в вашем Python-скрипте, указывающие путь к данным в HDFS и местоположение для сохранения результатов.

Опция `--deploy-mode` в команде `spark-submit` указывает Spark, где должен быть запущен драйвер приложения во время выполнения. Драйвер приложения является процессом, который создает контекст Spark, подает задания на выполнение и обрабатывает результаты. В зависимости от выбранного режима развертывания, драйвер может быть запущен на локальной машине, откуда выполняется spark-submit, или на одном из узлов кластера. В Spark существует два основных режима развертывания (`--deploy-mode`):
- Client Mode (Клиентский режим): В клиентском режиме (`--deploy-mode client`) драйвер запускается на той же машине, с которой была выполнена команда `spark-submit`. Этот режим позволяет легко отслеживать процесс выполнения приложения и является удобным для интерактивной разработки и тестирования. Однако, поскольку драйвер запущен на локальной машине, он может быть менее устойчивым к сбоям сети и имеет дополнительные требования к производительности локальной машины, особенно если задача обработки данных требует значительных ресурсов.
- Cluster Mode (Кластерный режим): В кластерном режиме (`--deploy-mode cluster`) драйвер запускается на одном из узлов кластера, который управляется менеджером кластера (например, YARN, Mesos или Kubernetes). Этот режим предоставляет более высокую устойчивость к сбоям, поскольку драйвер работает в той же среде, что и исполнители задач, и может быть лучше интегрирован с системами мониторинга и управления кластером. Кластерный режим предпочтителен для продакшен-задач и задач, требующих высокой доступности.

<h4>Кэширование и управление памятью</h4>

Кэширование (persistence) - это важная оптимизация в Spark, которая позволяет хранить RDD в памяти (или на диске) для повторного использования в последующих действиях. Методы кэширования:
- `persist(storageLevel)`: сохраняет RDD с указанным уровнем хранения.
- `cache()`: то же, что и `persist()` с уровнем `MEMORY_ONLY`.

Уровни хранения (StorageLevel):
- `MEMORY_ONLY`: хранит RDD в виде десериализованных объектов в JVM. Если не хватает памяти, некоторые партиции не будут кэшированы и будут пересчитываться по мере необходимости.
- `MEMORY_AND_DISK`: хранит в памяти, но при нехватке памяти сбрасывает партиции на диск.
- `MEMORY_ONLY_SER`: хранит сериализованные объекты (более эффективно по памяти, но требует затрат на сериализацию).
- `MEMORY_AND_DISK_SER`: аналогично, но с загрузкой на диск.
- `DISK_ONLY`: хранит партиции только на диске.

Память исполнителя (executor) делится на несколько частей:
- Execution Memory: используется для `shuffle`, `join`, `sort` и агрегаций.
- Storage Memory: используется для кэширования RDD и хранения broadcast переменных.
- User Memory: память, выделенная для данных пользователя (например, структуры данных, созданные в пользовательских функциях).
- Reserved Memory: память, зарезервированная системой.

Настройки памяти можно задавать через параметры:
- `--executor-memory`: общая память исполнителя.
- `spark.memory.fraction`: доля памяти, используемая для execution и storage (по умолчанию 0.6).
- `spark.memory.storageFraction`: доля памяти, зарезервированная для storage (по умолчанию 0.5).

Выделение памяти:
```python
conf = SparkConf()
# Память исполнителя
conf.set("spark.executor.memory", "4g")
conf.set("spark.executor.memoryOverhead", "1g")  # Дополнительная память

# Память драйвера
conf.set("spark.driver.memory", "2g")

# Настройки кэширования
conf.set("spark.memory.fraction", "0.6")         # Доля памяти для execution/storage
conf.set("spark.memory.storageFraction", "0.5")  # Доля storage от общей памяти

# Сериализация (уменьшает использование памяти)
conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
```

<h4>Spark Streaming</h4>

Spark Streaming — это модуль для обработки потоковых данных в реальном времени. Основная концепция: DStream (Discretized Stream). Он разбивает непрерывный поток данных на небольшие пакеты (микропакеты), называемые микробатчами. Каждый микробатч обрабатывается как обычный RDD (или DataFrame) с помощью стандартного Spark API.

Архитектура:
1. Источник данных (Kafka, Kinesis, TCP-сокет и т.д.) отправляет данные.
2. Spark Streaming получает данные и накапливает их в течение интервала пакетирования (например, 1 секунда).
3. Собранный за этот интервал микробатч передается ядру Spark для обработки.
4. Результаты каждого батча выводятся во внешние системы (файловую систему, БД, дашборды).

Ключевые возможности:
- Единая парадигма программирования: Один и тот же код (на RDD, DataFrame) можно использовать для пакетной и потоковой обработки.
- Отказоустойчивость: Наследуется от RDD. Данные реплицируются и восстанавливаются при сбоях.
- Exactly-Once Semantics: Гарантия того, что каждая запись будет обработана ровно один раз, даже в случае сбоев (при использовании совместимых источников и приемников).
- Интеграция с экосистемой Spark: Легко комбинировать потоковые данные с статическими (например, соединить поток кликов со справочником пользователей).
- Оконные агрегаты: Позволяют выполнять вычисления над скользящими окнами данных (например, "количество пользователей за последние 5 минут, обновляемое каждую минуту").

Начиная с Spark 2.0, появился Structured Streaming — более новая и мощная потоковая модель, построенная на основе SparkSQL и DataFrame/Dataset API. Основная идея: Поток данных рассматривается как бесконечно растущая таблица. Новые данные — это новые строки, добавляемые в таблицу.

Преимущества:
- Автоматическая оптимизация запросов через Catalyst.
- Семантика end-to-end exactly-once.
- Более простой и декларативный API.
- Поддержка событийного времени (event-time) и отложенных данных (watermarks).

<h4>Настройка параллелизма и партицирование</h4>

Производительность Spark напрямую зависит от эффективного параллелизма, который определяется партиционированием данных.

Параллелизм:
- Драйвер: Координатор, который создает SparkContext и преобразует пользовательский код в задания (Jobs).
- Исполнитель (Executor): Рабочий процесс на узле кластера, который выполняет задачи (Tasks) и хранит данные.
- Задание (Job): Набор этапов (Stages), вызванный действием (например, count(), saveAsTextFile()).
- Этап (Stage): Набор задач, которые могут быть выполнены вместе (например, до шафафла).
- Задача (Task): Единица работы, отправляемая на один исполнитель для обработки одной партиции данных.

Ключевой принцип: Количество задач на этапе = количеству партиций. Одна задача обрабатывает одну партицию на одном ядре исполнителя.

Начальное партиционирование:
- При чтении из HDFS: обычно одна партиция на один блок файла (например, 128 МБ).
- При создании из коллекции: можно задать количество партиций через `parallelize(data, numSlices)`.

Ключевые операции, меняющие партиционирование (Шаффл): `groupByKey()`, `reduceByKey()`, `join()`, `repartition()`. Эти операции требуют перемешивания данных между исполнителями, что очень дорого.

Настройка количества партиций: `spark.sql.shuffle.partitions` (по умолчанию 200). Определяет, на сколько партиций будут разбиты данные после шаффла. Если у вас мало данных, 200 партиций создадут много мелких задач и накладных расходов. Если данных очень много, 200 партиций могут быть слишком большими и привести к нехватке памяти.

Стратегии управления партиционированием:
- `repartition` - увеличение партиций. Увеличивает количество партиций до определенного значения, выполняя полный шаффл. Полезно перед записью большого объема данных или если партиции слишком велики.
- `coalesce` - уменьшение партиций. Уменьшает количество партиций до определенного значения, избегая полного шаффла. Данные объединяются внутри каждого исполнителя, где это возможно. Намного эффективнее, чем `repartition` для уменьшения.
- `partitionBy` - партиционирование по ключу. Создаст отдельные папки для каждой партиции, что значительно ускорит последующие запросы с фильтром по данному ключу.

Практические советы по настройке:
- Начните с мониторинга UI Spark: Смотрите на количество задач в каждом этапе и их размер. Идеальный размер задачи — 100-200 МБ.
- Настройте `spark.sql.shuffle.partitions`: Ориентируйтесь на общий объем данных после шаффла. Например, если у вас 50 ГБ данных после groupBy, то при целевом размере партиции в 128 МБ нужно установить 50 * 1024 / 128 ≈ 400 партиций.
- Используйте `coalesce` для уменьшения партиций перед записью небольшого результата, чтобы не создавать тысячи маленьких файлов.
- Используйте `repartition` по нужному столбцу перед сложными операциями `join`, если известно, что данные будут склеиваться по этому столбцу. Это может помочь избежать шаффла во время `join` (пре-шаффлинг).

<h3>2. Spark SQL</h3>
<h4>Основы</h4>

Spark SQL - это модуль Spark для работы со структурированными данными. Он позволяет выполнять SQL-запросы, а также читать и записывать данные в различных структурированных форматах.

Основные компоненты:
- DataFrame: Распределенная коллекция данных, организованная в именованные столбцы. Эквивалент таблицы в реляционной БД.
- DataSet: Типизированная версия DataFrame, доступна в Java и Scala.
- Catalyst Optimizer: Механизм оптимизации запросов, который строит и оптимизирует план выполнения запроса.
- Tungsten: Проект по оптимизации исполнения, работающий на уровне байт-кода.

Отличия от обычного SQL:
- Распределенное выполнение: Запросы выполняются распределенно на кластере, в отличие от традиционных СУБД, которые обычно работают на одном сервере.
- Ленивое выполнение: Как и в RDD, вычисления в Spark SQL происходят только при вызове действия (например, show, count, write).
- Источники данных: Spark SQL может работать с различными источниками (Hive, Parquet, JSON, JDBC и т.д.) и позволяет объединять данные из разных источников.
- UDF (User Defined Functions): Возможность определять собственные функции, но их производительность может быть ниже встроенных из-за того, что они не всегда могут быть оптимизированы Catalyst.
- Поддержка Hive: Spark SQL может работать с Hive метастаром и выполнять Hive QL запросы.

Пример работы с Spark SQL:
```python
df = spark.read.json("examples/src/main/resources/people.json")       # Создание DataFrame
df.createOrReplaceTempView("people")                                  # Регистрация DataFrame как временной таблицы
results = spark.sql("SELECT name, age FROM people WHERE age > 20")    # Выполнение SQL-запроса
```

<h4>Типы данных</h4>

Spark SQL поддерживает широкий спектр типов данных, которые определены в пакете `org.apache.spark.sql.types`. Они делятся на простые (atomic) и сложные (complex).

Простые типы данных:
- StringType: Строки.
- BinaryType: Бинарные данные.
- BooleanType: Логический тип (true/false).
- DateType: Дата (без времени).
- TimestampType: Дата и время.

Числовые типы:
- ByteType: 1 байт.
- ShortType: 2 байта.
- IntegerType: 4 байта.
- LongType: 8 байт.
- FloatType: 4 байта (с плавающей точкой).
- DoubleType: 8 байт (с плавающей точкой).
- DecimalType: Числа с фиксированной точностью и масштабом.

Сложные типы данных:
- ArrayType: Массив элементов одного типа. ArrayType(IntegerType) - массив целых чисел.
- MapType: Ассоциативный массив (ключ-значение). Ключи должны быть одного типа, значения - другого (или того же). MapType(StringType, IntegerType) - отображение строк в целые числа.
- StructType: Структура (или запись), которая содержит несколько полей с заданными типами и именами. StructType([StructField("name", StringType()), StructField("age", IntegerType())]) - структура с полями name (строка) и age (целое).

Работа с типами данных:
```python
from pyspark.sql.types import *

# Определение схемы с сложными типами
schema = StructType([
    StructField("name", StringType(), True),
    StructField("age", IntegerType(), True),
    StructField("addresses", ArrayType(StructType([
        StructField("street", StringType(), True),
        StructField("city", StringType(), True)
    ])), True),
    StructField("properties", MapType(StringType(), StringType()), True)
])

# Создание DataFrame с указанием схемы
df = spark.createDataFrame([], schema)
```

<h4>Соединение таблиц</h4>

Spark SQL позволяет выполнять соединения таблиц с помощью кода SQL или DataFrame API:
```python
# SQL
result_sql = spark.sql("""
    SELECT name, salary, department_name
    FROM employees
    [LEFT|RIGHT|FULL] JOIN departments USING (dept_id)
""")

# DataFrame API
result_df = employees_df.join(departments_df, "dept_id", "inner")          # INNER JOIN
result_df = employees_df.join(departments_df, "dept_id", "left_outer")     # LEFT JOIN
result_df = employees_df.join(departments_df, "dept_id", "right_outer")    # RIGHT JOIN
result_df = employees_df.join(departments_df, "dept_id", "full_outer")     # FULL JOIN
result_df = employees_df.crossJoin(projects_df)                            # CROSS JOIN
```

Если колонки в таблицах называются по разному, можно использовать следующий вариант соединения:
```python
result_df = employees_df.join(departments_df, employees_df.dept_id == departments_df.id, "inner")
```

<h4>Агрегация данных</h4>

Основные агрегатные функции:
```python
from pyspark.sql import functions as F

# Простые агрегации
aggregated = df.agg(
    F.count("id").alias("total_count"),
    F.avg("salary").alias("avg_salary"),
    F.sum("sales").alias("total_sales"),
    F.min("age").alias("min_age"),
    F.max("age").alias("max_age"),
    F.stddev("salary").alias("salary_stddev")
)
```

Группировка с агрегацией:
```python
# SQL
grouped_sql = spark.sql("""
    SELECT
        department,
        COUNT(*) as employee_count,
        AVG(salary) as avg_salary,
        MAX(salary) as max_salary,
        SUM(salary) as total_salary_budget
    FROM employees
    GROUP BY department
""")

# DataFrame API
grouped_df = employees_df.groupBy("department").agg(
    F.count("*").alias("employee_count"),
    F.avg("salary").alias("avg_salary"),
    F.max("salary").alias("max_salary"),
    F.sum("salary").alias("total_salary_budget")
)
```

<h4>Оконные функции</h4>

Функции окна позволяют выполнять вычисления across set of rows, которые связаны с текущей строкой. Примеры:
- Ранжирование: `ROW_NUMBER()`, `RANK()`, `DENSE_RANK()`
- Аналитические: `LAG()`, `LEAD()`, `FIRST_VALUE()`, `LAST_VALUE()`
- Агрегатные: `SUM()`, `AVG()`, `MIN()`, `MAX()`

Пример на Spark SQL:
```python
from pyspark.sql import Window

# Базовый пример
windowSpec = Window.partitionBy("department").orderBy("salary")
df_with_row_number = df.withColumn("row_number", row_number().over(windowSpec))

# Сложный пример
windowSpec = Window.partitionBy("department").orderBy("date").rowsBetween(-1, 1)
df.withColumn("moving_avg", F.avg("sales").over(windowSpec))
```

<h4>Подключение к Hive</h4>

Spark SQL поддерживает интеграцию с Hive. Это позволяет читать и записывать данные в таблицы Hive, используя существующий Hive Metastore. Для подключения к Hive необходимо иметь конфигурационные файлы Hive (hive-site.xml, core-site.xml, hdfs-site.xml) в classpath Spark или настроить Spark Session с указанием конфигураций.

Пример настройки Spark Session для работы с Hive:
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("HiveIntegration") \
    .config("spark.sql.warehouse.dir", "/user/hive/warehouse") \
    .config("hive.metastore.uris", "thrift://localhost:9083") \
    .enableHiveSupport() \
    .getOrCreate()
```
`.enableHiveSupport()` позволяет использовать Hive SerDes, Hive UDFs и доступ к Hive Metastore.

После этого можно выполнять HiveQL запросы:
```python
# Создание таблицы, если не существует
spark.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING HIVE")

# Загрузка данных в таблицу
spark.sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src")

# Выполнение запроса
spark.sql("SELECT * FROM src").show()
```

Также можно использовать DataFrame API для чтения и записи таблиц Hive:
```python
# Чтение таблицы Hive
df = spark.table("src")

# Запись в таблицу Hive
df.write.saveAsTable("src_backup")
```

<h4>Оптимизация производительности</h4>

Оптимизация производительности в Spark включает в себя настройку различных компонентов: конфигурации Spark, кода приложения, кластера и т.д.

Настройка конфигурации Spark:
- Использование памяти: Настройка памяти исполнителей (executors) и драйвера. Ключевые параметры: `spark.executor.memory`, `spark.driver.memory`, `spark.memory.fraction`, `spark.memory.storageFraction`.
- Параллелизм: Увеличение числа исполнителей и ядер. Параметры: `spark.executor.instances`, `spark.executor.cores`, `spark.default.parallelism`. Обычно `spark.default.parallelism` рекомендуется устанавливать в 2-3 раза больше количества ядер в кластере.
- Сериализация: Использование Kryo сериализации (более эффективная, чем Java сериализация) через `spark.serializer=org.apache.spark.serializer.KryoSerializer`.
- Данные shuffle: Настройка параметров shuffle, таких как `spark.sql.shuffle.partitions` (количество партиций после shuffle, по умолчанию 200) и `spark.shuffle.service.enabled` (для динамического выделения ресурсов).

Кэширование и персистентность:
- Кэширование данных в памяти или на диске с помощью `cache()`, `persist()` с выбором уровня хранения (`MEMORY_ONLY`, `MEMORY_AND_DISK`, etc.).
- Использование кэширования для данных, которые используются многократно.

Оптимизация запросов с помощью Catalyst Optimizer: Spark SQL использует Catalyst Optimizer для преобразования логического плана запроса в физический план. Можно влиять на оптимизацию через:
- Настройку параметров (например, `spark.sql.adaptive.enabled` для адаптивной оптимизации запросов в Spark 3.0 и выше).
- Использование `broadcast join` для небольших таблиц (`spark.sql.autoBroadcastJoinThreshold`).
- Ведение статистики по таблицам (через `ANALYZE TABLE`).

Партиционирование: Правильное партиционирование данных может значительно ускорить обработку. Используйте партиционирование по ключам, которые часто используются в фильтрах и соединениях. Предотвращение data skew (перекоса данных) путем выбора равномерно распределенных ключей партиционирования.

<h4>Оптимизация для больших данных</h4>