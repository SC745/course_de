<h2>MapReduce и YARN</h2>
<h3>1. MapReduce</h3>

MapReduce — это модель обработки для анализа больших объемов данных. Работает, разделяя задачу на множество маленьких задач, которые могут обрабатываться параллельно.

Ключевые понятия:
- Ключ-значение (Key-Value): Все данные в MapReduce представлены в виде пар ключ-значение. Входные, промежуточные и выходные данные — всё это пары.
- Map (Отображение): Функция, которая обрабатывает входную пару и генерирует набор промежуточных пар ключ-значение.
- Reduce (Свёртка): Функция, которая принимает промежуточный ключ и список всех значений, связанных с этим ключом, и "сворачивает" их, производя конечный результат (обычно другой список значений, часто всего одно значение).

MapReduce идеально подходит для задач, которые можно выразить как агрегацию или суммирование данных по какому-либо ключу. Классические примеры использования:
- Инвертирование индекса для поисковых систем: Самый известный пример. Преобразование "документ -> слова" в "слово -> список документов".
- Подсчет частоты слов (Word Count): "Hello World" для MapReduce. Подсчет, сколько раз каждое слово встречается в огромной коллекции текстов.
- Анализ логов: Анализ веб-логфайлов для подсчета посещений по URL, выявления ошибок, анализа поведения пользователей.
- Data Mining и машинное обучение: Вычисление матриц совместной встречаемости, алгоритмы кластеризации (например, k-Means), обработка графов (например, PageRank).
- ETL-процессы (Extract, Transform, Load): Извлечение данных из сырых источников, их преобразование и загрузка в хранилище данных.

Компоненты MapReduce:
- JobTracker: главный узел, который управляет всеми заданиями и ресурсами в кластере.
- TaskTracker: рабочие узлы, которые выполняют задачи, назначенные JobTracker.
- Job: единица работы, которая состоит из входных данных, программ Map и Reduce и конфигурации.
- Task: экземпляр выполнения Map или Reduce на части данных.

<h4>Проблемы классического MapReduce</h4>

MapReduce фреймворк (в Hadoop 1) состоял из двух основных компонентов:
- JobTracker: В предыдущих версиях Hadoop находился на мастер-узле и отвечал за распределение задач (jobs) по узлам-воркерам, управление ими и повторное выполнение задач в случае необходимости.
- TaskTracker: Узлы-воркеры, которые обрабатывают задачи, распределенные JobTracker'ом. Каждый TaskTracker отвечает за выполнение задач в рамках MapReduce-задания и отправку результатов обратно в JobTracker.

Проблемы классического MapReduce (Hadoop 1):
- JobTracker был единой точкой отказа и должен был управлять как ресурсами, так и выполнением заданий, что приводило к высокой нагрузке.
- Масштабируемость была ограничена, так как JobTracker должен был управлять всеми задачами и ресурсами в кластере.
- Поддерживались только задания MapReduce, что делало невозможным запуск других моделей вычислений (например, интерактивных запросов, потоковой обработки) в том же кластере.

Использование YARN решает эти проблемы и дает следующие преимущества:
- Масштабируемость: YARN отделяет управление ресурсами от управления заданиями, что позволяет кластеру масштабироваться до тысяч узлов. В классическом MapReduce JobTracker был перегружен управлением и тем и другим.
- Универсальность: YARN позволяет запускать не только MapReduce-задания, но и другие фреймворки (например, Apache Spark, Apache Tez, Apache Flink) в одном кластере. Это позволяет использовать один кластер для различных рабочих нагрузок (пакетная обработка, интерактивные запросы, потоковая обработка и т.д.).
- Улучшенное использование ресурсов: ResourceManager в YARN распределяет ресурсы динамически между всеми приложениями, что приводит к лучшему использованию кластера. В классическом MapReduce слоты были закреплены за Map и Reduce задачами, что могло приводить к простоям.
- Отказоустойчивость: В YARN отказ ApplicationMaster приводит к перезапуску приложения, но ResourceManager глобальный и может быть сделан отказоустойчивым (через HA). В классическом MapReduce отказ JobTracker останавливал весь кластер.
- Мультитенантность: YARN позволяет нескольким пользователям и приложениям одновременно использовать кластер, изолируя их друг от друга.

<h4>Основные этапы выполнения задач</h4>

Выполнение задачи в MapReduce состоит из нескольких четко определенных этапов:
1. Подготовка (Input Splits) - система разбивает входные данные на логические куски фиксированного размера, называемые сплитами, которые будут обработаны отдельными задачами на фазе Map.
2. Фаза Map (Отображение) - на этом этапе запускается множество Mapper'ов (задач Map), обычно по одной на каждый сплит. Mapper читает данные построчно (или по другим записям), пользовательская функция map применяется к каждой входной записи. Её задача — преобразовать входные данные в промежуточный формат "ключ-значение".
3. Shuffle и Sort (Перемешивание и Сортировка) - состоит из двух подэтапов:
  - Partitioning (Разделение): Выходные данные всех Mapper'ов распределяются по Reduce-задачам. По умолчанию используется хэш-функция от ключа: hash(ключ) mod N, где N — количество Reduce-задач. Это гарантирует, что все пары с одинаковым ключом попадут в одну и ту же Reduce-задачу.
  - Sorting (Сортировка): Система собирает все промежуточные пары "ключ-значение" для одного и того же ключа (например, для слова "hello") и сортирует их по ключу. Перед началом фазы Reduce данные для каждого Reducer'а представляют собой отсортированный список, где за одним ключом следует список всех его значений.
4. Фаза Reduce (Сведение) - на этом этапе запускаются Reducer'ы (задачи Reduce). Пользовательская функция reduce применяется к каждому уникальному ключу и его списку значений. Её задача — агрегировать эти значения и выдать финальный результат - отсортированную группу данных, где уникальный ключ ассоциирован со списком всех его значений.
5. Запись результата (Output) - каждый Reducer записывает свои выходные данные в конечные файлы (обычно по одному на узел, где работал Reducer). Эти файлы хранятся в распределенной файловой системе (например, HDFS в Hadoop).

<h4>Механизмы MapReduce</h4>

Shuffle - Это процесс переноса данных с выходов Mapper'ов на входы Reducer'ов. Это самый сетевой и ресурсоемкий этап во всем конвейере MapReduce. Этапы Shuffle:
1. Партиционирование (Partitioning): Сразу после выполнения map(), результат записывается в память (буфер) и делится на партиции с помощью партиционера (Partitioner). Партиционер по умолчанию использует хэш-функцию от ключа: hash(key) mod R, где R — количество редьюсеров. Это гарантирует, что все пары с одинаковым ключом попадут в одну и ту же партицию (и, следовательно, к одному редьюсеру).
2. Сортировка (Sort): Внутри каждой партиции данные сортируются по ключу. Это делается непосредственно на стороне маппера.
3. Спил (Spill) на диск: Когда буфер в памяти заполняется, его содержимое (уже разбитое на партиции и отсортированное) "сбрасывается" (spill) на локальный диск маппера. Происходит несколько таких сбросов.
4. Слияние (Merge): После завершения всех map-операций, все спил-файлы на диске объединяются в один отсортированный файл для каждой партиции. Теперь у нас есть по одному файлу для каждого редьюсера на каждом маппере.
5. Копирование (Fetch): Редьюсеры запускают HTTP-копирование (fetch) своих партиций со всех мапперов.
6. Слияние на стороне редьюсера: По мере поступления данных, редьюсер также выполняет их слияние в единый отсортированный поток. Именно этот поток подается на вход функции reduce(key, iterator).

Combiner - мини-редьюсер, который выполняется непосредственно на стороне маппера, еще до этапа Shuffle. Значительно уменьшает объем данных, передаваемых по сети. Комбайнер принимает выходные данные маппера и "объединяет" их локально. Часто его код идентичен коду редьюсера.

Speculative Execution (спекулятивное выполнение) — это механизм для борьбы с медленно выполняющимися задачами (stragglers). Иногда некоторые задачи выполняются медленнее других из-за проблем с оборудованием (например, медленный диск) или нагрузки на узел. Это может задержать все задание, так как JobTracker ждет завершения всех задач. Как работает:
- JobTracker отслеживает прогресс всех задач. Если он замечает, что некоторые задачи выполняются значительно медленнее, чем средние по кластеру, он запускает дублирующую задачу (speculative task) на другом узле.
- Обе задачи (оригинальная и спекулятивная) выполняются параллельно. Та из них, которая завершится первой, будет использована, а вторая будет убита.
- Это не применяется к задачам, которые уже близки к завершению, и не применяется, если спекулятивное выполнение отключено.

<h4>Кастомизация</h4>

MapReduce спроектирован как расширяемая архитектура. Можно переопределить почти любой его компонент.

Partitioner (Партиционер) - это класс, который определяет, в какую партицию (и, следовательно, на какой редьюсер) отправится пара (key, value) с выхода маппера. По умолчанию HashPartitioner - распределяет ключи по редьюсерам на основе хэша ключа, обеспечивая равномерное распределение. Кастомизация может потребоваться в том случае, если, например, один ключ встречается на порядки чаще других: HashPartitioner отправит все его значения на один редьюсер, создав "горячую точку". Кастомный партиционер может, например, разбить этот ключ на несколько псевдо-ключей.

InputFormat (Формат ввода) - это класс, который отвечает за две вещи:
- Разделение данных на сплиты (splits): Определяет, как входной файл (например, в HDFS) будет разбит на куски для параллельной обработки мапперами.
- Чтение данных: Предоставляет RecordReader, который читает данные из сплита и преобразует их в пары (key, value) для функции map.

Стандартные форматы:
- `TextInputFormat`: По умолчанию для текстовых файлов. Ключ — смещение в байтах, значение — строка текста.
- `KeyValueTextInputFormat`: Для текстовых файлов в формате "ключ[табуляция]значение".
- `SequenceFileInputFormat`: Для бинарных файлов SequenceFile (очень эффективно для промежуточных данных MapReduce).

Кастомизация применяется для чтения файлов в нестандартных форматах (например, XML)

OutputFormat (Формат вывода) - Аналогично InputFormat, но отвечает за запись результатов работы редьюсеров. Можно писать в базу данных, в NoSQL-хранилище, в свой собственный бинарный формат и т.д.

<h4>Оптимизация MapReduce</h4>

Высокая производительность в MapReduce достигается за счет минимизации операций I/O (диск и сеть).

Методы оптимизации:
- Использование Combiners
- Установка размера блока HDFS: Количество Map-задач определяется количеством входных сплитов (обычно равно количеству HDFS-блоков). Слишком много мелких задач — большие накладные расходы. Слишком мало — плохой параллелизм.
- Настройка количества Reduce-задач: По умолчанию их 1 — это ужасно для параллелизма. Слишком мало редьюсеров: Недоиспользуется кластер, один редьюсер может стать узким местом. Слишком много редьюсеров: Создает большую нагрузку на shuffle, много мелких выходных файлов. Эмпирическое правило: Количество редьюсеров должно быть таким, чтобы каждый редьюсер обрабатывал данные от ~1 ГБ до ~5 ГБ. Можно настраивать в процессе выполнения.
- Сжатие данных:
  - Промежуточное сжатие (map output): Сжатие выхода мапперов кардинально снижает объем данных, передаваемых через сеть на этапе shuffle.
  - Сжатие выхода редьюсеров: Полезно для экономии места в HDFS, если эти данные будут потом читаться другими задачами (например, в Hive).
- Тюнинг параметров:
  - Размер буфера в памяти для spill-операций (io.sort.mb).
  - Процент заполнения буфера, при котором начинается spill (io.sort.spill.percent).
  - Количество потоков для копирования данных на редьюсере (mapreduce.reduce.shuffle.parallelcopies).

<h3>2. Hadoop</h3>

<h4>Запуск задачи WordCount</h4>

WordCount — это "Hello, World!" в мире распределенных вычислений. Эта программа подсчитывает количество вхождений каждого слова в заданном наборе файлов.

Процесс запуска:
1. Подготовка данных: Исходные текстовые файлы помещаются в HDFS (Hadoop Distributed File System) в входную директорию.
```bash
hdfs dfs -put /local/path/to/input /user/username/input
```
2. Подготовка кода: Написанный Java-код (реализующий классы Mapper и Reducer) компилируется и упаковывается в JAR-файл.
3. Запуск задачи: Задача запускается с помощью команды hadoop jar. Пример команды:
```bash
hadoop jar wordcount.jar org.apache.hadoop.examples.WordCount /user/username/input /user/username/output
```
Где:
- `wordcount.jar` — ваш собранный JAR-файл.
- `org.apache.hadoop.examples.WordCount` — главный класс, содержащий метод main.
- `/user/username/input` — путь к входным данным в HDFS.
- `/user/username/output` — путь, куда HDFS положит результаты (этой директории не должно существовать перед запуском).

<h4>Конфигурационные файлы</h4>

Hadoop крайне гибко настраивается через XML-файлы конфигурации. Основные файлы находятся в директории `$HADOOP_HOME/etc/hadoop/`.

Ключевые конфигурационные файлы:
- `core-site.xml` — настройки, общие для всей экосистемы Hadoop.
  - `fs.defaultFS`: Определяет URI файловой системы по умолчанию (адрес NameNode). Например, hdfs://namenode-host:9000.
- `hdfs-site.xml` — настройки specifically для HDFS.
  - `dfs.replication`: Количество реплик каждого блока данных (по умолчанию 3).
  - `dfs.namenode.name.dir`: Путь на локальном диске, где NameNode хранит метаданные (список блоков, их расположение и т.д.).
  - `dfs.datanode.data.dir`: Путь на локальном диске, где DataNode хранит сами блоки данных.
- `mapred-site.xml` — настройки для фреймворка MapReduce (в версиях Hadoop 2 и 3).
  - `mapreduce.framework.name`: Указывает, где запускать задачи MapReduce (обычно yarn).
  - Настройки памяти для Map и Reduce задач (например, mapreduce.map.memory.mb).
- `yarn-site.xml` — настройки для менеджера ресурсов YARN (Yet Another Resource Negotiator).
  - `yarn.resourcemanager.hostname`: Хост, где работает ResourceManager.
  - `yarn.nodemanager.aux-services`: Определяет вспомогательные службы, например, `mapreduce_shuffle`, которые нужны для работы MapReduce поверх YARN.
  - `yarn.nodemanager.resource.memory-mb`: Общий объем памяти на узле, который YARN может использовать для контейнеров.

<h4>Концепция Data Locality</h4>

Data Locality (Локалитет данных) — это фундаментальный принцип производительности в Hadoop. Суть концепции: "Перемещать вычисления к данным дешевле, чем перемещать данные к вычислениям".

Почему это важно:
- Данные в HDFS огромны (терабайты/петабайты).
- Передача этих данных по сети создает большую нагрузку и является "узким местом".
- Пропускная способность сети (bandwidth) — это самый дефицитный ресурс в кластере.

Когда ResourceManager (через ApplicationMaster) планирует запуск задачи Map, он смотрит на расположение данных:
- Идеальная локальность (Data Local): Задача Map запускается на том же самом физическом узле, где находится обрабатываемый блок данных. Данные читаются с локального диска. Это наилучший вариант.
- Локальность в пределах стойки (Rack Local): Если на узле с данными нет свободных ресурсов, задача запускается на другом узле в той же самой сетевой стойке (rack). Передача данных пойдет через коммутатор стойки, что быстрее, чем через магистраль сети дата-центра.
- Разные стойки (Off-Rack): Если нет свободных ресурсов в стойке, задача запускается на узле в другой стойке. Данные будут передаваться по магистральной сети, что медленнее всего.

Задачи Reduce по умолчанию не имеют строгой локальности, так как они должны получать данные со всех Mapper'ов, и их запускают там, где есть свободные ресурсы.

<h4>Fault Tolerance</h4>

Fault Tolerance (Отказоустойчивость) — это способность системы продолжать работать при сбоях отдельных компонентов. Hadoop спроектирован так, чтобы быть отказоустойчивым на уровне как хранения данных (HDFS), так и их обработки (MapReduce/YARN).

Отказоустойчивость HDFS достигается с помощью репликации данных и автоматического восстановления реплик при сбоях DataNode, а также с помощью обеспечения отказоустойчивости NameNode (в системах высокой доступности).

Отказоустойчивость MapReduce/YARN достигается с помощью повторного выполнения упавших задач, перезапуска контейнера ApplicationMaster и настройки высокой доступности ResourceManager

<h3>3. YARN</h3>

Управление ресурсами в Hadoop осуществляется в основном с помощью YARN (Yet Another Resource Negotiator), который отвечает за распределение системных ресурсов между различными приложениями, работающими в кластере Hadoop. YARN позволяет динамически распределять ресурсы в зависимости от потребностей приложений, что обеспечивает эффективное использование ресурсов. Это контрастирует со статическим распределением, при котором ресурсы для каждого приложения выделяются строго фиксированной порцией.

Основные компоненты YARN:
- ResourceManager (RM): Менеджер ресурсов, который отвечает за распределение ресурсов, необходимых для работы распределенных приложений, и наблюдение за узлами кластера, где эти приложения выполняются. ResourceManager включает планировщик ресурсов (Scheduler) и диспетчер приложений (ApplicationsManager, AsM).
- ApplicationMaster (AM): Мастер приложения, ответственный за планирование его жизненного цикла, координацию и отслеживание статуса выполнения, включая динамическое масштабирование потребления ресурсов, управление потоком выполнения, обработку ошибок и искажений вычислений, выполнение локальных оптимизаций. Каждое приложение имеет свой экземпляр ApplicationMaster. ApplicationMaster выполняет произвольный пользовательский код и может быть написан на любом языке программирования благодаря расширяемым протоколам связи с менеджером ресурсов и менеджером узлов.
- NodeManager (NM): Менеджер узла – агент, запущенный на узле кластера, который отвечает за отслеживание используемых вычислительных ресурсов (CPU, RAM и пр.), управление логами и отправку отчетов об использовании ресурсов планировщику. NodeManager управляет абстрактными контейнерами – ресурсами узла, доступными для конкретного приложения.
- Контейнер (Container): Набор физических ресурсов (ЦП, память, диск, сеть) в одном вычислительном узле кластера.

Принцип работы Hadoop YARN можно описать следующим образом:
1. Клиентское приложение отправляет запрос в кластер
2. Менеджер ресурсов выделяет необходимые ресурсы для контейнера и запускает ApplicationMaster для обслуживания этого приложения
3. ApplicationMaster отправляет запрос менеджеру узла NodeManager, включая контекст запуска контейнера Container Launch Context (CLC)
4. ApplicationMaster выделяет контейнеры для приложения в каждом узле и контролирует их работу до завершения работы приложения
5. Для запуска контейнера менеджер узла копирует в локальное хранилище все необходимые зависимости (данные, исполняемые файлы, архивы)
6. По завершении задачи мастер приложения отменяет выделенный контейнер в диспетчере ресурсов, завершая жизненный цикл распределенного задания.
7. Клиент может отслеживать состояние распределенного приложения, обращаясь к менеджеру ресурсов или сразу к мастеру приложения.

<h4>Менеджеры в YARN</h4>

ResourceManager — это главный администратор кластера. Его основные функции делятся на две категории:
- Планировщик (Scheduler): Распределяет ресурсы запущенным приложениям, используя встроенные политики планирования (FIFO, Capacity Scheduler, Fair Scheduler)
- Менеджер приложений (ApplicationsManager): Принимает запросы на запуск новых приложений (переданных клиентом), запускает ApplicationMaster, выполняет мониторинг состояния ApplicationMaster, в случае сбоя перезапускает.

NodeManager — это агент, который работает на каждом отдельном узле данных в кластере. Его функции:
- Управление ресурсами узла: Отслеживает доступные ресурсы на своем узле (процессор, память, диск) и постоянно сообщает об их наличии и использовании ResourceManager.
- Запуск и управление контейнерами: Получает команды от ApplicationMaster (которые, в свою очередь, получили ресурсы от RM) на запуск контейнеров. NM создает контейнер (изолированное процессное окружение) с заданными ресурсами и запускает в нем процесс (например, задачу MapReduce или исполнитель Spark).
- Мониторинг контейнеров: Отслеживает использование ресурсов (CPU, память) запущенными контейнерами. Если контейнер превышает лимиты, NM может его завершить.
- Управление жизненным циклом приложения на узле: По запросу ResourceManager'а, NodeManager может завершить контейнер или даже весь ApplicationMaster, если это необходимо.

ApplicationMaster - процесс, координирующий выполнение приложения в YARN. Основные функции:
- Переговоры о ресурсах: AM запрашивает у ResourceManager контейнеры для выполнения задач приложения.
- Управление задачами: После получения контейнеров AM работает с NodeManager для запуска в них задач (например, мапперов или редьюсеров в MapReduce).
- Отслеживание прогресса: AM отслеживает прогресс выполнения задач, обрабатывает сбои (например, повторно запускает упавшие задачи).
- Завершение приложения: Когда все задачи завершены, AM отправляет финальный отчет ResourceManager и завершает свою работу.

<h4>Планировщики и очереди</h4>

YARN использует планировщики для распределения ресурсов. Планировщик обрабатывает распределение ресурсов для заданий, отправленных в YARN, согласно политике выделения, установленной для его типа. Различают 3 типа планировщиков:
- FIFO (First In First Out): Самый простой, который запускает приложения в порядке подачи, помещая их в очередь. Приложение, отправленное первым, сначала получает ресурсы, а по его завершении планировщик обслуживает следующее приложение в очереди. FIFO не подходит для общих кластеров, поскольку большие приложения будут занимать все ресурсы, а очереди станут длиннее из-за более низкой скорости обслуживания.
- Capacity: Более сложный вариант FIFO, который позволяет совместно использовать кластер Hadoop между группами внутри организации, выделяя каждой команде определенную емкость общего кластера. Очереди могут быть дополнительно разделены иерархически, но приложения в них по умолчанию планируются с использованием расписания FIFO. Отдельная выделенная очередь позволяет запускать небольшие задания сразу после их отправки.
- Fair: Самый продвинутый вариант планировщика с учетом приоритетности заданий, который стремится распределить ресурсы так, чтобы все запущенные приложения получили одинаковую долю. Fair Scheduler позволяет приложениям YARN совместно и динамически использовать ресурсы в большом кластере Hadoop без предварительного указания их емкости.

FIFO-планировщик блокирует небольшое задание до завершения большого задания. Capacity-планировщик поддерживает отдельную очередь для небольших заданий, чтобы запускать их сразу после поступления запроса. Однако в этом случае выполнение больших заданий требует больше времени из-за лимитов емкости кластера, ограниченной очередью. Так Capacity-планировщик гарантирует, что приоритетные и мелкие задания не будут слишком долго задерживаться по сравнению с планировщиком FIFO. Но при наличии свободных ресурсов, Capacity-планировщик может выделить их для заданий в очереди, даже если это повысит ее емкость. Это называется эластичностью очереди.

У Fair-планировщика нет требований к резервированию емкости: он динамически распределяет ресурсы по всем принятым заданиям. Когда задание запускается, оно получает все ресурсы кластера, если является единственным. Следующее запущенное задание получит ресурсы при высвобождении контейнеров – фиксированного объема ОЗУ и ЦП. По завершении небольшого задания планировщик назначает ресурсы большому. Таким образом, Fair-планировщик устраняет недостатки FIFO и Capacity, позволяя своевременно завершать небольшие задания с высокой степенью утилизации кластера, которая показывает эффективность использования его ресурсов. На практике именно этот вариант используется чаще всего.

Очереди в YARN — это механизм организации многопользовательской работы и управления ресурсами кластера. Они позволяют:
- Изолировать ресурсы между разными командами, отделами или проектами
- Гарантировать пропускную способность для критически важных приложений
- Ограничивать "жадные" приложения, чтобы одно задание не монополизировало весь кластер
- Управлять приоритетами выполнения заданий
- Обеспечивать справедливое распределение ресурсов между пользователями

Важные моменты:
- Очереди могут быть иерархическими (иметь подочереди).
- Можно настроить политики для размещения приложений в очередях (на основе имени пользователя, группы и т.д.).
- После изменения конфигурации необходимо обновить настройки планировщика без перезапуска RM: yarn rmadmin -refreshQueues.

Пример настройки:
```xml
<configuration>
  <!-- Определение корневой очереди -->
  <property>
    <name>yarn.scheduler.capacity.root.queues</name>
    <value>queueA,queueB,queueC</value>
  </property>

  <!-- Емкость (в процентах) для каждой очереди -->
  <property>
    <name>yarn.scheduler.capacity.root.queueA.capacity</name>
    <value>50</value>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.queueB.capacity</name>
    <value>30</value>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.queueC.capacity</name>
    <value>20</value>
  </property>

  <!-- Максимальная емкость, которую может использовать очередь (может превышать гарантированную) -->
  <property>
    <name>yarn.scheduler.capacity.root.queueA.maximum-capacity</name>
    <value>70</value>
  </property>

  <!-- Права доступа для отправки заданий в очередь -->
  <property>
    <name>yarn.scheduler.capacity.root.queueA.acl_submit_applications</name>
    <value>userA,userB</value>
  </property>

  <!-- Права на администрирование очереди -->
  <property>
    <name>yarn.scheduler.capacity.root.queueA.acl_administer_queue</name>
    <value>admin</value>
  </property>

  <!-- Максимальное количество параллельных приложений в очереди -->
  <property>
    <name>yarn.scheduler.capacity.root.queueA.maximum-applications</name>
    <value>100</value>
  </property>
</configuration>
```

<h4>Мониторинг и метрики</h4>

YARN предоставляет мощные встроенные инструменты для мониторинга состояния как всего кластера, так и отдельных приложений:
1. Web UI ResourceManager: Показывает общее состояние кластера, список приложений, узлы и очереди планировщика. Обычно расположен по адресу `http://<rm-host>:8088`
2. Web UI ApplicationMaster: Показывает детальную информацию о конкретном приложении — прогресс задач, логи, использованные ресурсы, граф выполнения (для Spark), список мапперов/редьюсеров (для MapReduce) и т.д. Ссылка на него доступна из UI ResourceManager.
3. Инструменты командной строки (yarn command):
- `yarn application -list` — список всех приложений.
- `yarn application -status <application_id>` — детальный статус конкретного приложения.
- `yarn node -list` — показывает список всех узлов в кластере, что позволяет оценить общее количество ресурсов.
- `yarn node -status <node_id>` — предоставляет детальную информацию о состоянии и метриках конкретного узла.
- `yarn queue -status <queue_name>` — выводит информацию о конкретной очереди YARN.
- `yarn logs -applicationId <application_id>` — получение агрегированных логов приложения.
4. JMX-метрики: И ResourceManager, и NodeManager, и ApplicationMaster предоставляют метрики через JMX. Эти метрики можно собирать системами мониторинга, такими как Prometheus (через JMX Exporter) или Ganglia.
- Примеры метрик RM: cluster-metrics (количество узлов, приложений), queue-metrics (использование очередей).
- Примеры метрик NM: containers-allocated, containers-completed, container-memory-usage, container-cpu-usage.
5. Логи (Logs): Логи YARN компонентов (ResourceManager, NodeManager) обычно лежат на локальных дисках узлов. Логи приложений (вывод stdout/stderr ваших задач) по умолчанию агрегируются и могут быть получены через `yarn logs` команду, что избавляет от необходимости ходить по всем узлам.

<h4>Балансировка нагрузки и автоматическое масштабирование</h4>

YARN автоматически балансирует нагрузку на уровне распределения контейнеров по узлам:
- Data Locality: Планировщик старается размещать контейнеры на узлах, где находятся их данные
- Распределение ресурсов: Контейнеры равномерно распределяются по всему кластеру
- Учет загрузки узлов: Планировщик избегает перегрузки отдельных NodeManager

YARN сам по себе не управляет размером кластера, но интегрируется с системами оркестрации:
- YARN Node Labels: Позволяют маркировать узлы (например, "GPU", "High-Memory", "Compute-Intensive"), приложения могут запрашивать контейнеры на узлах с определенными метками
- Интеграция с облачными провайдерами: В Hadoop 3.x+ появилась поддержка автоматического масштабирования YARN Auto-scaling. Также можно использовать внешние системы (Kubernetes, Docker Swarm, Auto Scaling Groups)

Пример настройки автоматического масштабирования:
```bash
# Установка политик автоскейлинга
yarn rmadmin -addToClusterNodeLabels "GPU,HighMem"
yarn rmadmin -replaceLabelsOnNode "node1:8088=GPU" "node2:8088=HighMem"
```

<h4>Высокая доступность и резервирование ресурсов</h4>

ResourceManager — единая точка отказа, поэтому HA критически важно для production-кластеров.

Архитектура HA ResourceManager:
- Active-Standby режим: Один Active RM и один или несколько Standby RM
- Автоматический failover: При падении Active RM, один из Standby автоматически становится активным
- Хранение состояния: Состояние RM сохраняется в ZooKeeper или HDFS

Настройка высокой доступности:

```xml
<!-- yarn-site.xml -->
<configuration>
  <!-- Включение HA -->
  <property>
    <name>yarn.resourcemanager.ha.enabled</name>
    <value>true</value>
  </property>

  <!-- ID кластера RM -->
  <property>
    <name>yarn.resourcemanager.cluster-id</name>
    <value>cluster1</value>
  </property>

  <!-- Список всех RM -->
  <property>
    <name>yarn.resourcemanager.ha.rm-ids</name>
    <value>rm1,rm2,rm3</value>
  </property>

  <!-- Адреса каждого RM -->
  <property>
    <name>yarn.resourcemanager.hostname.rm1</name>
    <value>rm-host1.company.com</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname.rm2</name>
    <value>rm-host2.company.com</value>
  </property>

  <!-- Хранилище состояния в ZooKeeper -->
  <property>
    <name>yarn.resourcemanager.store.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
  </property>

  <!-- Адреса ZooKeeper -->
  <property>
    <name>yarn.resourcemanager.zk-address</name>
    <value>zk1:2181,zk2:2181,zk3:2181</value>
  </property>
</configuration>
```

YARN позволяет приложениям резервировать ресурсы для будущего использования. Это важно для приложений, которые требуют гарантированного доступа к ресурсам в определенное время (например, по расписанию). Как работает:
1. Запрос резервирования: ApplicationMaster может отправить запрос на резервирование ресурсов в определенный временной интервал.
2. Планировщик с резервированием: Capacity Scheduler и Fair Scheduler поддерживают резервирование. Планировщик проверяет, есть ли достаточно ресурсов в указанное время, и если да, то резервирует их.
3. Использование резервирования: Когда наступает время, приложение может использовать зарезервированные ресурсы.

Преимущества резервирования:
- Гарантирует, что критически важные задания получат ресурсы в нужное время.
- Позволяет планировать выполнение заданий (например, ежедневные отчеты в пиковое время).