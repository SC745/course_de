<h2>Другие компоненты экосистемы Hadoop</h2>
<h3>1. Apache Ranger</h3>
<h4>Введение</h4>

Apache Ranger предлагает комплексный фреймворк для управления безопасностью и политиками доступа в Hadoop экосистеме. Он позволяет администраторам легко управлять политиками безопасности для различных служб Hadoop, включая HDFS, HBase, Hive, и другие.

Функции Ranger:
1. Централизованное управление политиками: Администратор может создавать, управлять и применять политики безопасности для всех компонентов (HDFS, Hive, Kafka и т.д.) из единого веб-интерфейса. Политики определяют, кому (пользователь/группа/роль) что разрешено делать (читать, писать, выполнять и т.д.) с каким ресурсом (база данных, таблица, столбец, топик, путь в HDFS).
2. Детализированная авторизация (Authorization): Поддерживает различные уровни контроля доступа:
  - На уровне ресурсов (например, доступ к конкретной папке в HDFS).
  - На уровне столбцов (Column Masking) — маскирование данных в определенных столбцах (например, показывать только последние 4 цифры номера кредитной карты).
  - На уровне строк (Row-Level Filtering) — динамическое фильтрование строк на основе атрибутов пользователя (например, менеджер видит только данные своего отдела).
3. Сквозной аудит (Auditing): Ranger собирает и хранит детальные логи всех событий доступа к данным. Администратор может видеть, кто, когда, к какому ресурсу обращался и была ли операция разрешена или запрещена. Эти логи можно интегрировать с внешними системами мониторинга и SIEM (например, Splunk, ELK Stack).
4. Безопасность на основе тегов (Tag-Based Policies): Эта функция позволяет отделить описание политик от физической структуры данных. С помощью Apache Atlas (система управления метаданными и governance) данные помечаются тегами (например, "PII", "Financial", "Confidential"). В Ranger затем создаются политики для этих тегов ("Только аудиторы могут читать данные с тегом PII"). Это делает политики более гибкими и управляемыми.
5. Шифрование данных (Data Encryption): Интегрируется с проектом Apache Knox для управления ключами шифрования и обеспечения безопасности данных в состоянии покоя (at-rest) и в движении (in-transit).
6. Поддержка стандартов безопасности: Легко интегрируется с корпоративными системами, такими как LDAP/Active Directory для аутентификации пользователей и Kerberos для безопасной аутентификации самих сервисов Hadoop.

<h4>Интеграция и роль в Hadoop</h4>

До появления таких систем, как Ranger, управление безопасностью в Hadoop было децентрализованным и сложным. Каждый компонент (HDFS, Hive, HBase) имел свои собственные, зачастую примитивные, механизмы безопасности (например, POSIX-права в HDFS или права доступа в Hive).

Роль Ranger — стать единым центром компетенции по безопасности для всей экосистемы:
- "Единое окно" для администратора: Вместо настройки прав в каждом сервисе отдельно, админ делает это один раз в UI Ranger.
- Согласованность политик: Обеспечивает, что политики безопасности применяются одинаково ко всем инструментам экосистемы, устраняя "слепые зоны".
- Ускорение compliance: Помогает компаниям соответствовать строгим требованиям регуляторов (таким как GDPR, HIPAA, PCI DSS) за счет централизованного аудита и детализированного контроля доступа.
- Enabler для Self-Service Analytics: Позволяет безопасно предоставлять доступ к данным большим командам аналитиков, не опасаясь утечек, так как доступ можно ограничить на очень тонком уровне (столбец, строка).
- Фактически, Ranger стал де-факто стандартом для управления безопасностью в современных дистрибутивах Hadoop, таких как Cloudera Data Platform (CDP) и Hortonworks (ныне часть Cloudera).

Ranger работает по принципу плагинов (plugins). Для каждого компонента экосистемы существует свой легковесный плагин (Ranger Plugin), который устанавливается на узлы с этими компонентами.

Как это работает:
1. Администратор создает политику в веб-UI Ranger.
2. Плагин на стороне сервиса (например, Hive) периодически опрашивает сервер Ranger (Policy Admin Server) и загружает актуальные политики в кеш.
3. Когда пользователь пытается выполнить запрос (например, SELECT * FROM sales_table), плагин Hive перехватывает этот запрос и проверяет его против закешированных политик.
4. Плагин принимает решение: разрешить или запретить операцию, и записывает событие аудита обратно на сервер Ranger.

Примеры интеграции с ключевыми компонентами:
- Apache HDFS: Доступ к файлам и директориям. Пример политики: Разрешить группе `analysts` только `READ` доступ к пути `/data/sales/`. Запретить группе `contractors` любой доступ к `/data/hr/`.
- Apache Hive: Доступ к базам данных, таблицам, представлениям и столбцам. Возможность выполнять операции (`SELECT`, `UPDATE`, `CREATE`). Пример политики: Разрешить пользователю `john` выполнять `SELECT` по таблице customers, но для столбца `credit_card` применить маскирование (маска `XXXX-XXXX-XXXX-####`).
- Apache HBase: Доступ к пространствам имен (namespaces), таблицам, семействам столбцов (column families) и ячейкам. Пример политики: Разрешить группе `service_team` делать `PUT` и `GET` в таблицу `user_sessions`.
- Apache Kafka: Доступ к топикам (topics), возможность производить (Produce) и потреблять (Consume) сообщения. Пример политики: Разрешить приложению `log_ingester` производить сообщения в топик `app_logs`. Разрешить группе `fraud_detection` потреблять сообщения из топика `transactions`.
- Apache Solr: Доступ к коллекциям (collections), наборам документов и полям. Пример политики: Запретить группе `interns` доступ к коллекции `employee_reviews`.
- YARN: Доступ к очередям (queues) и возможность отправлять задания. Пример политики: Разрешить группе `data_science` отправлять задания в очередь `ml_queue`.
- Apache Atlas: Связка "безопасность + глоссарий данных". Политики на основе тегов, созданных в Atlas. Пример политики: Создать в Atlas тег `PII`. Пометить им таблицы и столбцы с персональными данными. В Ranger создать политику: "Только члены роли `GDPR_Compliance` имеют доступ к ресурсам с тегом `PII`".
- Cloud-хранилища (S3, ADLS): В современных дистрибутивах (CDP) Ranger может управлять доступом к объектам в облачных хранилищах, согласовывая политики Ranger с native ACLs облачных провайдеров.

<h4>Типы данных и политик безопасности</h4>

Apache Ranger предоставляет возможность определять политики доступа для различных типов данных и ресурсов в экосистеме Hadoop. Ресурсы могут быть разными в зависимости от сервиса (HDFS, Hive, HBase, Kafka и т.д.).

Типы данных, для которых Ranger может управлять доступом, включают:
- В HDFS: файлы и директории (пути).
- В Hive: базы данных, таблицы, представления, столбцы и даже операции (`SELECT`, `UPDATE`, `CREATE` и т.д.).
- В HBase: пространства имен, таблицы, семейства столбцов, ячейки.
- В Kafka: топики, группы потребителей, транзакции и т.д.

Политики безопасности в Ranger можно разделить на:
- Политики доступа (Access Policies): Определяют, кто (пользователь или группа) имеет доступ к какому ресурсу и какие действия разрешены. Они могут быть разрешающими (allow) или запрещающими (deny).
- Политики маскирования данных (Data Masking Policies): Применяются к столбцам в Hive или других компонентах, чтобы маскировать чувствительные данные (например, номера кредитных карт) для определенных пользователей. Типы масок: частичное отображение, хеширование, постоянная маска и т.д.
- Политики фильтрации строк (Row-Level Filtering Policies): Позволяют ограничить доступ к строкам в таблице на основе условия (например, пользователь может видеть только строки, где регион равен 'US').

Кроме того, Ranger поддерживает политики на основе тегов (Tag-Based Policies), которые позволяют назначать политики не напрямую на ресурсы, а на теги, присвоенные ресурсам через Apache Atlas. Это обеспечивает более гибкое управление доступом, основанное на классификации данных.

<h4>Настройка политики доступа</h4>

Процесс настройки политики доступа в Ranger обычно выполняется через веб-интерфейс Ranger Admin UI. Рассмотрим пример настройки политики для Hive, шаги:
1. Войдите в веб-интерфейс Ranger (например, `http://ranger-server:6080`).
2. Перейдите к сервису, для которого вы хотите создать политику (например, Hive).
3. Нажмите на кнопку создания новой политики (Add New Policy).
4. Заполните детали политики:
  - Policy Name: Уникальное имя политики.
  - Resource Path: Укажите ресурсы, к которым применяется политика (например, база данных, таблица, столбец). Можно использовать wildcards (например, `sales_*`).
  - Select Group / Select User: Укажите пользователя или группу, для которых применяется политика.
  - Permissions: Выберите разрешения. Для каждого разрешения можно указать, разрешить (Allow) или запретить (Deny).
5. При необходимости настройте условия для маскирования данных или фильтрации строк.
6. Сохраните политику.

После сохранения политика будет распространена на соответствующий сервис (через плагин Ranger) и начнет применяться при следующих запросах.

Управление политиками:
- Редактирование: Можно изменить существующие политики.
- Удаление: Удаление ненужных политик.
- Включение/Отключение: Временное отключение политики без удаления.
- Аудит: Просмотр логов доступа для мониторинга применения политик.

Пример политики в Hive, разрешающей группе `analysts` выполнять `SELECT` ко всем столбцам таблицы `sales` в базе `default`:
```bash
Policy Name: allow-analysts-read-sales
Database: default
Table: sales
Column: *
Group: analysts
Permissions: Select -> Allow
```

<h4>KMS</h4>

KMS (Key Management Server) в контексте Hadoop относится к Hadoop KMS, который является сервером управления ключами для шифрования данных в HDFS. KMS предоставляет API для создания, управления и использования ключей шифрования, а также интегрируется с HDFS для поддержки прозрачного шифрования (HDFS Transparent Encryption).

Связь между Apache Ranger и KMS заключается в том, что Ranger может управлять политиками доступа к ключам шифрования, хранящимися в KMS. Это означает, что Ranger предоставляет авторизацию для операций с ключами в KMS. Например, с помощью Ranger можно определить:
- Кто может создавать, удалять или вращать ключи шифрования.
- Кто может получать метаданные ключей или сам материал ключа.

Как это работает:
1. KMS настроен на аутентификацию с помощью Kerberos и авторизацию через Ranger.
2. Когда пользователь или сервис пытается выполнить операцию с ключом (например, получить ключ для чтения зашифрованного файла), KMS обращается к Ranger для проверки прав доступа.
3. Ranger проверяет политики, определенные для KMS, и возвращает решение о разрешении или запрете операции.
4. Таким образом, Ranger расширяет свои функции управления безопасностью до уровня шифрования, обеспечивая единое место для управления политиками доступа к данным и ключам шифрования.

Пример политики Ranger для KMS:
```bash
Policy Name: allow-cluster-admins-manage-keys
Resource: key (или конкретное имя ключа)
User/Group: cluster-admins
Permissions: Manage -> Allow
```

<h4>Интеграция с Hive</h4>

Интеграция Apache Ranger с Apache Hive позволяет управлять доступом к данным в Hive на уровне баз данных, таблиц, столбцов и даже строк. Ranger предоставляет авторизацию для запросов Hive, а также аудит всех операций.

Для настройки интеграции нужно установить плагин Ranger для Hive на каждый узел, где работает HiveServer2, затем задать конфигурацию в файле hive-site.xml:
```xml
<property>
    <name>hive.security.authorization.manager</name>
    <value>org.apache.ranger.authorization.hive.authorizer.RangerHiveAuthorizerFactory</value>
</property>
<property>
    <name>hive.security.authenticator.manager</name>
    <value>org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator</value>
</property>
```

Затем нужно перезапустить HiveServer2 и можно будет создавать политики.

<h4>Управление ключами шифрования</h4>

Apache Ranger и Apache Knox вместе обеспечивают безопасность данных в Hadoop, включая управление ключами шифрования. Ranger отвечает за авторизацию и аудит, а Knox предоставляет шлюз для безопасного доступа и управления ключами.

Управление ключами шифрования:
1. Hadoop KMS: Это сервер для управления ключами шифрования в Hadoop. Он поддерживает создание, хранение и управление ключами, используемыми для шифрования данных в HDFS (например, прозрачное шифрование HDFS).
2. Интеграция Ranger с KMS: Ranger может управлять политиками доступа к операциям KMS (например, создание ключа, удаление, получение). Политики Ranger определяют, кто может выполнять операции с ключами.
3. Создание политик Ranger для KMS: В веб-интерфейсе Ranger выберите сервис KMS. Создайте политики для управления ключами.
4. Использование Knox для безопасного доступа к KMS: Knox выступает в роли прокси для KMS, обеспечивая аутентификацию и авторизацию запросов. Клиенты обращаются к KMS через Knox, а Knox проверяет политики Ranger.

Пример потока запроса на получение ключа через Knox и Ranger:
1. Клиент отправляет запрос на получение ключа в Knox.
2. Knox аутентифицирует клиента (например, с помощью LDAP).
3. Knox обращается к Ranger для проверки авторизации (имеет ли клиент право на доступ к ключу).
4. Если разрешено, Knox перенаправляет запрос в KMS.
5. KMS возвращает ключ через Knox клиенту.

<h3>2. Apache Knox</h3>
<h4>Введение</h4>

Apache Knox — это шлюз безопасности (API Gateway) для экосистемы Apache Hadoop и других современных платформ данных, таких как Spark, Hive, HBase и т.д. Его основная задача — обеспечить единый, безопасный и унифицированный точку входа для всех взаимодействий с кластером Hadoop без необходимости прямого доступа к его узлам.

Основные функции:
- Единая точка доступа (Unified Access Point): Knox предоставляет один URL-адрес (шлюз) для доступа ко всем сервисам кластера. Вместо того чтобы запоминать разные порты и адреса для HDFS, YARN, Hive, пользователи обращаются к одному домену, а Knox перенаправляет запросы к нужному сервису.
- Аутентификация: Проверяет подлинность пользователя. Knox поддерживает множество механизмов: LDAP/Active Directory, SAML, OAuth 2.0 / OpenID Connect, Kerberos, базовую аутентификацию и др.
- Авторизация: Определяет, что может делать аутентифицированный пользователь. Knox использует политики доступа на основе ролей (RBAC), которые можно гибко настраивать.
- Федерация идентификации (Identity Federation): Knox позволяет внешним системам (например, корпоративным порталам или приложениям) использовать свои токены для доступа к сервисам Hadoop. Пользователю не нужно заново вводить логин и пароль.
- Обратный прокси (Reverse Proxy): Knox действует как посредник между клиентами (пользователями, приложениями) и сервисами кластера. Он принимает входящие HTTP-запросы, перенаправляет их к соответствующему внутреннему сервису, а затем возвращает ответ клиенту.
- Аудит и Мониторинг: Knox ведет детальные логи всех входящих запросов (кто, когда, к какому сервису обращался). Это критически важно для соблюдения регуляторных требований (таких как GDPR, HIPAA) и расследования инцидентов безопасности.
- Безопасность на транспортном уровне: Обеспечивает шифрование трафика и поддерживает работу через HTTPS для защиты данных в процессе передачи.

Преимущества использования:
- Упрощение безопасности: Вместо настройки безопасности для каждого сервиса Hadoop по отдельности, можно централизованно настраивать политики в одном месте — в Knox. Это упрощает управление и снижает риск ошибок.
- Сокрытие топологии кластера: Внешние клиенты не знают внутреннего устройства кластера. Они видят только шлюз Knox. Это защищает уязвимые компоненты от прямого воздействия извне.
- Упрощение для клиентов: Разработчикам и аналитикам не нужно разбираться с тонкостями Kerberos или знать внутренние адреса сервисов. Они используют один простой endpoint с стандартными методами аутентификации (например, логин/пароль или токен).
- Поддержка устаревших клиентов: Knox может "обернуть" современные, сложные протоколы аутентификации (как Kerberos) в более простые (как HTTP Basic Auth), позволяя старым приложениям работать с безопасным кластером Hadoop.
- Масштабируемость и отказоустойчивость: Можно развернуть несколько экземпляров Knox за балансировщиком нагрузки для обеспечения высокой доступности и обработки большого числа запросов.
- Интеграция с корпоративными системами: Благодаря поддержке SAML, OAuth и LDAP/AD, Knox легко встраивается в существующую корпоративную инфраструктуру идентификации (Single Sign-On).

<h4>Протоколы безопасности</h4>

Knox обеспечивает безопасность на нескольких уровнях, используя следующие ключевые протоколы и технологии:
- TLS/SSL (Transport Layer Security / Secure Sockets Layer): Шифрование всего трафика между клиентом и шлюзом Knox, а также между Knox и внутренними сервисами (опционально). Knox использует SSL-сертификаты для защиты канала связи, предотвращая перехват и подмену данных (атаки "человек посередине"). Это основа конфиденциальности.
- Протоколы аутентификации:
  - LDAP / Active Directory: Самый распространенный способ. Knox проверяет логин и пароль пользователя против корпоративного LDAP-сервера или AD.
  - Kerberos: Knox может сам аутентифицироваться с помощью Kerberos от имени пользователя для доступа к сервисам Hadoop, которые требуют этого протокола (например, HDFS). Пользователь при этом может использовать более простой метод (например, токен).
  - SAML 2.0 (Security Assertion Markup Language): Позволяет организовать единый вход (SSO) через Identity Provider (IdP), например, Okta или Ping Identity. Пользователь аутентифицируется на стороне IdP, а Knox получает и проверяет SAML-утверждение.
  - OAuth 2.0 / OpenID Connect (OIDC): Стандарт для аутентификации и авторизации через сторонние провайдеры (Google, GitHub, корпоративные IdP). Knox может выступать в роли клиента OAuth, получая и проверяя access token.
- Протоколы авторизации:
  - RBAC (Role-Based Access Control): Это не протокол, а модель. Knox управляет доступом через предопределенные роли (например, ADMIN, USER), которые сопоставляются с пользователями или группами из LDAP/AD. Политики определяют, какая роль имеет доступ к какому URL-пути сервиса.
  - ACL (Access Control Lists): Более детальные списки контроля доступа для управления правами.
- Токены (Tokens): Knox может генерировать и проверять собственные управляющие токены (KnoxSession token), которые используются для поддержания сессии пользователя после успешной аутентификации, избавляя от постоянной переаутентификации.

<h4>Методы аутентификации</h4>

Apache Knox поддерживает богатый набор методов аутентификации, позволяя интегрироваться с практически любой корпоративной системой идентификации. Эти методы настраиваются через так называемые Provider'ы аутентификации в топологии Knox.

Основные методы:
- LDAP / Active Directory: Самый распространенный метод. Knox выступает в роли клиента LDAP. Когда пользователь отправляет логин и пароль (например, через Basic Authentication в заголовке HTTP-запроса), Knox проверяет их, выполняя привязку (bind) к серверу LDAP/AD с этими учетными данными. Идеален для корпоративных сред, где AD является центральным хранилищем пользователей.
- Basic Authentication: Логин и пароль в кодировке Base64 передаются в заголовке каждого HTTP-запроса (Authorization: Basic base64encoded>). Сам по себе небезопасен. Должен обязательно использоваться в паре с TLS/SSL (HTTPS), чтобы учетные данные не перехватывались в открытом виде. Используется для автоматизированных скриптов и API-вызовов, где легко управлять заголовками.
- SAML 2.0: Протокол для единого входа (SSO). Пользователь перенаправляется на страницу Identity Provider (IdP), например, Okta или Azure AD, где проходит аутентификацию. IdP возвращает в Knox криптографически подписанное "утверждение" (SAML Assertion) о том, кто пользователь. Knox выступает в роли поставщика услуг (Service Provider, SP). Используется для интеграции с корпоративными порталами и организации SSO.
- OAuth 2.0 / OpenID Connect (OIDC): Современный протокол на основе токенов. Пользователь авторизуется у провайдера (Google, GitHub, Keycloak, Azure AD), который выдает Knox access token. Knox проверяет этот токен и, при необходимости, получает информацию о пользователе. OAuth 2.0 — это протокол авторизации ("что можно делать"), а OIDC — его надстройка для аутентификации ("кто это"). Используется для веб-приложений, мобильных приложений и предоставления доступа сторонним сервисам без раскрытия паролей.
- Kerberos: Knox может сам получать билет Kerberos для доступа к внутренним сервисам Hadoop (например, HDFS или YARN), которые требуют этого протокола. При этом конечный пользователь может аутентифицироваться любым другим способом (например, через LDAP). Knox действует как "мост" между простой веб-аутентификацией и сложным Kerberos-окружением. Критически важен для безопасных кластеров Hadoop, где сквозной Kerberos является обязательным.
- Сертификаты клиента (mTLS): Вместо пароля клиент (приложение или пользователь) предъявляет Knox свой SSL-сертификат. Knox проверяет его подлинность и валидность против доверенного центра сертификации (CA). Используется для сервис-сервисного взаимодействия (Machine-to-Machine, M2M) с высокими требованиями к безопасности.
- Токен-базисная аутентификация (Knox Tokens): После успешной первичной аутентификации (например, по логину/паролю) Knox может выдать клиенту собственный токен сессии. Этот токен используется в последующих запросах вместо постоянной передачи логина и пароля.

<h4>Проблемы настройки</h4>

Настройка Knox может быть сопряжена с рядом типичных проблем. Вот самые распространенные из них и способы их решения:
- Ошибки SSL/TLS ("Certificate Trust", "SSL Handshake Failure"): Запросы к Knox падают с ошибками PKIX path building failed, unable to find valid certification path to requested target или SSLHandshakeException. Могут возникать, когда Knox или внутренний сервис использует самоподписанный сертификат, не доверенный Knox или клиенту. Возможные решения:
  - Для доступа из браузера: Импортировать сертификат Knox в хранилище доверенных сертификатов ОС или браузера.
  - Для доступа через curl/API: Использовать флаг -k (--insecure) для тестирования или добавьте сертификат CA Knox в truststore Java-приложения/JVM, из которого делается запрос.
  - Для доверия Knox к внутренним сервисам: Добавьте публичные сертификаты всех внутренних сервисов (HDFS, YARN и т.д.) в truststore самого Knox (файл gateway.jks по умолчанию).
- Ошибки аутентификации ("401 Unauthorized", "403 Forbidden"): Пользователь не может войти, получает ошибки 401 или 403. Возможными причинами этому могут быть неправильные логин и пароль или неверная настройка провайдера аутентификации (неправильный URL LDAP, неверная поисковая база DN). Решения:
  - Включить детальное логирование аутентификации в Knox (DEBUG уровень для пакетов org.apache.knox.gateway). В логах будет видно, куда идет запрос к LDAP и почему он отклоняется.
  - Проверить конфигурацию провайдера в файле топологии (topology.xml). Убедитесь, что main.ldapRealm.userDnTemplate или main.ldapRealm.contextFactory.url указаны верно.
  - Проверить членство пользователя в группах и настройки авторизации (роли в shiro.ini или политики в Apache Ranger, если они используются).
- Ошибки маршрутизации ("404 Not Found", "Connection Refused"): Knox возвращает 404 или не может соединиться с внутренним сервисом.
- Возможные причины - опечатка в имени сервиса или URL в определении топологии, внутренний сервис не запущен или слушает на другом порту или межсетевые экраны (firewall) блокируют соединение между хостом Knox и хостом сервиса. Решения:
  - Тщательно проверить файл топологии. Убедитесь, что для сервиса WEBHDFS указан правильный URL, например `http://namenode-host:50070/webhdfs/v1`.
  - Проверить доступность сервиса напрямую, с хоста Knox (`curl -v http://internalservice-host:port`).
  - Проверить правила файервола между хостом Knox и хостами кластера.
- Проблемы с Kerberos-аутентификацией: Knox не может получить билет Kerberos для доступа к сервисам, ошибки типа "GSSException".
- Возможные причины - неправильно сгенерированный или размещенный keytab файл для пользователя Knox, расхождение во времени между хостом Knox и KDC-сервером (Kerberos очень чувствителен к времени), неверно указанное имя principal в конфигурации. Решения:
  - Убедиться, что keytab файл существует в правильном месте и права на чтение есть у пользователя, под которым работает Knox.
  - Настроить синхронизацию времени (NTP) на всех узлах кластера, включая Knox и KDC.
  - Использовать утилиты вроде klist и kinit для проверки возможности получения TGT вручную с хоста Knox.
- Низкая производительность: Запросы через Knox выполняются медленно. Возможные причины - Knox становится "бутылочным горлышком", если развернут в единственном экземпляре, неправильные настройки пула соединений (connection pooling) к внутренним сервисам, отсутствие кэширования результатов аутентификации (например, членства в группах LDAP). Решения:
  - Развернуть несколько экземпляров Knox за балансировщиком нагрузки (например, HAProxy или F5).
  - Настроить параметры пула соединений в топологии Knox.
  - Включить и настройте кэширование для провайдера LDAP.

Логи Knox находятся в папке `$KNOX_HOME/logs/`. При возникновении проблемы стоит обратить внимание на `gateway.log` с включенным DEBUG-уровнем для соответствующего компонента.

<h3>3. Apache Atlas</h3>
<h4>Введение</h4>

Apache Atlas — это платформа с открытым исходным кодом, предназначенная для управления и управления метаданными в экосистеме Hadoop и других системах хранения и обработки данных. Его главная цель — помочь организациям понять, какими данными они владеют, откуда они берутся, как преобразуются и кто их использует.

Ключевые функции и задачи:
- Data Catalog (Каталог данных): Atlas служит централизованным реестром всех активов данных. Он автоматически собирает метаданные из различных источников (например, Hive, Kafka, HBase, Sqoop) и представляет их в удобном для поиска и навигации виде. Пользователи могут быстро найти нужные наборы данных, таблицы, столбцы и т.д.
- Data Lineage (Прослеживаемость данных): Это одна из самых мощных функций Atlas. Она визуально отображает путь данных от источника до потребителя, показывая все преобразования и процессы, через которые прошли данные. Это критически важно для:
  - Отслеживания влияния изменений (Impact Analysis): Понимание того, какие отчёты и модели пострадают, если изменить исходную таблицу.
  - Отладки: Быстрого поиска корня проблемы в данных.
  - Соответствия регуляторным требованиям: Например, для выполнения GDPR или CCPA, чтобы знать, где хранятся персональные данные.
- Data Governance (Управление данными): Atlas предоставляет инструменты для внедрения политик управления данными.
  - Классификация данных: Данные можно помечать тегами (например, `PII` - персональные данные, `SENSITIVE`, `FINANCIAL`).
  - Политики безопасности: Интеграция с Apache Ranger позволяет создавать политики доступа на основе этих классификаций. Например, "только сотрудники отдела аналитики имеют доступ к данным с тегом `SENSITIVE`".
- Поиск и обнаружение данных: Пользователи могут искать данные по их имени, описанию, тегам или другим атрибутам. Например, "найти все таблицы, содержащие email-адреса и отмеченные тегом PII".
- Открытость и расширяемость: Atlas имеет гибкую архитектуру и предоставляет API для интеграции с новыми инструментами и системами, не входящими в "коробочную" поставку.

Apache Atlas предоставляет ряд ключевых преимуществ, которые делают его центральным элементом современной платформы управления данными:
- Единая точка истины для метаданных: Atlas объединяет метаданные из разнородных систем (Hadoop, RDBMS, Kafka и др.) в едином каталоге. Это устраняет разрозненность информации и позволяет получить полную картину о данных в организации.
- Сквозная прослеживаемость данных (End-to-End Data Lineage): Возможность визуально отследить путь данных от исходной системы (например, логов приложений) через все этапы ETL/ELT и вплоть до конечных отчетов и моделей ML. Это критически важно для анализа влияния, отладки и соблюдения регуляторных требований.
- Активное управление данными (Active Data Governance): Atlas не просто пассивно хранит метаданные, а позволяет активно управлять данными через политики. Интеграция с Apache Ranger позволяет автоматически применять правила безопасности на основе классификаций данных (например, "заблокировать доступ к любым данным с тегом PII").
- Повышение продуктивности и самообслуживания (Data Self-Service): Аналитики и ученые по данным могут самостоятельно находить и понимать нужные им наборы данных через мощный поиск и понятные lineage-графы, не тратя время на обращение к инженерам.
- Снижение рисков и обеспечение compliance: Возможность точно знать, где хранятся конфиденциальные данные (ПИИ), как они перемещаются и кто к ним имеет доступ. Это прямое подспорье для выполнения требований GDPR, CCPA, HIPAA и других стандартов.
- Открытость и расширяемость: Будучи проектом с открытым исходным кодом, Atlas не привязывает организацию к вендору. Его богатый REST API позволяет легко интегрировать с любыми проприетарными или кастомными системами, создавая единое пространство метаданных.
- Улучшение качества данных: Понимание происхождения данных и процессов их преобразования помогает быстрее выявлять корневые причины проблем с качеством данных.

<h4>Установка и настройка</h4>

Предварительные требования:
- Hadoop HDFS & YARN: Atlas предназначен для работы в экосистеме Hadoop.
- Apache HBase или Cassandra: Используются как основное хранилище для метаданных Atlas (по умолчанию рекомендуется HBase).
- Apache Solr или Elasticsearch: Используются для индексации и полнотекстового поиска по метаданным (по умолчанию Solr).
- Apache Kafka: Служит шиной событий для асинхронной передачи метаданных от хуков (hooks) к Atlas.
- Java 8 и выше.

Основные шаги установки:
1. Загрузка: Скачайте дистрибутив Apache Atlas с официального сайта.
2. Настройка зависимостей:
  - HBase: Настройте HBase и создайте необходимые таблицы для Atlas с помощью предоставленных скриптов.
  - Solr: Настройте коллекции Solr, которые будет использовать Atlas.
  - Kafka: Убедитесь, что Kafka запущена и доступна.
3. Конфигурация Atlas:
  - Основные настройки находятся в файле `atlas-application.properties`.
  - Необходимо указать пути к HBase, ZooKeeper, Solr и Kafka.
  - Настроить параметры бэкенда хранилища (например, `atlas.graph.storage.hostname` для HBase).
4. Запуск: Запустите сервер Atlas с помощью скрипта `bin/atlas_start.py`.
5. Интеграция с источниками данных (Hooks):
  - Для каждого инструмента (Hive, Spark, Kafka и т.д.) нужно скопировать JAR-файлы хуков Atlas в их classpath и добавить соответствующие настройки в конфигурационные файлы (например, hive-site.xml для Hive).
  - Хуки перехватывают операции (например, создание таблицы в Hive) и отправляют события с метаданными в Kafka, откуда их consumes сервер Atlas.

После запуска веб-интерфейс Atlas будет доступен по умолчанию на `http://<atlas_host>:21000`.

<h4>Типы метаданных и управление ими</h4>

Atlas оперирует метаданными, представленными в виде графа знаний (Knowledge Graph). Это позволяет устанавливать богатые связи между разными объектами.

Ключевые концепции:
- Типы (Types): Это шаблоны или "классы" для метаданных. Они определяют, какие атрибуты могут быть у сущности.
  - Встроенные типы: Atlas поставляется с предопределенными типами для популярных систем: `hive_table`, `hive_column`, `kafka_topic`, `fs_path` и т.д.
  - Пользовательские типы: Вы можете создавать свои собственные типы для описания уникальных для вашей организации объектов данных через REST API или UI.
- Сущности (Entities): Это конкретные экземпляры типов. Например, таблица Hive с именем `sales_fact` — это сущность типа `hive_table`. Каждая сущность имеет:
  - GUID: Глобально уникальный идентификатор.
  - Атрибуты: Определяются её типом (например, `name`, `db`, `owner` для `hive_table`).
  - Связи (Relationships): Связи с другими сущностями (например, таблица `sales_fact` связана с колонками `sales_id`, `amount` и т.д.).
- Классификации (Classifications): Это теги или метки, которые можно прикреплять к сущностям для их категоризации. Классификации могут нести и свою логику.
  - Простой тег: `PII`, `Confidential`.
  - Распространяемый тег (Propagated): Например, тег PII, поставленный на таблицу, автоматически "распространяется" на все её колонки. Если тег поставлен на колонку `email`, он может автоматически появиться на всех таблицах, куда эта колонка попадает через процессы ETL (благодаря Data Lineage).

Пример управления метаданными:
1. Автоматический сбор: Хук Hive перехватывает выполнение DDL-запроса `CREATE TABLE sales (...)`.
2. Создание сущностей: Atlas автоматически создает:
  - Сущность типа `hive_table` с именем `sales`.
  - Несколько сущностей типа `hive_column` (например, `sales_id`, `customer_name`, `amount`).
  - Связи между ними (table -> columns).
3. Ручное/Автоматическое добавление классификаций:
  - Администратор через UI помечает колонку `customer_name` тегом `PII`.
  - Запускается ETL-процесс, который читает из `sales` и пишет в `sales_aggregated`.
4. Обновление графа: Хук Spark перехватывает ETL-задание, и Atlas автоматически строит lineage-связь между `sales` и `sales_aggregated`.
5. Результат: Теперь в интерфейсе Atlas можно найти таблицу `sales_aggregated`, увидеть её полный lineage вплоть до исходной системы, увидеть, что из-за lineage-связи тег `PII` из колонки `customer_name` "подсвечивает" и все последующие процессы, где эти данные используются. Также можно настроить в Apache Ranger политику, которая запретит доступ к любым данным с тегом `PII` для неавторизованных пользователей.

Таким образом, Apache Atlas создает взаимосвязанный граф всех метаданных в организации, что является фундаментом для эффективного управления данными и их безопасностью.

<h4>Интеграция с внешними системами</h4>

Atlas спроектирован как платформа с открытыми API, что позволяет интегрировать его с широким спектром систем за пределами экосистемы Hadoop.

Механизмы интеграции:
- REST API: Основной способ для кастомных интеграций. Позволяет создавать, читать, обновлять и удалять типы, сущности и классификации. Через API можно программно загружать метаданные из любой системы.
- Мessaging (Kafka): Асинхронный механизм для потоковой передачи метаданных. Системы могут отправлять события в формате JSON в определенные топики Kafka, которые Atlas потребляет.
- Atlas Hooks: Специальные плагины, которые встраиваются в другие системы и автоматически перехватывают операции (DDL/DML), отправляя метаданные в Atlas.

Примеры интеграций с внешними системами:
- Реляционные БД (MySQL, PostgreSQL, Oracle): Инструменты: Используются утилиты вроде sqoop-hook или кастомные скрипты, которые с помощью API Atlas регистрируют метаданные о таблицах и схемах после их импорта через Sqoop или другую ETL-утилиту.
- Облачные хранилища (AWS S3, ADLS Gen2): Кастомные скрипты, которые через API регистрируют пути к файлам и папкам (s3_bucket, adls_gen2_path) как сущности в Atlas. Можно связать эти сущности с процессами Spark, которые читают/пишут в эти хранилища.
- BI-системы (Tableau, Power BI): Можно интегрировать через API, чтобы регистрировать дашборды и отчеты как сущности в Atlas. Это позволяет построить lineage не только до таблицы в Hive, но и до конечного отчета, который использует бизнес-пользователь.
- Системы ETL/ELT (dbt, Informatica, Talend): dbt имеет встроенную генерацию документации, которую можно адаптировать для отправки метаданных в Atlas через его API. Для коммерческих ETL-инструментов пишутся кастомные скрипты, которые парсят логи выполнения или метаданные из репозитория и загружают их в Atlas.

<h4>Интеграция с инструментами Hadoop</h4>

Это "родная" территория для Atlas, где интеграция наиболее глубокая и осуществляется через хуки (Hooks). Хук — это JAR-файл, который помещается в `CLASSPATH` целевого сервиса (например, Hive). Когда в этом сервисе происходит операция (создание таблицы, выполнение запроса), хук перехватывает её, извлекает метаданные и отправляет событие в Kafka, откуда его забирает и обрабатывает Atlas.

Ключевые интеграции:
- Apache Hive: Отслеживается создание/удаление баз данных, таблиц, представлений, разделов (partitions). Выполнение запросов (для построения lineage). В Atlas появляются сущности `hive_db`, `hive_table`, `hive_column`, `hive_process` (для запросов). Строится детальный lineage между таблицами, участвующими в `INSERT OVERWRITE TABLE ... SELECT ....`
- Apache Spark: Отслеживаются задания Spark, которые читают и пишут данные в Hive, HDFS или другие источники. Создается сущность `spark_process`, которая связывает входные и выходные потоки данных. Это позволяет включить Spark в общий граф lineage, показывая, как данные преобразуются в сложных ETL-пайплайнах.
- Apache Kafka: Отслеживается создание топиков и их схем (Avro). В Atlas появляются сущности `kafka_topic`. При использовании совместно со Spark Structured Streaming или другими потребителями, можно построить lineage, показывающее, как данные из Kafka топика попадают в HDFS или витрины данных.
- Apache Sqoop: Отслеживаются задания импорта/экспорта данных из реляционных БД в HDFS/Hive. Создается сущность `sqoop_process`, которая связывает таблицу-источник в RDBMS с целевой таблицей в Hive или файлом в HDFS.
- HDFS (через Apache Ranger): Отслеживаются метаданные о пути, размере, владельце файлов. Создаются сущности `fs_path`. Это позволяет классифицировать не только структурированные данные в таблицах, но и сырые файлы в HDFS.

После настройки интеграций с этими инструментами, веб-интерфейс Atlas превращается в мощную карту данных. Вы можете кликнуть на любую таблицу и увидеть граф, который показывает:
- Вверх по течению (Upstream): Откуда пришли данные (исходные файлы, топики Kafka, таблицы из RDBMS).
- Вниз по течению (Downstream): Куда данные уходят (в какие витрины, отчеты, модели ML).

Это создает целостную, прозрачную и управляемую среду данных, где любая информация о данных и их движении доступна в несколько кликов.

<h4>Безопасность метаданных</h4>

Безопасность в Apache Atlas работает на нескольких уровнях, защищая как доступ к самой платформе, так и конфиденциальность самих метаданных:
- Аутентификация (Authentication): Проверить, кто пытается получить доступ к Atlas:
  - Kerberos: Наиболее распространенный и безопасный метод в корпоративных средах Hadoop. Atlas интегрируется с KDC (Key Distribution Center), и пользователи/сервисы проходят аутентификацию с помощью ключевых таблиц (keytabs).
  - LDAP / Active Directory: Позволяет использовать существующие корпоративные учетные записи для входа в UI Atlas и вызовов API.
  - Токены JWT (JSON Web Tokens): Может использоваться для аутентификации между сервисами.
  - HTTP Basic Authentication: Простой метод (логин/пароль), подходит для тестовых сред, но не рекомендуется для продакшена из-за низкой безопасности.
  - Apache Knox: Часто используется как шлюз (Gateway) для обеспечения единой точки входа и аутентификации для всех компонентов Hadoop, включая Atlas.
- Авторизация (Authorization): Определить, что может делать аутентифицированный пользователь:
  - Внутренние роли Atlas: Atlas предоставляет базовые роли для управления доступом к операциям внутри самого каталога:
    - `ADMIN`: Полный доступ ко всем функциям (управление типами, сущностями, импорт/экспорт).
    - `DATA_STEWARD`: Может управлять сущностями и классификациями (тегами), но не может менять типы метаданных.
    - `DATA_SCIENTIST`: Может только просматривать, искать и исследовать метаданные. Не может вносить изменения.
  - Интеграция с Apache Ranger (Ключевой механизм): Ranger используется для тонкой настройки политик доступа к самим метаданным.
    - Вы можете создать политику в Ranger, которая разрешает определенной группе пользователей видеть только сущности с определенным тегом (например, PUBLIC) или, наоборот, запрещает видеть сущности с тегом HIGHLY_CONFIDENTIAL.
    - Политика может разрешать операции только для определенных типов сущностей (например, "Группа analysts может добавлять теги только к сущностям типа hive_table").
- Шифрование (Encryption): Защитить метаданные "на отдыхе" (at rest) и "в движении" (in transit):
  - In Transit: Все коммуникации с Atlas (через UI, API) должны быть защищены с помощью HTTPS (TLS/SSL). Коммуникации между Atlas и его зависимостями (Kafka, HBase) также настраиваются на использование SASL_SSL или других механизмов шифрования.
  - At Rest: Метаданные хранятся в HBase. Шифрование на уровне HBase и HDFS (Transparent Data Encryption - TDE) обеспечивает защиту данных на дисках.

<h4>Высокая доступность и масштабирование</h4>

Высокая доступность в Atlas достигается за счет запуска нескольких экземпляров сервера Atlas. Несколько инстансов Atlas работают за балансировщиком нагрузки (например, HTTP Load Balancer). Они не имеют состояния (stateless) — вся состояние хранится во внешних системах (HBase, Solr). Используется Apache Zookeeper для координации между активными инстансами. Один инстанс становится активным (Active), остальные — горячими резервами (Standby). В случае падения активного инстанса, Zookeeper помогает одному из standby-инстансов быстро стать активным, обеспечивая минимальное время простоя.

Для обеспечения истинной HA, все компоненты, от которых зависит Atlas, также должны быть настроены в режиме высокой доступности:
- HBase (в режиме HA)
- Solr/Elasticsearch (в кластерном режиме, например, SolrCloud)
- Kafka (в кластерном режиме)
- Zookeeper (ансамбль из 3/5 нод)

Поскольку инстансы Atlas не имеют состояния, для обработки увеличения нагрузки можно просто добавить новые инстансы за балансировщиком. Это позволяет масштабировать пропускную способность API и веб-интерфейса. Основная нагрузка по хранению и поиску ложится на HBase и Solr. Масштабирование Atlas подразумевает в первую очередь масштабирование этих бэкендов:
- HBase: Добавление RegionServers для распределения нагрузки хранения и запросов.
- Solr/Elasticsearch: Добавление нод в кластер для распределения индексов и увеличения производительности поиска.

<h4>Политики безопасности и резервное копирование</h4>

Политики безопасности в контексте Atlas — это в первую очередь политики авторизации, которые определяют, "кто и что может делать". Базовые политики создаются в конфигурационных файлах Atlas (для простых сценариев), продвинутые - в Apache Ranger. Ranger предоставляет централизованную консоль для управления политиками.

Примеры политик в Apache Ranger для Atlas:
- Политика на основе классификации (тега):
  - Ресурс: Классификация `PII`
  - Группа пользователей: `data_engineers`
  - Разрешения: `entity-read`, `entity-update`
- Политика на основе типа сущности:
  - Ресурс: Тип `kafka_topic`
  - Группа пользователей: `kafka_admins`
  - Разрешения: `entity-create`, `entity-update`, `entity-delete`
- Политика "Отрицание" (Deny):
  - Ресурс: Классификация `HR_CONFIDENTIAL`
  - Группа пользователей: `contractors`
  - Разрешения: `entity-read` -> Deny

Метаданные в Atlas — это ценный актив, и их потеря недопустима. Стратегия резервного копирования фокусируется на двух ключевых бэкендах:
- Резервное копирование графа метаданных (HBase): Использование встроенных утилит HBase, таких как `hbase snapshot` и `hbase backup`. Создание снапшотов таблиц Atlas в HBase (это происходит почти мгновенно и без простоев), копирование этих снапшотов в удаленное хранилище (например, другой кластер HDFS, облачное хранилище S3/ADLS) с помощью ExportSnapshot. В случае сбоя, снапшоты разворачиваются в новом кластере HBase.
- Резервное копирование индекса поиска (Solr): Использование возможностей SolrCloud по созданию снапшотов индексов и их резервному копированию. Запуск API Solr для создания снапшота указанной коллекции (например, `fulltext_index`), копирование файлов снапшота в удаленное хранилище, восстановление индексов из снапшотов в новом кластере Solr.
- Экспорт/Импорт через API Atlas: Использование встроенных REST API endpoints (`/api/atlas/admin/export` и `/api/atlas/admin/import`). Позволяет сделать логический бэкап, который можно перенести между кластерами с разными версиями HBase или даже в другую версию Atlas (с осторожностью). Может быть медленным для очень больших объемов метаданных и не обеспечивает point-in-time восстановление, как снапшоты HBase.

<h4>Оптимизация производительности и управление версиями</h4>

Оптимизация производительности:
- Прослеживаемость данных (Data Lineage): Это ключевая функция Atlas. Она визуализирует полный путь данных: откуда они попали в систему, какие преобразования (Hive, Spark) проходили и куда были загружены.
- Использование для оптимизации: С помощью Lineage можно определить влияние изменений, удалить неиспользуемые данные и оптимизировать конвейеры данных.
- Управление политиками (Policies): Atlas позволяет задавать политики доступа и использования данных. Хотя это напрямую не ускоряет запросы, это предотвращает выполнение неавторизованных и потенциально тяжелых задач, которые могут негативно повлиять на производительность всего кластера.

Управление версиями:
- Atlas отслеживает изменения в структуре данных. Например, если в таблице Hive была добавлена или удалена колонка, Atlas сохранит обе версии этой таблицы как сущности.
- Это позволяет построить полную историю изменений схемы данных. Вы можете увидеть, как выглядела таблица неделю назад, кто и когда изменил её структуру. Это критически важно для аудита и понимания эволюции данных.

<h3>4. Apache Ambari</h3>
<h4>Введение</h4>

Apache Ambari — это программное решение с открытым исходным кодом, предназначенное для управления и мониторинга кластеров Apache Hadoop. Проще говоря, это веб-инструмент с графическим интерфейсом, который значительно упрощает жизнь администраторам и инженерам данных, работающим с Hadoop-экосистемой. Его ключевая ценность — в централизации управления сложными распределенными системами.

Основные задачи Ambari можно разделить на несколько категорий:
- Provisioning и Управление Кластером: Ambari автоматизирует процесс установки компонентов Hadoop (HDFS, YARN, Hive, HBase, Spark, Kafka и многих других) на множество узлов, позволяет изменить конфигурационные файлы (например, `core-site.xml`, `hdfs-site.xml`) для всех узлов кластера из одного веб-интерфейса. Ambari сам позаботится о распространении этих файлов и перезапуске необходимых служб. Легко запускать, останавливать и перезапускать все службы кластера (например, NameNode, ResourceManager, HiveServer2).
- Мониторинг и оперативное управление: Ambari в реальном времени показывает "здоровье" каждой службы и каждого узла (зеленый, желтый, красный индикаторы), собирает сотни метрик по использованию CPU, памяти, дискового пространства, сети, а также специфические метрики Hadoop (например, использование HDFS, загрузка YARN, латенси HBase). Есть настраиваемые уведомления (по email, SNMP) о критических событиях, таких как отказ узла, нехватка дискового пространства или сбой службы.
- Безопасность: Ambari может автоматизировать сложный процесс включения и управления аутентификацией Kerberos для всего кластера, позволяет назначать пользователям и группам разные уровни доступа к управлению кластером (только просмотр, администрирование и т.д.).
- Интеграция с другими инструментами: Весь функционал Ambari доступен через REST API, что позволяет автоматизировать задачи управления и интегрировать Ambari с другими системами. Ambari использует концепцию "стеков" (например, HDP — Hortonworks Data Platform), что позволяет управлять версиями компонентов и их зависимостями.

Использование Ambari предоставляет ряд существенных преимуществ, особенно по сравнению с ручным управлением кластером:
- Резкое снижение сложности (Simplification): Вместо того чтобы вручную подключаться к десяткам или сотням узлов через SSH и редактировать конфигурационные файлы, администратор делает всё через один веб-интерфейс или REST API. Такие операции, как добавление нового узла в кластер, масштабирование служб или обновление конфигураций, превращаются из многочасовых рутинных процедур в несколько кликов.
- Повышение надёжности и стабильности: Возможность в реальном времени видеть состояние всего кластера и каждой службы в нем позволяет быстро обнаруживать и устранять проблемы до того, как они приведут к простою. Ambari гарантирует, что конфигурационные файлы одинаковы на всех узлах кластера, что исключает множество ошибок, вызванных "дрейфом конфигураций".
- Ускорение развертывания и ввода в эксплуатацию: Полностью функциональный Hadoop-кластер можно развернуть за часы, а не дни или недели, конфигурации кластера можно сохранять и воспроизводить, что критически важно для создания идентичных сред для разработки, тестирования и производства.
- Экономия ресурсов и снижение TCO (Total Cost of Ownership): Ambari позволяет администраторам среднего уровня эффективно управлять сложными кластерами, которые в противном случае требовали бы глубоких знаний о каждом компоненте Hadoop. Администраторы тратят меньше времени на "тушение пожаров" и рутинные задачи, освобождая время для стратегических улучшений и оптимизации.
- Открытость и интегрируемость: Открытый исходный код и REST API позволяет интегрировать Ambari с системами мониторинга предприятия (например, Grafana через дополнительные прокси), системами автоматизации (Ansible, Chef) и платформами CI/CD.

<h4>Установка и настройка</h4>

Настройка окружения:
1. Отключить SELinux или настроить его в разрешающем режиме.
2. Настроить фаервол или отключить его.
3. Настроить беспарольный SSH-доступ с Ambari-сервера на все остальные узлы кластера.
4. Установить NTP для синхронизации времени на всех узлах (критически важно для распределенных систем).
5. Убедиться, что на всех узлах разрешены DNS-запросы (прямые и обратные).

Основные шаги установки:
1. Установка репозиториев: На сервере, который будет Ambari-сервером, нужно добавить репозиторий Ambari (ранее это были репозитории HDP, сейчас, после слияния Hortonworks и Cloudera, используется репозиторий CDP — Cloudera Data Platform).
2. Установка Ambari Server: Установить пакеты ambari-server с помощью менеджера пакетов (yum или apt). Выполнить команду настройки: `ambari-server setup`. Этот скрипт задаст вопросы о настройке базы данных (встроенная PostgreSQL или внешняя, например, MySQL/PostgreSQL), учетной записи и т.д. Запустить сервер: `ambari-server start`.
3. Запуск мастера установки через Web UI: Открыть в браузере `http://<ambari-server-hostname>:8080`, Логин/пароль по умолчанию: `admin` / `admin`.
4. Создание кластера: Дать имя вашему кластеру, выбрать версию стека (например, HDP 3.1), вписать hostname всех узлов, которые будут входить в кластер, указать приватный SSH-ключ, чтобы Ambari Server мог подключиться к узлам и установить ПО. Ambari проверит доступность узлов и выполнит предварительные проверки (отключенный SELinux, NTP и пр.).
5. Выбор служб: Выбрать, какие компоненты Hadoop вы хотите установить (HDFS, YARN, MapReduce2, Hive, Tez, ZooKeeper и т.д.).
6. Назначение ролей: Распределить Master-компоненты (NameNode, ResourceManager) и Slave-компоненты (DataNode, NodeManager) по узлам.
7. Настройка конфигураций: Задать основные параметры, такие как пароли для баз данных (например, для Hive), директории для данных HDFS и логов.
8. Запуск и установка: Ambari покажет сводную информацию, и после подтверждения начнется процесс установки пакетов, настройки конфигураций и запуска служб. Весь процесс можно наблюдать в реальном времени.

<h4>Визуализация и мониторинг данных</h4>

Ambari предоставляет богатый и интуитивно понятный интерфейс для визуализации состояния кластера:
- Дашборды служб: Для каждой службы (HDFS, YARN, HBase и т.д.) есть свой детализированный дашборд.
  - Пример для HDFS: Показывает общий объем HDFS, оставшееся пространство, количество Live/Dead DataNodes, количество файлов и блоков.
  - Пример для YARN: Показывает загрузку кластера (использованная/доступная память и vCores), количество запущенных/завершенных приложений, активные пользователи.
- Визуализация метрик в реальном времени: Ambari автоматически строит графики и тепловые карты по ключевым метрикам (CPU usage, network I/O, JVM heap usage, HDFS bytes read/written).
- Мониторинг приложений и заданий: Через Ambari можно получить быстрый доступ к UI компонентов. Например, кликнув на ссылку, можно перейти в интерфейс YARN ResourceManager, чтобы посмотреть детали запущенных задач (Spark, MapReduce). Аналогично, есть ссылки на UI HBase, NameNode UI и другие.
- Система оповещений: В интерфейсе есть раздел "Alerts", где отображается история всех предупреждений. Можно настроить пороги срабатывания для разных метрик (например, "отправить email, если свободное место в HDFS упало ниже 10%").
- Журналы (Logs): Ambari предоставляет удобный доступ к логам всех служб со всех узлов кластера прямо из веб-интерфейса. Это избавляет от необходимости вручную подключаться к каждому серверу для поиска логов.

<h4>Управление ресурсами и конфигурацией</h4>

Это одна из сильнейших сторон Ambari. Управление осуществляется на нескольких уровнях:
- Жизненный цикл конфигураций:
  - Версионирование: При каждом изменении конфигурации (например, параметров HDFS) Ambari сохраняет новую версию. Это позволяет в любой момент увидеть, что было изменено, кем и когда, а при необходимости — легко откатиться к предыдущей, стабильной версии.
  - Групповое применение: Изменение, внесенное в конфигурацию службы (например, увеличение размера кучи для NameNode), автоматически применяется ко всем узлам, где эта служба работает.
  - Предпросмотр и распространение: После внесения изменений Ambari показывает, на каких конкретно узлах файлы будут изменены, и запрашивает подтверждение. После подтверждения он распространяет конфиги и указывает, какие службы требуют перезапуска.
- Уровни конфигурации: Ambari работает с иерархией конфигураций, что обеспечивает гибкость:
  - Уровень кластера: Настройки по умолчанию для всех служб.
  - Уровень службы: Настройки, специфичные для конкретной службы (например, Hive).
  - Уровень хоста: Уникальные настройки для отдельного узла (редко используется, но позволяет решать специфичные для хоста проблемы).
- Управление ресурсами кластера:
  - Мониторинг аппаратных ресурсов: Ambari собирает и отображает метрики использования CPU, оперативной памяти, дискового I/O и сети для каждого узла.
  - Управление ресурсами YARN: Через Ambari можно настраивать очереди и политики планировщика YARN Capacity Scheduler или Fair Scheduler, распределяя вычислительные ресурсы (vCores, Memory) между пользователями и отделами.
  - Управление хранилищем HDFS: Ambari предоставляет инструменты для мониторинка использования дискового пространства, отслеживания "самых прожорливых" пользователей и планирования балансировки данных между DataNodes.
- Динамические конфигурации: Некоторые параметры могут быть изменены "на лету" (без перезапуска службы), что минимизирует влияние на производственные процессы.

<h4>Автоматизация и диагностика задач</h4>

Ambari не только показывает проблемы, но и активно помогает их решать:
- Автоматизация операционных задач: Для каждой службы доступен набор предопределенных действий. Например:
  - HDFS: "Включить/Выключить Safe Mode", "Rebalance HDFS" (балансировка данных), "Удалить узел данных" (Decommission).
  - YARN: "Обновить узлы" (при добавлении новых NodeManager).
  - Hive: "Обновить Hive Metastore".
- Мастеры (Wizards) для сложных операций:
  - Добавление нового узла: Специальный мастер проводит администратора через весь процесс добавления нового DataNode или NodeManager в кластер, включая установку ПО, настройку и запуск служб.
  - Включение безопасности Kerberos: Это чрезвычайно сложная ручная процедура, которую Ambari автоматизирует, предоставляя пошагового мастера для настройки KDC, генерации keytab'ов и распространения их по узлам.
- Диагностика и устранение неисправностей: При обнаружении проблемы часто предоставляет конкретную рекомендацию по ее устранению (например, "DataNode на узле `dn07.example.com` недоступен. Проверьте сетевое подключение и состояние службы").
  - Панель оповещений (Alerts Dashboard): Централизованное место, где видны все текущие проблемы. Каждое оповещение имеет уровень серьезности (CRITICAL, WARNING) и описание.
  - Рекомендации и проверки здоровья (Health Checks): Ambari постоянно выполняет проверяет доступность и работоспособность служб, следит за дисковым пространством, проверяет корректность работы JournalNodes в HDFS и мониторит задержки в работе ZooKeeper.
- Глубокая интеграция с логированием: Интерфейс позволяет искать по ключевым словам в логах всех служб со всех узлов. Можно фильтровать по уровню лога (ERROR, WARN, INFO). При клике на оповещение система часто сразу перенаправляет администратора в раздел логов соответствующей службы, где можно увидеть детали ошибки, что ускоряет диагностику.
- Автоматическое восстановление (Ambari Metrics & Alerts with Automated Actions): Хотя полноценное автоматическое исправление (например, перезапуск службы при сбое) в стандартной поставке ограничено, эту функциональность можно реализовать через интеграцию Ambari REST API с внешними системами оркестрации, такими как Ansible или через кастомные скрипты, которые будут реагировать на оповещения.

<h4>Отказоустойчивость и восстановление</h4>

Ambari сам по себе не является системой репликации данных или координатором отказоустойчивости. Его роль — значительно упрощать развертывание, мониторинг и управление этими механизмами, которые встроены в компоненты Hadoop:
- Автоматизация настройки высокой доступности: Ручная настройка HA для таких критических компонентов, как NameNode в HDFS или ResourceManager в YARN, — очень сложная процедура. Ambari предоставляет мастера, которые автоматизируют этот процесс:
  - HDFS NameNode HA: Ambari настраивает архитектуру на основе Quorum Journal Manager (QJM), запускает необходимые JournalNodes и ZKFailoverController (ZKFC) на нужных узлах.
  - YARN ResourceManager HA: Аналогично, Ambari настраивает активный и резервный ResourceManager и интегрирует их с ZooKeeper для автоматического переключения при отказе.
- Мониторинг состояния высокой доступности: В интерфейсе Ambari наглядно отображается, какой из NameNode или ResourceManager находится в активном состоянии, а какой — в резервном. Система оповещений сразу укажет на проблему, если, например, ZKFC перестанет работать.
- Восстановление после сбоев:
  - Автоматический перезапуск служб: Ambari Agent на каждом узле постоянно мониторит состояние запущенных служб. Если какая-либо служба (например, DataNode или NodeManager) аварийно завершает работу, Agent может автоматически попытаться её перезапустить.
  - Восстановление данных в HDFS: Сам процесс восстановления данных — это функция HDFS. HDFS автоматически реплицирует блоки данных между разными DataNodes. Если один DataNode выходит из строя, HDFS обнаруживает недостающие реплики и автоматически копирует их на другие исправные узлы до достижения заданного фактора репликации (по умолчанию 3). Роль Ambari здесь — показать администратору, что этот процесс происходит: отслеживание количества "под угрозой" (Under-Replicated) блоков, визуализация состояния всех DataNodes, оповещение администратора о критически низком количестве живых DataNodes или о проблемах с дисковым пространством, которые могут помешать процессу восстановления.
- Восстановление состояния кластера (Disaster Recovery):
  - Резервное копирование конфигураций: Ambari позволяет легко экспортировать все конфигурации кластера ("Blueprint"). Это позволяет быстро восстановить идентичный кластер в случае полного отказа.
  - Интеграция с снапшотами HDFS: Ambari может запускать действия для создания снапшотов HDFS критически важных каталогов, что является частью стратегии DR.

<h4>Оптимизация производительности и управление версиями</h4>

Управление версиями:
- Версионирование конфигураций: Каждое изменение любого конфигурационного файла (например, hive-site.xml) сохраняется в Ambari с номером версии, временем и автором. Это позволяет легко отследить, какое изменение привело к проблеме, и мгновенно откатиться к предыдущей, рабочей версии.
- Управление версиями ПО (Stacks): Ambari позволяет управлять целыми стеками программного обеспечения (HDP, CDP). Администратор может обновлять версии компонентов (например, перейти с Hive 2.x на Hive 3.x) через интерфейс Ambari, который управляет зависимостями и последовательностью обновления.

Оптимизация производительности:
- Мониторинг ресурсов: Ambari предоставляет детальные метрики по использованию CPU, памяти, диска и сети. Администратор может выявлять "узкие места" (bottlenecks) на уровне оборудования.
- Настройка конфигураций: На основе метрик можно оптимизировать производительность, изменяя параметры в Ambari. Например, увеличить размер куч JVM для NameNode или настроить параметры исполнителей (executors) для Spark.
- Анализ выполнения задач: Через ссылки на UI YARN ResourceManager и Spark History Server, доступные из Ambari, можно анализировать медленные задачи, находить данные-скивья (data skew) и оптимизировать код приложений.

<h3>5. Apache Hue</h3>
<h4>Введение</h4>

Apache Hue (Hadoop User Experience) — это открытый веб-интерфейс, который делает экосистему Hadoop и работу с большими данными простой и доступной. Его главная цель — позволить конечным пользователям (аналитикам, дата-инженерам, бизнес-пользователям) взаимодействовать с данными в Hadoop без необходимости написания сложного кода или использования командной строки.

Hue — это не один инструмент, а сборник множества "приложений" (apps) в одном интерфейсе. Вот его ключевые функции:
- Редактор запросов (Query Editors):
  - Hive Editor: Визуальный редактор для написания и выполнения HiveQL-запросов. Позволяет просматривать метаданные (базы данных, таблицы), сохранять часто используемые запросы и визуализировать результаты в виде графиков и таблиц.
  - Impala Editor: Аналогичный редактор для Apache Impala, обеспечивающий выполнение SQL-запросов с низкой задержкой прямо в реальном времени.
  - Spark Editor: Поддержка написания кода на Scala, Python (PySpark) и SQL для Apache Spark.
  - Pig Editor: Редактор для скриптов Apache Pig.
  - DBQuery (JDBC Editor): Позволяет подключаться к любым реляционным СУБД (MySQL, PostgreSQL, Oracle и др.) через JDBC и выполнять в них SQL-запросы.
- Работа с файлами (File Browser): Графический интерфейс для просмотра и управления файлами и каталогами в HDFS и облачных хранилищах (например, S3). Позволяет загружать, скачивать, удалять, переименовывать файлы и менять их разрешения, аналогично проводнику в Windows или Finder в macOS.
- Работа с базами данных (Data Browsers): Позволяет просматривать метастанции Hive и Impala (базы данных, таблицы, столбцы), изучать схемы таблиц и выполнять предварительный просмотр данных.
- Планировщик рабочих процессов (Workflow Scheduler & Dashboard): Интеграция с Apache Oozie. Позволяет визуально создавать, планировать и мониторить сложные рабочие процессы (workflows) и координаторы Oozie, которые могут включать в себя задачи Hive, Pig, Spark, Java и другие.
- Инструменты для работы с метаданными и поиска:
  - Apache Atlas: Интеграция для просмотра и отслеживания происхождения данных (data lineage).
  - Apache Solr Search: Встроенный интерфейс для индексации и полнотекстового поиска по данным с помощью Apache Solr.
- Безопасность и аутентификация: Поддерживает аутентификацию через LDAP/AD, PAM, OAuth и другие механизмы. Интегрируется с системами авторизации Hadoop (как Apache Ranger), обеспечивая безопасный доступ на основе ролей (RBAC).
- Визуализация данных (Dashboards): Позволяет создавать интерактивные дашборды и отчеты на основе результатов SQL-запросов. Пользователи могут строить различные графики (линейные, столбчатые, круговые) без необходимости использовать отдельные BI-инструменты.

Apache Hue предоставляет интерфейсы для работы с различными типами данных, хранящихся в Hadoop и других системах. Вот основные из них:
- Структурированные данные: через редакторы Hive и Impala, Hue позволяет писать SQL-запросы к данным, хранящимся в HDFS в структурированном виде (например, таблицы Hive в формате ORC, Parquet, Avro и т.д.). Также поддерживаются подключения к реляционным базам данных через JDBC.
- Полуструктурированные данные: Hue может работать с полуструктурированными данными, такими как JSON, XML, CSV, с помощью тех же SQL-инструментов (Hive, Impala), если для них задана схема. Также файловый браузер позволяет просматривать raw-файлы.
- Неструктурированные данные: через файловый браузер пользователи могут просматривать и управлять файлами в HDFS (текстовые файлы, логи, изображения и т.д.). Однако для анализа неструктурированных данных обычно требуется их обработка с помощью инструментов, таких как Spark или Pig, редакторы для которых также есть в Hue.
- Индексированные данные (поиск): с помощью интеграции с Solr Hue позволяет выполнять полнотекстовый поиск по индексированным данным.
- Метаданные: Hue интегрируется с Apache Atlas для просмотра метаданных и отслеживания происхождения данных.
- Данные в реальном времени: Hue не является системой реального времени, но может использоваться для запросов к данным, которые обновляются периодически, через Impala (которые могут работать с данными, обновляемыми в near-real-time) или Spark Streaming (через редактор Spark).

<h4>Применение в Hadoop</h4>

Hue играет роль универсального шлюза и рабочего центра для различных пользователей внутри экосистемы Hadoop. Роли и сценарии использования:
- Для аналитиков и специалистов по Data Science: Быстро проанализировать данные, провести A/B тестирование, построить отчет для менеджмента. Аналитик заходит в Hue, открывает редактор Hive или Impala, пишет SQL-запрос для агрегации данных, получает результат и сразу же строит на его основе график для дашборда. Всё происходит в одном окне браузера без переключения между инструментами.
- Для дата-Инженеров и разработчиков: Разработать, отладить и запустить ETL-процесс (Extract, Transform, Load). Применение Hue:
  - Использование редакторов Hive, Spark, Pig для отладки отдельных скриптов.
  - С помощью Файлового браузера проверить наличие и качество исходных данных в HDFS.
  - С помощью Планировщика рабочих процессов (Oozie) собрать из отлаженных скриптов единый пайплайн, настроить его расписание и мониторить выполнение.
- Для администраторов: Быстро проверить состояние кластера, найти проблему, предоставить доступ пользователю. Администратор может через Hue проверить занятое место в HDFS, просмотреть логи выполненных задач, управлять доступом к файлам и базам данных.
- Для бизнес-Пользователей (с технической подготовкой): Регулярно генерировать стандартные отчеты из готовых таблиц. Пользователь может открыть сохраненный SQL-запрос, выполнить его и обновить данные на дашборде, не обращаясь к отделу аналитики.

Как Hue интегрируется в экосистему:
- HDFS / S3: Hue использует File Browser для прямого взаимодействия с хранилищем.
- Hive/Impala: Редакторы запросов отправляют SQL непосредственно на соответствующие сервисы.
- Spark: Отправляет задания через Livy Server.
- Oozie: Визуальный интерфейс для создания XML-файлов workflow и координаторов.
- YARN: Может показывать статусы приложений через интеграцию с ResourceManager.
- Ranger / LDAP: Для управления безопасностью и аутентификацией.

<h4>Настройка для работы с Hadoop</h4>

Настройка Hue включает в себя несколько шагов:
1. Установка: Hue может быть установлен из пакетов (например, RPM или DEB) или собран из исходного кода. Также он часто поставляется как часть дистрибутивов Hadoop (Cloudera, Hortonworks).
2. Конфигурация: основной файл конфигурации - hue.ini. В нем настраиваются:
  - База данных: Hue по умолчанию использует SQLite, но для production рекомендуется использовать MySQL или PostgreSQL.
  - Настройки безопасности: аутентификация (LDAP, PAM, OAuth и др.), SSL.
  - Интеграция с Hadoop:
    - HDFS: указание адреса NameNode и порта, настройки безопасности (Kerberos, если используется).
    - YARN: указание адреса ResourceManager.
    - Hive/Impala: указание адресов серверов HiveServer2 и Impala Daemon.
    - Oozie: указание адреса Oozie сервера.
    - Solr: указание адреса Solr сервера.
  - Другие сервисы, такие как Spark, HBase, и т.д.
3. Запуск: после настройки Hue запускается как демон (служба). В зависимости от способа установки, это может быть команда systemctl start hue или запуск скриптом.
4. Проверка: после запуска можно открыть веб-интерфейс по умолчанию на порту 8888 и проверить подключение к различным сервисам.

<h4>Интеграция с HDFS и YARN</h4>

Hue предоставляет полнофункциональный веб-интерфейс File Browser для работы с HDFS:
- Навигация по файловой системе: Древовидное представление каталогов HDFS
- Операции с файлами: Загрузка/скачивание файлов через браузер, создание, переименование, копирование, перемещение и удаление папок и файлов
- Управление разрешениями: изменение владельца файлов, настройка прав доступа (chmod), установка ACL (Access Control Lists)
- Просмотр файлов: Текстовые файлы с подсветкой синтаксиса, предпросмотр бинарных файлов, автоматическое определение кодировки

Техническая реализация:
- Использует WebHDFS REST API или HttpFS для взаимодействия с HDFS
- Поддерживает все операции файловой системы через HTTP-интерфейс
- Интегрируется с системами безопасности (Kerberos, Ranger)

Hue предоставляет мощный интерфейс для мониторинга и управления заданиями YARN:
- Мониторинг приложений: Просмотр запущенных, завершенных и failed заданий, фильтрация по пользователю, очереди, типу приложения, детальная информация о каждом приложении
- Анализ производительности: Использование ресурсов (CPU, Memory, Containers), время выполнения заданий, статусы мапперов и редьюсеров
- Просмотр логов: доступ к логам Application Master, логи контейнеров напрямую из интерфейса, поиск по логам

Поддерживаемые типы приложений:
- MapReduce: Классические MR задания
- Spark: Приложения Spark через Spark History Server
- Tez: DAG-исполнение для Hive и Pig
- Другие: Произвольные YARN-приложения

<h4>Настройка безопасности</h4>

Безопасность в Hue — это комплексный подход, который включает в себя аутентификацию пользователей, авторизацию действий и защиту данных при передаче. Основные аспекты:
- Интеграция с безопасностью Hadoop: Hue не управляет правами доступа к данным самостоятельно, а выступает в роли клиента и посредника. Все запросы к HDFS, Hive и другим сервисам выполняются от имени авторизованного пользователя Hue.
- Kerberos: Если кластер Hadoop защищен с помощью Kerberos (что является стандартом в корпоративной среде), Hue должен быть настроен для работы с ним. Это означает:
  - Создание `keytab`-файла для пользователя Hue (hue).
  - Настройка Hue для использования этого `keytab` для получения Kerberos-билетов (обычно через `kinit` или встроенную поддержку).
  - Это позволяет Hue (выдавать себя за конечного пользователя при обращении к защищенным сервисам.
- SSL/TLS: Для шифрования трафика между браузером пользователя и сервером Hue необходимо настраивать SSL. Это критически важно для защиты учетных данных и данных.
- Конфиденциальность конфигурации: Файл конфигурации hue.ini может содержать пароли и ключи (например, для базы данных Hue). Необходимо ограничивать права доступа к этому файлу.

Основные принципы безопасности Hue:
- Пользователь аутентифицируется в Hue.
- Hue проверяет его права на выполнение действий через механизмы авторизации (например, Sentry или Ranger в старых версиях, сейчас чаще через Ranger или напрямую в HDFS ACLs).
- При обращении к сервисам Hadoop (например, к HDFS), Hue передает запрос от имени этого пользователя, используя Kerberos-аутентификацию.

<h4>Механизмы аутентификации</h4>

Hue поддерживает множество гибких механизмов аутентификации, которые настраиваются в секции `[[auth]]` файла `hue.ini`. Основные механизмы:
- Backend по умолчанию (`desktop.auth.backend.AllowFirstUserDjangoBackend`): Первый зарегистрировавшийся пользователь становится администратором Hue. Простая схема, подходит для тестовых сред, но не для продакшена.
- LDAP / Active Directory: Наиболее распространенный механизм в корпоративных средах. Hue выступает в роли LDAP-клиента и проверяет логин/пароль пользователя против корпоративного каталога (например, Microsoft Active Directory или OpenLDAP). Требуется указать URL LDAP-сервера, DN (Distinguished Name) для пользователя, привязки, а также настройки для синхронизации групп пользователей. Позволяет использовать единые учетные записи для доступа ко всей инфраструктуре.
- PAM (Pluggable Authentication Modules): Аутентификация через системные модули PAM на хосте, где работает Hue. Может быть полезно для интеграции с локальными учетными записями ОС.
- SAML (SAML 2.0): Поддержка федеративной аутентификации через поставщиков удостоверений (Identity Provider, IdP) таких как Okta, Auth0, Microsoft ADFS. Позволяет использовать Single Sign-On (SSO) для доступа к Hue.
- OAuth: Поддержка протокола OAuth 2.0 для аутентификации через внешние провайдеры, такие как Google, Facebook, GitHub и др. Часто используется для публичных или гибридных сред.
- Списковый Backend (`desktop.auth.backend.ListAuthBackend`): Создается статический "белый список" пользователей, которым разрешен вход. Не безопасен для реального использования.
- RemotingUserBackend: Используется в связке с Hadoop's HTTP Authentication (например, при использовании Knox Gateway).

<h4>Конфигурация для работы с несколькими кластерами Hadoop</h4>

По своей сути, один экземпляр Hue предназначен для работы с одним кластером Hadoop. Он конфигурируется путем указания хостов и портов для каждого из сервисов (HDFS NameNode, YARN ResourceManager, HiveServer2 и т.д.) в одном `hue.ini`. Однако есть стратегии для организации работы с несколькими кластерами:
- Запуск нескольких экземпляров Hue (Наиболее чистое решение): Для каждого кластера Hadoop развертывается и настраивается свой собственный, полностью независимый экземпляр Hue. Каждый экземпляр Hue имеет свой собственный файл `hue.ini`, в котором прописаны контакты только для сервисов своего кластера. Каждый экземпляр работает на своем собственном порту.
- Использование Load Balancer / Reverse Proxy (например, Apache HTTP Server, Nginx): Запускаются несколько экземпляров Hue (как в подходе 1), но пользователи обращаются к единому URL.Прокси (например, Nginx) стоит перед всеми экземплярами Hue. На основе URL (например, `/cluster1` -> Hue instance 1, `/cluster2` -> Hue instance 2) или поддомена (например, `cluster1.hue.company.com`, `cluster2.hue.company.com`) прокси перенаправляет запросы на соответствующий бэкенд-экземпляр Hue.
- Ограниченная поддержка "High Availability" в Hue: В конфигурации Hue можно указать несколько серверов для одного типа сервиса (например, несколько HDFS NameNode в режиме HA). Но это не означает работу с разными независимыми кластерами. Это механизм отказоустойчивости внутри одного кластера.

Пример для HDFS:
```bash
# В hue.ini
[hadoop]
  [[hdfs_clusters]]
    [[[default]]]
      # Адреса NameNode в одном кластере (HA)
      fs_defaultfs=hdfs://my-ha-nameservice
      logical_name=my-ha-nameservice
      webhdfs_url=http://namenode01:50070/webhdfs/v1
      webhdfs_url=http://namenode02:50070/webhdfs/v1
```