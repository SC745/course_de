<h2>HDFS</h2>
<h3>1. Основные концепции и архитектура</h3>

HDFS (Hadoop Distributed File System), является ключевым компонентом экосистемы Hadoop, предназначенным для хранения больших объемов данных на нескольких машинах в распределенном порядке. HDFS обеспечивает высокую пропускную способность к данным, обладает высокой устойчивостью к отказам и предназначен для развертывания на недорогом оборудовании. Она разбивает большие файлы на меньшие блоки (размером по умолчанию 128MB или 256MB, однако этот параметр можно перенастроить) и распределяет их по нескольким узлам в кластере. Это позволяет эффективно обрабатывать большие наборы данных с помощью программной модели MapReduce.

Архитектура:
- NameNode - управляющий узел, который хранит метаданные всей файловой системы. NameNode отслеживает структуру каталогов и метаданные для всех файлов, включая информацию о том, какие блоки данных составляют каждый файл и на каких DataNodes эти блоки расположены. Также регулирует доступ клиентов к файлам и отслеживает работоспособность DataNodes.
- DataNode - узлы хранящие фактические данные и обслуживающие запросы на чтение и запись от NameNode. В HDFS файл разбивается на один или несколько блоков, и каждый блок копируется на несколько DataNodes в соответствии с заданным фактором репликации для обеспечения отказоустойчивости.
- Secondary NameNode - несмотря на свое название, не является резервным узлом для NameNode. Его основная функция — периодически сливать изменения журнала NameNode с его текущим состоянием в файловой системе, помогая уменьшить размер журнала и предотвращая потерю данных.

Основные концепции:
- Блочное хранение: Основной единицей хранения в HDFS является "блок". Когда файл сохраняется в HDFS, он разделяется на один или несколько блоков. Эти блоки хранятся на узлах DataNodes.
- DataNodes и NameNode: Узлы DataNodes отвечают за хранение и извлечение блоков по запросу. NameNode хранит информацию о дереве файловой системы и метаданные всех файлов и директорий. Эти метаданные включают информацию, такую как расположение блоков, хранящихся на DataNodes.
- Распределение данных и согласованность: HDFS разработан для работы с большими файлами. Он распределяет данные по нескольким узлам для обеспечения высокой доступности и устойчивости данных. При хранении данных в HDFS они автоматически реплицируются на узлах DataNodes в соответствии с коэффициентом репликации, обеспечивая наличие копии данных даже в случае сбоя оборудования.

Типы данных:
- Текстовые форматы (CSV, TSV, JSON, лог-файлы)
- Последовательные файлы (Sequence Files)
- Колоночные форматы (Parquet, ORC)
- Форматы сериализации (Avro, Protocol Buffers, Thrift)
- Специализированные форматы (RCFile)

<h4>Чтение и запись</h4>

Процесс чтения:
1. Клиент запрашивает у NameNode расположение блоков файла
2. NameNode возвращает список DataNode, содержащих каждый блок
3. Клиент обращается напрямую к DataNode за блоками
4. DataNode передают данные клиенту
5. Если DataNode недоступен, клиент запрашивает блок у реплики

Процесс записи:
1. Клиент запрашивает у NameNode создание нового файла
2. NameNode проверяет права и возвращает список DataNode для записи
3. Клиент начинает передачу данных первому DataNode в конвейере
4. DataNode передает данные следующему DataNode в цепочке
5. После записи всех блоков клиент сообщает NameNode о завершении
6. NameNode обновляет метаданные файловой системы

Обновление файлов: HDFS изначально разработана для модели "write-once, read-many". Поэтому она не поддерживает произвольное изменение уже записанных файлов. Однако можно дописывать данные в конец файла (append) и переименовывать файлы. Полное обновление (изменение существующих данных) невозможно без перезаписи всего файла.

<h4>Сравнение с традиционными файловыми системами</h4>

HDFS оптимизирован для несколько иных задач, нежели традиционные файловые системы, поэтому в некоторых аспектах может отличаться:
- Цель: HDFS нужна для хранения огромных объемов данных и пакетной обработки, традиционная файловая система - для работы с файлами на одном компьютере и интерактивного использования.
- Модель доступа: HDFS поддерживает запись один раз, чтение много раз, не позволяет измененять файлы быстро. Традиционная файловая система поддерживает многократную запись и чтение и произвольное изменение файлов.
- Взаимодействие с данными: В HDFS код обработки отправляется на те узлы, где уже находятся данные, в традиционных файловых системах данные перемещаются к месту выполнения вычислений (к CPU).
- Размер блока: В HDFS - очень большой (128 МБ+), чтобы минимизировать затраты на поиск, в традиционных файловых системах - маленький (обычно 4 КБ), для эффективной работы с множеством мелких файлов.
- Стоимость: HDFS разработана для работы на недорогом, стандартном оборудовании, традиционная файловая система часто предполагает использование надежного и дорогого оборудования.
- Отказоустойчивость: В HDFS обеспечивается на уровне ПО через репликацию данных на разные узлы, традиционная файловая система часто полагается на аппаратное обеспечение (RAID, резервные блоки питания).
- Задержка (Latency): В HDFS Высокая, так как HDFS оптимизирована для высокой пропускной способности (throughput), а не для скорости отклика. В традиционных файловых системах низкая.

<h4>Масштабируемость и доступность</h4>

Масштабирование:
- Горизонтальное масштабирование: Для увеличения емкости хранилища и пропускной способности можно добавлять новые DataNodes в кластер. NameNode автоматически начнет использовать их для размещения новых блоков и репликации существующих.
- Вертикальное масштабирование: NameNode можно усилить, увеличивая оперативную память и процессорную мощность, так как он хранит все метаданные в памяти.

Обеспечение целостности данных:
- При записи каждого блока данных вычисляется контрольная сумма (checksum) и сохраняется отдельно.
- При чтении данных снова вычисляется контрольная сумма и сравнивается с сохраненной. Если они не совпадают, то данные считаются поврежденными, и клиент запрашивает другую реплику этого блока.
- DataNode также периодически проверяет целостность хранимых блоков, сверяя контрольные суммы. Если блок поврежден, DataNode сообщает об этом NameNode, и тот инициирует репликацию исправной копии.

HDFS располагает механизвами обеспечения высокой доступности:
- Отказоустойчивость: Потеря одного или даже двух (при коэффициенте 3) DataNodes не приведет к потере данных. HDFS автоматически обнаруживает потерю узла (по пропавшим "сердцебиениям") и инициирует процесс перерепликации недостающих блоков на другие живые узлы.
- Целостность данных: DataNodes периодически проверяют целостность хранимых блоков с помощью контрольных сумм. При обнаружении поврежденного блока он помечается как невалидный, и его копия запрашивается с другой реплики.
- Надежность NameNode: Для защиты от единой точки отказа используется режим High Availability (HA). В этом режиме работает два NameNode — Active и Standby. Standby Node постоянно синхронизирует свое состояние с Active Node и мгновенно берет на себя управление в случае сбоя.

<h4>Производительность</h4>

Размер блока в HDFS влияет на производительность:
- Чем меньше блок, тем его проще обрабатывать, но чаще приходится обращаться за информацией о блоках к NameNode. Уменьшает расход памяти.
- Чем больше блок, тем реже приходится обращаться за информацией о блоках к NameNode. Также большие блоки оптимизированы для пакетной обработки. Увеличивает расход памяти при использовании с множеством маленьких файлов.

<h3>2. Репликация и хранение данных</h3>
<h4>Коэффициент репликации</h4>

Коэффициент (фактор) репликации - это настройка конфигурации, которая указывает количество копий блока, которые должны быть сохранены на разных DataNodes. По умолчанию коэффициент репликации в HDFS равен трем. Это означает, что для каждого блока данных, сохраненного в HDFS, существует три копии на разных DataNodes. Эта стратегия репликации гарантирует высокую доступность и сохранность данных. Когда данные записываются в HDFS, первая копия блока сохраняется на локальном узле, если узлом записи является DataNode. Вторая копия хранится на узле в другой стойке, а третья копия - на узле в той же стойке, но на другом узле. Эта стратегия снижает риск потери данных из-за сбоя стойки и оптимизирует производительность извлечения данных.

Коэффициент репликации может быть установлен на уровне файла при его создании или изменен позже. Критически важные данные могут иметь более высокий коэффициент репликации, в то время как менее критические данные могут иметь более низкий коэффициент репликации для экономии места.

Изменение коэффициента репликации:
```bash
# Изменить фактор репликации для файла log.txt на 2
hdfs dfs -setrep -w 2 /user/data/log.txt
```

Действия при увеличении коэффициента репликации:
1. NameNode обнаруживает, что количество реплик блока меньше целевого.
2. Он помечает блок в очереди на репликацию.
3. Выбирает подходящие DataNodes (согласно политике размещения) и отдает команду одному из существующих DataNodes передать блок новым узлам.
4. Процесс продолжается до тех пор, пока для всех блоков файла не будет достигнуто новое количество реплик.

Действия при уменьшении коэффициента репликации:
1. NameNode обнаруживает, что количество реплик блока больше целевого.
2. Он помечает "лишние" реплики для удаления.
3. При следующем отправке heartbeat от DataNode, NameNode дает команду на удаление этих реплик.
4. Процесс более медленный, так как он ждет следующего отчета от DataNode.

<h4>Распределение данных</h4>

По умолчанию все HDFS-блоки реплицируются 3 раза, если клиентом (пользователем или приложением) не задано другое значение коэффициента репликации. С целью повышения надежности для хранения 2-ой и 3-ей реплики выбираются те узлы данных, которые расположены в разных серверных стойках. Последующие реплики могут храниться на любых серверах. Чтобы предотвратить потерю данных в случае сбоя кластера, следует настроить сервер имен так, чтобы он знал, на каких серверных стойках расположены узлы данных. Это делается с помощью специального механизма Hadoop — rack awareness.

HDFS периодически балансирует распределение данных по DataNodes, чтобы обеспечить эффективное использование всех узлов и предотвратить перегрузку отдельных узлов. В случае сбоя DataNode, HDFS автоматически реплицирует блоки, хранящиеся на неисправном узле, на другие узлы согласно коэффициенту репликации, обеспечивая отсутствие потери данных. Важно отметить, что в HDFS отсутствуют инструменты поддержки ссылочной целостности данных, которые могут гарантировать идентичность реплик. А, поскольку репликация выполняется в асинхронном режиме, т.е. с задержкой, вопрос идентичности реплик остается открытым – по крайней мере, на время распространения копий. Проверка целостности данных находится в зоне ответственности клиента.

<h4>Меры обеспечения надежности</h4>

HDFS реализует многоуровневую систему обеспечения надежности, выходящую далеко за рамки простой репликации:
- Репликация блоков: Основной механизм, как уже обсуждалось. Гарантирует, что данные сохраняются при потере одного или нескольких узлов.
- Распределение реплик по стойкам (Rack Awareness): Политика размещения реплик в разных стойках защищает от сбоя целого сетевого коммутатора или потери питания в стойке.
- Восстановление после сбоев (Self-Healing): DataNode каждые 3 секунды отправляют NameNode сигнал "сердцебиения" (Heartbeat). Если NameNode не получает heartbeat от DataNode в течение заданного таймаута (например, 10 минут), он помечает этот узел как вышедший из строя и проверяет, для каких блоков количество живых реплик стало ниже фактора репликации. Эти блоки помечаются для немедленной репликации. NameNode инициирует создание новых копий этих блоков на других рабочих DataNodes.
- Проверка целостности данных (Data Integrity): При записи каждого блока данных клиент вычисляет контрольную сумму (checksum) (обычно CRC-32) и сохраняет ее в скрытом файле рядом с блоком, при чтении проверяет полученные данные, вычисляя контрольную сумму заново и сравнивая ее с сохраненной.
- Если обнаружено несоответствие, клиент запрашивает данные с другой реплики этого же блока. Поврежденная реплика затем помечается и заменяется.
- Протоколирование операций (Edit Log и FsImage): NameNode хранит все метаданные файловой системы. Для надежности все изменения (создание файла, удаление и т.д.) сначала записываются в устойчивое журнал операций (Edit Log). Это гарантирует, что метаданные не будут потеряны при перезагрузке NameNode.
- Вторичный NameNode (Standby NameNode в HA-режиме): В современных кластерах с высокой доступностью (High Availability, HA) работает Standby NameNode, который постоянно синхронизируется с активным NameNode и может мгновенно заменить его в случае сбоя, минимизируя простой.
- Снапшоты (Snapshots): Позволяют создавать моментальные снимки данных на определенный момент времени, защищая от случайного удаления или изменения данных пользователем.

<h4>Управление метаданными и Heartbeat</h4>

Управление метаданными осуществляется с помощью NameNode, на котором хранятся метаданные:
- FsImage: Полный "снимок" метаданных файловой системы на определенный момент времени. Содержит иерархию файлов и директорий, права доступа, владельцев и отображение файлов на блоки. Не содержит информации о расположении блоков.
- Edit Log ("Журнал операций"): Последовательность всех операций, изменяющих метаданные файловой системы (запись нового файла, удаление, изменение репликации и т.д.), которые произошли после загрузки последнего FsImage.
- Блоки и их расположение в оперативной памяти: При запуске NameNode загружает FsImage в память и применяет к нему все транзакции из Edit Log. В результате в оперативной памяти формируется полная карта: File -> [Block1, Block2, ...] -> {DataNode1, DataNode2, ...}. Эта информация о расположении блоков не сохраняется на диск и перестраивается динамически из отчетов DataNode.

В HDFS есть метод проверки работоспособности узлов - heartbeat: DataNodes каждые 3 секунды отправляют NameNode короткий сигнал, подтверждая свою работоспособность. Вместе с heartbeat периодически передается BlockReport — полный список блоков, хранящихся на этом DataNode. NameNode интерпретирует отсутствие heartbeat в течение 10 минут как отказ DataNode.

Механизм Heartbeat:
1. Подтверждение работоспособности (Liveness): Каждый DataNode отправляет короткий сигнал (heartbeat) на NameNode каждые 3 секунды (значение настраивается). Получая heartbeat, NameNode понимает, что узел жив и работает.
2. Передача команд (Command Propagation): Ответ NameNode на heartbeat содержит команды для DataNode.
3. Передача отчетов о блоках (Blockreport): Периодически (по умолчанию каждые 6 часов или при перезапуске) каждый DataNode отправляет NameNode полный список всех блоков, которые у него хранятся. Именно на основе этих отчетов NameNode и восстанавливает в оперативной памяти полную карту расположения блоков по кластеру.

Обработка отказов:
1. Обнаружение отказа: NameNode перестает получать heartbeat от DataNode
2. Помечание узла неактивным: NameNode исключает "мертвый" узел из списка доступных для операций чтения/записи
3. Перерепликация блоков: NameNode проверяет, какие блоки стали недореплицированными (количество копий меньше заданного коэффициента репликации)
4. Восстановление: Запускается процесс копирования недостающих реплик с других DataNodes на новые живые узлы
5. Автоматическое восстановление: Процесс полностью автоматизирован и не требует вмешательства администратора

<h4>Отслеживание состояния NameNode и балансировка</h4>

NameNode является центральным компонентом HDFS, который хранит метаданные файловой системы и управляет всеми операциями с файлами, поэтому его состояние критически важно для работы кластера. В традиционной конфигурации HDFS имеется один активный NameNode, что создает единую точку отказа. При сбое NameNode весь кластер становится недоступным, DataNode продолжают хранить данные, но к ним нельзя обратиться до перезапуска NameNode.

Есть несколько способов отслеживания состояния NameNode:
- Веб-интерфейс (Web UI): Доступен по умолчанию на порту 9870 (http://namenode-host:9870). Показывает ключевые метрики: объем данных, количество Live/Dead Nodes, количество недостающих блоков, емкость кластера
- Журналы (Logs): NameNode пишет подробные логи, которые помогают диагностировать проблемы
- JMX-метрики: NameNode предоставляет метрики через JMX, которые можно собирать системами мониторинга (Prometheus, Grafana)

Балансировка решает проблему неравномерного распределения данных по кластеру, которая возникает из-за добавления новых DataNode, удаления старых DataNode и разного размера узлов в гетерогенном кластере. Балансировка требуется при разнице в загрузке между узлами больше порогового значение (по умолчанию 10%), после добавления новых пустых DataNode и после удаления заполненных DataNode. Она выполняется с помощью утилиты `balancer`:
```bash
# Запуск балансировки
hdfs balancer -threshold 10

# Запуск с ограничением пропускной способности
hdfs balancer -D dfs.balancer.max-size-to-move=10g -D dfs.datanode.balance.bandwidthPerSec=10485760

# Проверить статус балансировки
hdfs balancer -status

# Посмотреть распределение данных
hdfs dfsadmin -report | grep "Configured Capacity\|Used"
```

Ключевые параметры `balancer`:
- `-threshold`: Процентное отклонение от среднего (по умолчанию 10%)
- `-policy`: Политика балансировки (datanode или blockpool)
- `-exclude`: Исключение определенных узлов из балансировки

Алгоритм балансировки:
1. Анализ: Собирает статистику по использованию дискового пространства на всех DataNode
2. Планирование: Определяет, какие блоки и куда перемещать для выравнивания загрузки
3. Перемещение: Последовательно перемещает блоки с перегруженных узлов на менее загруженные
4. Соблюдение политик: Сохраняет правила размещения реплик (разные стойки)

<h3>2. Высокодоступная HDFS</h3>

Высокая доступность HDFS относится к способности системы продолжать функционировать без перерывов даже при отказе одного или нескольких компонентов. До введения HA HDFS полагалась на один NameNode для управления пространством имен файловой системы и метаданными всех файлов и директорий. Если NameNode выходил из строя, вся файловая система становилась недоступной. Высокодоступный режим решает эту проблему, позволяя настроить несколько узлов NameNode в конфигурации "активный-резервный".

<h4>Secondary NameNode и Edit Log</h4>

Метаданные хранятся на NameNode в двух местах:
- FsImage  - полный снимок (snapshot) метаданных на диск. Загружается при запуске NameNode.
- EditLog (Журнал редактирования) — последовательность всех операций, изменяющих метаданные после загрузки FsImage (создание файла, переименование, удаление, изменение прав доступа, назначение блоков DataNode).

При работе кластера EditLog постоянно растет, а FsImage устаревает. Если NameNode перезапустится, ему придется очень долго применять огромный EditLog к старой FsImage. Кроме того, размер EditLog на диске может стать опасно большим. Эту проблему решает Secondary NameNode. Его главная функция — периодическое объединение FsImage и EditLog для создания новой, актуальной FsImage.

Алгоритм работы Secondary NameNode:
- Запрос: Secondary NameNode периодически опрашивает NameNode и просит его начать новый сегмент EditLog.
- Копирование: NameNode отправляет Secondary NameNode текущую FsImage и все EditLog'и.
- Объединение: Secondary NameNode загружает FsImage в оперативную память, применяет к ней все операции из EditLog, создавая новую, актуальную FsImage.
- Отправка: Новая FsImage отправляется обратно в NameNode.
- Применение: NameNode принимает новую FsImage и начинает использовать ее как основную, старый FsImage и старые EditLog'и удаляются.

В высокодоступной HDFS нет Secondary NameNode, его заменяет Standby NameNode - узел, который выполняет аналогичную роль, но являющийся настоящим резервным узлом в отличие от него.

<h4>QJM</h4>

Quorum Journal Manager является ключевым компонентом HA HDFS. Он обеспечивает механизм общего хранения, который используется как активным, так и резервным NameNode для синхронизации их состояния.

Как работает QJM:
- Настраивается группа JournalNodes (обычно нечетное количество), каждый из которых действует как общее внешнее хранилище, записывающее каждое изменение, сделанное в метаданных HDFS активным NameNode.
- Когда активный NameNode вносит изменения в метаданные, он записывает запись об изменениях в JournalNodes. Этот процесс известен как запись в журнал изменений.
- Резервный NameNode находится в постоянном режиме чтения этих журналов изменений JournalNodes и применяет изменения к своему собственному образу пространства имен в памяти, гарантируя, что он готов немедленно взять на себя управление в случае сбоя активного NameNode.

Преимущества:
- Высокая отказоустойчивость: Выдерживает отказ до (N-1)/2 JournalNodes
- Строгая согласованность данных
- Производительность: Параллельная запись уменьшает задержки, асинхронная репликация к Standby NameNode и оптимизированная работа с сегментами Edit Log
- Отсутствие единой точки отказа: Нет зависимости от внешних систем хранения (NFS), распределенная архитектура
- Автоматическое восстановление: Самовосстановление после временных сбоев сети, автоматическое переподключение JournalNodes

Недостатки:
- Сложность настройки и управления: Требует настройки нескольких компонентов, необходимость мониторинга состояния кворума, сложность отладки проблем сети
- Требования к сетевой инфраструктуре: Низкая задержка между узлами критична для производительности, высокие требования к пропускной способности, чувствительность к сетевому разделению (split-brain)
- Накладные расходы на запись
- Ограничения масштабирования: Увеличение количества JournalNodes не всегда улучшает производительность, рост количества узлов увеличивает вероятность сбоев
- Сложность восстановления при потере кворума: При одновременном отказе большинства JournalNodes требуется ручное вмешательство, процедура восстановления сложна и рискованна

Настройка высокодоступной HDFS с QJM включает в себя несколько шагов:
- Конфигурацию нескольких NameNode и указание одного как активного, а другого как резервного.
- Настройку JournalNodes на отдельных машинах для обеспечения избыточности и устойчивости к отказам. Файлы конфигурации HDFS (hdfs-site.xml) на всех NameNode и DataNodes необходимо обновить, указав JournalNodes.
- Реализацию автоматического переключения на резервный режим включает настройку ZooKeeper и ZKFailoverController на каждом NameNode. ZooKeeper используется для выбора активного NameNode, а ZKFailoverController управляет процессом переключения.

Файл hdfs-site.xml:
```xml
<!-- Включение HA -->
<property>
  <name>dfs.nameservices</name>
  <value>mycluster</value>
</property>

<!-- Список NameNodes в кластере -->
<property>
  <name>dfs.ha.namenodes.mycluster</name>
  <value>nn1,nn2</value>
</property>

<!-- RPC адреса для каждого NameNode -->
<property>
  <name>dfs.namenode.rpc-address.mycluster.nn1</name>
  <value>namenode1:8020</value>
</property>
<property>
  <name>dfs.namenode.rpc-address.mycluster.nn2</name>
  <value>namenode2:8020</value>
</property>

<!-- HTTP адреса -->
<property>
  <name>dfs.namenode.http-address.mycluster.nn1</name>
  <value>namenode1:9870</value>
</property>
<property>
  <name>dfs.namenode.http-address.mycluster.nn2</name>
  <value>namenode2:9870</value>
</property>

<!-- Настройка JournalNodes -->
<property>
  <name>dfs.namenode.shared.edits.dir</name>
  <value>qjournal://journalnode1:8485;journalnode2:8485;journalnode3:8485/mycluster</value>
</property>

<!-- Класс для журнала -->
<property>
  <name>dfs.journalnode.edits.dir</name>
  <value>/var/lib/hadoop/journal</value>
</property>

<!-- Включение автоматического failover -->
<property>
  <name>dfs.ha.automatic-failover.enabled</name>
  <value>true</value>
</property>
```

Запуск QJM:

```bash
# Запуск JournalNodes
hdfs --daemon start journalnode    # На каждой машине JournalNode

# Инициализация HA в JournalNodes
hdfs namenode -initializeSharedEdits    # На одном из NameNodes (активном)

#Запуск NameNodes
hdfs --daemon start namenode       # Запуск первого NameNode
hdfs namenode -bootstrapStandby    # Запуск второго NameNode в режиме standby
hdfs --daemon start namenode

# Настройка ZooKeeper для автоматического failover
hdfs zkfc -formatZK         # Инициализация HA в Zookeeper
hdfs --daemon start zkfc    # Запуск ZKFC на каждом NameNode
```

<h3>4. Работа с HDFS</h3>
<h4>Работа с файлами и каталогами</h4>

Работа с каталогами:
```bash
# Создание каталога
hdfs dfs -mkdir /user/data
hdfs dfs -mkdir -p /user/data/raw  # Создание с родительскими каталогами

# Просмотр содержимого
hdfs dfs -ls /user/
hdfs dfs -ls -h /user/data  # Человеко-читаемый формат
hdfs dfs -ls -R /user/      # Рекурсивный обход

# Удаление
hdfs dfs -rmdir /user/empty_dir  # Удаление пустого каталога
hdfs dfs -rm -r /user/temp_data  # Рекурсивное удаление
```

Работа с файлами:
```bash
# Загрузка файлов
hdfs dfs -put local_file.txt /user/data/
hdfs dfs -copyFromLocal local_file.txt /user/data/  # Аналог put

# Скачивание файлов
hdfs dfs -get /user/data/file.txt ./local_copy.txt
hdfs dfs -copyToLocal /user/data/file.txt ./local/  # Аналог get

# Создание пустого файла
hdfs dfs -touchz /user/data/empty_file.txt
```

Поиск:
```bash
# Поиск файлов по имени (рекурсивно)
hdfs dfs -find /user/ -name "*.log"

# Поиск файлов по шаблону
hdfs dfs -find /user/data -name "data_*2024*.csv"

# Поиск каталогов
hdfs dfs -find /user/ -type d -name "temp*"

# Поиск файлов, измененных за последние N дней
hdfs dfs -find /user/logs/ -mtime -7    # Последние 7 дней
hdfs dfs -find /user/ -mtime +30        # Старше 30 дней

# Поиск файлов по размеру
hdfs dfs -find /user/ -size +100M  # Больше 100MB
hdfs dfs -find /user/ -size -10M   # Меньше 10MB

# Комбинированные условия
hdfs dfs -find /user/ -name "*.txt" -and -size +1M
```

<h4>Просмотр и управление содержимым файлов и каталогов</h4>

Просмотр содержимого:
```bash
# Просмотр файлов
hdfs dfs -cat /user/data/file.txt
hdfs dfs -tail /user/data/logfile.log  # Последние строки
hdfs dfs -tail -f /user/data/logfile.log  # Режим follow

# Постраничный просмотр
hdfs dfs -cat /user/data/large_file.txt | less

# Проверка размера
hdfs dfs -du -h /user/data/  # Размер каталога (человеко-читаемый)
hdfs dfs -du -s -h /user/data/  # Суммарный размер
hdfs dfs -df -h /user/         # Свободное место в HDFS
```

Управление содержимым:
```bash
# Проверка существования
hdfs dfs -test -e /user/data/file.txt && echo "Файл существует"

# Подсчет строк в файле
hdfs dfs -cat /user/data/file.txt | wc -l

# Поиск в файлах
hdfs dfs -cat /user/data/*.log | grep "ERROR"
```

<h4>Копирование и перемещение</h4>

Копирование:
```bash
# Копирование файлов
hdfs dfs -cp /user/data/source.txt /user/data/backup/
hdfs dfs -cp /user/data/file1.txt /user/data/file2.txt /user/destination/

# Рекурсивное копирование каталогов
hdfs dfs -cp -r /user/source_dir/ /user/backup_dir/
```

Перемещение:
```bash
# Перемещение файлов
hdfs dfs -mv /user/data/old_name.txt /user/data/new_name.txt
hdfs dfs -mv /user/temp/file.txt /user/final/

# Перемещение между каталогами
hdfs dfs -mv /user/source/*.log /user/destination/logs/
```

Копирование между локальной ФС и HDFS:
```bash
# Из локальной в HDFS
hdfs dfs -put /local/path/*.csv /user/data/input/
hdfs dfs -copyFromLocal /local/file.json /user/data/

# Из HDFS в локальную
hdfs dfs -get /user/data/results/*.csv ./local_results/
hdfs dfs -copyToLocal /user/data/report.txt ./reports/
```

<h4>Изменение прав доступа</h4>

Базовые права:
```bash
# Просмотр текущих прав
hdfs dfs -ls -h /user/data/
# Вывод: -rw-r--r--  3 user group  1.2G 2024-01-15 10:30 file.txt

# Изменение прав (аналогично chmod в Linux)
hdfs dfs -chmod 755 /user/data/script.py
hdfs dfs -chmod u+x /user/data/executable.sh  # Добавить выполнение владельцу
hdfs dfs -chmod go-w /user/data/config.conf   # Запретить запись группе и другим

# Рекурсивное изменение прав
hdfs dfs -chmod -R 750 /user/private_data/
```

Управление владельцами и группами:
```bash
# Изменение владельца
hdfs dfs -chown newuser:newgroup /user/data/file.txt
hdfs dfs -chown -R hdfsuser:hdfsgroup /user/project/

# Изменение только группы
hdfs dfs -chgrp analytics /user/data/reports/
hdfs dfs -chgrp -R developers /user/source_code/
```

Специальные права и ACL:
```bash
# Установка sticky bit (только владелец может удалять)
hdfs dfs -chmod +t /user/shared/temp/

# Расширенные ACL (Access Control Lists)
hdfs dfs -setfacl -m user:john:r-x /user/data/sensitive/
hdfs dfs -setfacl -m group:readonly:r-- /user/data/reports/
hdfs dfs -getfacl /user/data/protected/  # Просмотр ACL

# Рекурсивное применение ACL
hdfs dfs -setfacl -R -m user:auditor:r-- /user/financial_data/
```

<h4>Изменение репликации и проверка состояния</h4>

Изменение коэффициента репликации:
```bash
# Изменить коэффициент репликации для файла
hdfs dfs -setrep 2 /user/data/important_file.txt

# Рекурсивно изменить для всей директории
hdfs dfs -setrep -R 3 /user/data/

# Проверить текущий коэффициент репликации
hdfs dfs -ls /user/data/important_file.txt
# В выводе смотрим третье число (количество реплик)
# -rw-r--r--   3 hdfsuser supergroup   123456 2024-01-15 10:30 /user/data/file.txt
```

Проверка состояния:
```bash
# Детальная проверка файла (fsck)
hdfs fsck /user/data/file.txt -files -blocks -locations

# Проверка всей файловой системы
hdfs fsck / -files -blocks -racks

# Проверка конкретной директории
hdfs fsck /user/data/ -files -blocks

# Поиск недостающих блоков
hdfs fsck / -list-corruptfileblocks

# Проверка с лечением
hdfs fsck / -move   # Переместить поврежденные блоки в /lost+found
hdfs fsck / -delete # Удалить поврежденные блоки
```

<h4>Выполнение операций от другого пользователя</h4>

```bash
# Через sudo (на edge-узле)
sudo -u hdfsuser hdfs dfs -ls /user/data/

# Подключиться к узлу и выполнить команду от другого пользователя
ssh datanode01.cluster.local "sudo -u hdfs hdfs dfsadmin -report"

# Или подключиться как целевой пользователь
ssh hdfs@namenode01.cluster.local "hdfs dfsadmin -report"
```

<h4>Объединение файлов и проверка целостности</h4>

Объединение файлов:
```bash
# Объединение файлов из HDFS в локальный файл
hdfs dfs -getmerge /user/data/part-* ./merged_file.txt

# Объединение с добавлением переносов строк
hdfs dfs -getmerge -nl /user/logs/day* ./all_logs.txt

# Объединение напрямую в HDFS (через временный файл)
hdfs dfs -getmerge /user/data/chunk* /tmp/merged.tmp
hdfs dfs -put /tmp/merged.tmp /user/data/final_merged.txt
hdfs dfs -rm /tmp/merged.tmp

# Объединение через pipe (для больших файлов)
hdfs dfs -cat /user/data/part-* | hdfs dfs -put - /user/data/combined.txt
```

Проверка целостности:
```bash
# Проверка контрольных сумм файла
hdfs dfs -checksum /user/data/file.parquet

# Сравнение контрольных сумм локального и HDFS файла
hdfs dfs -checksum /user/data/file.txt | awk '{print $3}'
md5sum local_file.txt

# Проверка целостности блоков
hdfs fsck /user/data/ -blocks -locations

# Проверка на наличие поврежденных блоков
hdfs fsck / -list-corruptfileblocks

# Мониторинг целостности в реальном времени
hdfs fsck / -files -blocks | grep -i "corrupt\|missing"

# Восстановление поврежденных блоков
hdfs debug recoverLease -path /user/data/corrupted_file -retries 3
```

<h4>Использование дискового пространства</h4>

Базовые команды:
```bash
# Общее использование для пути (в байтах)
hdfs dfs -du /user/data/

# Суммарное использование директории
hdfs dfs -du -s /user/data/

# Человеко-читаемый формат
hdfs dfs -du -h /user/data/

# Суммарно в человеко-читаемом формате
hdfs dfs -du -s -h /user/data/

# Рекурсивный просмотр с сортировкой по размеру
hdfs dfs -du -h /user/ | sort -hr
```

Анализ свободного места:
```bash
# Общая статистика по файловой системе
hdfs dfs -df
hdfs dfs -df -h  # Человеко-читаемый формат

# Детальный отчет по всем DataNodes
hdfs dfsadmin -report

# Статистика использования по узлам
hdfs dfsadmin -report | grep -A 5 "Configured Capacity"

# Проверка квот
hdfs dfs -count -q /user/data/
```