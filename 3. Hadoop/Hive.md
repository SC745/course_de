<h2>Hive</h2>
<h3>1. Введение в Apache Hive</h3>
<h4>Основные понятия</h4>

Apache Hive — это распределенная, отказоустойчивая система хранения данных, предназначенная для аналитики на большом масштабе, которая позволяет считывать, записывать и управлять петабайтами данных, хранящихся в распределенном хранилище, используя SQL. Hive представляет собой критически важный компонент многих архитектур озер данных, поскольку метахранилище Hive (Hive Metastore, HMS) предоставляет централизованный репозиторий метаданных, который легко анализировать для принятия основанных на данных решений. Hive построен поверх Apache Hadoop и поддерживает хранение на S3, ADLS (Azure), HDFS, и на других хранилищах данных, позволяя пользователям считывать, записывать и управлять петабайтами данных с использованием SQL.

Ключевые характеристики:
- Не для OLTP (Online Transaction Processing): Hive не предназначен для оперативной обработки транзакций, где требуются короткое время отклика и операции UPDATE/DELETE в реальном времени. Он создан для OLAP (Online Analytical Processing) — для анализа больших объемов данных (петабайты).
- "Схема на чтение" (Schema-on-Read): В отличие от традиционных СУБД с "схемой на запись" (Schema-on-Write), где данные проверяются при загрузке, Hive применяет схему при выполнении запроса. Это позволяет загружать данные очень быстро, но ошибки в формате данных обнаруживаются только в момент запроса.
- Абстракция над MapReduce: Изначально Hive транслировал HiveQL-запросы в задания MapReduce, которые выполнялись на кластере Hadoop. Сегодня он может использовать и другие движки выполнения, такие как Apache Tez или Apache Spark, что значительно ускоряет обработку.

Архитектура Hive состоит из нескольких ключевых компонентов:
- HiveQL Language: Язык запросов, очень похожий на SQL. Пользователи пишут запросы на HiveQL, чтобы взаимодействовать с данными.
- Driver: Принимает запросы от пользователя (через CLI, JDBC и т.д.), управляет их жизненным циклом и возвращает результаты. Он создает сессию для запроса и отслеживает статистику его выполнения.
- Compiler: "Мозг" Hive. Выполняет следующие функции:
  - Парсинг: Проверяет синтаксис HiveQL-запроса.
  - Планирование: Обращается к Metastore для получения метаданных (схемы таблиц, типы данных, расположение данных в HDFS).
  - Генерация плана выполнения: На основе метаданных компилятор генерирует оптимальный логический и физический план выполнения запроса (чаще всего в виде Directed Acyclic Graph — DAG задач).
- Metastore: Центральное хранилище метаданных Hive, обычно работает на реляционной базе данных (например, MySQL, PostgreSQL), что делает его независимым от самого Hive. В нем содержится информация:
  - Структура таблиц (имена и типы столбцов).
  - Разделы (Partitions) и секции (Buckets) таблиц.
  - Месторасположение данных в HDFS или другом хранилище.
- Execution Engine: Выполняет план запроса, полученный от компилятора. Изначально это был исключительно движок MapReduce, но сейчас Hive поддерживает более быстрые движки:
  - Apache Tez: Оптимизированный фреймворк для выполнения сложных DAG-задач, который минимизирует задержки за счет устранения лишних этапов записи на диск.
  - Apache Spark: Использует мощный движок обработки данных в памяти для еще большей скорости.
- Thrift Server: Предоставляет интерфейс для подключения к Hive из внешних приложений через JDBC, ODBC и другие протоколы. Это позволяет инструментам визуализации данных (например, Tableau) подключаться к Hive напрямую.
- CLI (Command Line Interface) и GUI (например, Hue): Интерфейсы для пользователей, позволяющие вводить запросы в интерактивном режиме.

<h4>Применение Hive</h4>

Hive решает одну главную задачу: обеспечить удобный и мощный инструмент для пакетного анализа больших данных, хранящихся в распределенной файловой системе (в первую очередь, HDFS).

Типичные сценарии применения:
- ETL-процессы (Extract, Transform, Load): Hive идеально подходит для очистки, преобразования и подготовки сырых данных. Можно загрузить "сырые" данные в HDFS, а затем с помощью HiveQL-запросов структурировать их, отфильтровать ненужное, объединить с другими данными и загрузить результат в итоговую таблицу для анализа.
- Интерактивная аналитика и бизнес-аналитика (BI): Аналитики и специалисты по данным могут использовать Hive для выполнения сложных ad-hoc-запросов к огромным наборам данных. Благодаря интеграции с инструментами BI через JDBC/ODBC, можно строить дашборды и отчеты поверх данных в Hive.
- Хранилище данных (Data Warehouse) в Hadoop: Hive часто выступает в роли ядра корпоративного хранилища данных, построенного на Hadoop. Он предоставляет реляционную абстракцию поверх низкоуровневых файлов в HDFS, позволяя организациям централизованно хранить и анализировать все свои данные в одном месте.
- Исследовательский анализ данных: Специалисты по данным могут использовать Hive для первичного изучения данных, построения гипотез и проверки их на больших выборках.

С появлением Apache Spark и его компонента Spark SQL, Hive перестал быть единственным выбором для SQL-запросов в Hadoop. Spark SQL часто работает быстрее, особенно для итеративных алгоритмов, благодаря обработке в памяти. Однако Hive не умер, его Metastore - это стандарт каталога метаданных в экосистеме Hadoop. Такие инструменты, как Spark, Presto и Impala, часто используют Hive Metastore для получения информации о таблицах. Кроме того, движок выполнения Hive на базе Tez/Spark остается очень мощным и конкурентоспособным.

<h4>Хранение данных</h4>

Hive предоставляет мощные механизмы для организации хранения данных, которые напрямую влияют на производительность запросов. Данные в Hive хранятся в файлах в HDFS. Выбор формата критически важен для скорости чтения/записи и сжатия.

Форматы хранения (Storage Formats):
- Текстовые форматы (читаемые человеком):
  - `TEXTFILE`: Формат по умолчанию. Простой текст, где строки разделены символом новой строки, а поля — разделителями (например, запятой в CSV). Минусы: низкая производительность, отсутствие сжатия.
- Бинарные форматы (оптимизированные для обработки):
  - `SEQUENCEFILE`: Бинарный формат, разбивающий данные на блоки с возможностью сжатия. Был популярен в прошлом.
  - `RCFILE` (Record Columnar File): Хранит данные в колоночном формате, что ускоряет агрегатные запросы, которые читают не все столбцы.
  - `ORC` (Optimized Row Columnar): Современный, высокопроизводительный колоночный формат. Обеспечивает отличное сжатие и очень быстрое чтение благодаря встроенным индексам, статистике (минимальные/максимальные значения) и возможности пропускать целые блоки данных при запросе.
  - `Parquet`: Еще один популярный колоночный формат, независимый от экосистемы Hadoop. Особенно эффективен при работе со сложными вложенными данными. Часто используется вместе со Spark.

<h4>Настройка безопасности и управление доступом</h4>

Безопасность в Hive строится на основе инфраструктуры безопасности Hadoop и дополнительных модулей авторизации:
- Аутентификация (Authentication): Kerberos - стандартный и наиболее надежный способ аутентификации в корпоративных кластерах Hadoop. Пользователи и сервисы (например, HiveServer2) должны подтверждать свою подлинность с помощью "билетов" Kerberos. Без этого доступ к кластеру невозможен.
- Авторизация (Authorization): Определяет, что может делать аутентифицированный пользователь. Hive предоставляет несколько механизмов:
  - SQL-стандарт на основе ролей (Hive SQL Authorization) - аналогичен механизму в традиционных СУБД. Используются знакомые `GRANT` и `REVOKE`, можно выдавать привилегии (`SELECT`, `INSERT`, `UPDATE`, `ALL`, ...) на уровне базы данных, таблицы или даже столбца. Привилегии можно выдавать пользователям и ролям:
  - Apache Ranger: Предоставляет централизованное, детализированное управление доступом для всей экосистемы Hadoop (Hive, HDFS, Kafka и т.д.). Позволяет настраивать политики с помощью удобного UI, включает в себя аудит всех операций.
- Маскирование и фильтрация данных (Data Masking & Filtering):
  - Column-Level Masking (в Ranger): Можно настроить политики, чтобы разные пользователи видели разные представления данных в одном столбце. Например, пользователю из отдела маркетинга может показываться только первые 4 цифры номера кредитной карты.
  - Row-Level Filtering (в Ranger): Можно автоматически добавлять к запросам пользователя условия WHERE. Например, менеджер может видеть только те записи о продажах, которые относятся к его региону.
- Шифрование (Encryption): HDFS Transparent Encryption - данные могут шифроваться "на лету" при записи в HDFS и расшифровываться при чтении. Hive, работая поверх HDFS, автоматически получает преимущества этого шифрования без изменения кода запросов.

<h4>Управление данными</h4>

Одним из ключевых преимуществ Hive является его масштабируемость, благодаря возможности обработки больших наборов данных на инфраструктуре Hadoop, поддержке синтаксиса SQL и возможности подключения с использованием различных клиентов (например, Beeline, JDBC, и др.) из различных языков программирования, таких как Java, Scala, C#, Python и многих других. Hive также предлагает различные способы обработки данных, включая MapReduce, скрипты Pig и HiveQL. Однако следует отметить, что Hive не предназначен для обработки OLTP и не является реляционной базой данных (RDBMS), что означает отсутствие поддержки обновлений на уровне строк для систем реального времени.

Hive по умолчанию сохраняет данные в папке `/user/hive/warehouse` в HDFS, если при создании таблицы не указано другое местоположение с помощью выражения `LOCATION`. Hive играет роль хранилища данных для Hadoop, и все файлы данных баз данных и таблиц по умолчанию лежат по адресу HDFS `/user/hive/warehouse`, однако можно также хранить файлы данных Hive в другом месте на HDFS, S3 или любой другой совместимой с Hadoop файловой системе.

Для определения местонахождения таблицы Hive можно воспользоваться командой:
```bash
hive -S -e "DESCRIBE FORMATTED table_name;" | grep 'Location' | awk '{ print $NF }'
```

Работая с Hive, необходимо помнить о двух разных хранилищах данных:
- Метахранилище Hive: это централизованное хранилище для метаданных, включая информацию о базах данных, таблицах и столбцах в вашем кластере Hive. Эти метаданные важны для анализа данных и обеспечивают эффективную работу с данными. Hive включает компонент HCatalog, который управляет таблицами и хранилищем данных, облегчая интеграцию между Hive, Apache Pig и MapReduce. Используя метахранилище и HCatalog, можно обеспечить доступ к метаданным и их использование в различных приложениях и сервисах через WebHCat — RESTful API для HCatalog. Оно по умолчанию использует базу данных Derby, однако возможно использование и других RDBMS, таких как MySQL, Postgres и т. д. Метахранилище по умолчанию имеет название metastore_db.
- Основное хранилище данных Hive (где фактически хранятся данные таблиц): Представляет собой распределённое хранилище для обработки и анализа больших объёмов данных. Hive позволяет выполнять SQL-подобные запросы (HiveQL) на данных, хранящихся в различных базах данных и файловых системах, интегрированных с Hadoop​​. При запуске запросов Hive использует Hadoop для распределенной обработки, превращая их в серию автоматически сгенерированных задач MapReduce, Tez или Spark.

<h4>Настройка Hive</h4>

Настройка Hive осуществляется в основном через файл hive-site.xml или напрямую в сессии с помощью команды SET.

Ключевые настройки:
- Движок выполнения:
  - `hive.execution.engine=mr` (MapReduce, устарел)
  - `hive.execution.engine=tez` (Apache Tez, рекомендуется для производительности)
  - `hive.execution.engine=spark` (Apache Spark)
- Оптимизация:
  - `hive.auto.convert.join=true` (автоматическое преобразование JOIN в MAPJOIN)
  - `hive.exec.parallel=true` (параллельное выполнение стадий запроса)
  - `hive.exec.parallel.thread.number` (количество параллельных потоков)
- Управление памятью: Настройки контейнеров Tez/MapReduce (mapreduce.map.memory.mb, mapreduce.reduce.memory.mb).
- Партиционирование и букинг:
  - `hive.exec.dynamic.partition=true` (включение динамических партиций)
  - `hive.exec.dynamic.partition.mode=nonstrict`

Настройка в сессии:
```sql
-- Установить движок выполнения на Tez
SET hive.execution.engine=tez;

-- Включить параллельное выполнение
SET hive.exec.parallel=true;
```

<h3>2. HiveQL</h3>
<h4>Основы HiveQL</h4>

HiveQL — это SQL-подобный язык запросов в Hive, который позволяет пользователям взаимодействовать с данными, хранящимися в HDFS. Он поддерживает многие конструкции из SQL-92, но также имеет некоторые расширения для работы с большими данными.

Основные особенности HiveQL:
- Подмножество SQL: HiveQL поддерживает многие стандартные операторы и функции SQL, такие как `SELECT`, `FROM`, `WHERE`, `GROUP BY`, `ORDER BY`, `JOIN` и т.д.
- Расширения для Big Data: HiveQL включает расширения для работы с большими данными, такие как:
  - Партиционирование и бакетирование: Позволяют оптимизировать запросы за счет организации данных.
  - Работа со сложными типами данных: Поддержка `ARRAY`, `MAP`, `STRUCT` и `UNIONTYPE`.
  - Трансформации данных: Возможность использования пользовательских функций (UDF) и скриптов (например, с помощью TRANSFORM).

Ключевые отличия от традиционного SQL:
- `INSERT` не для одиночных записей: В Hive нет оператора INSERT INTO ... VALUES. Вместо этого используются:
  - `INSERT OVERWRITE` — для перезаписи данных
  - `INSERT INTO` — для добавления данных
- Отложенная обработка: Запросы выполняются как распределенные задания, что занимает время (от секунд до часов).

<h4>Типы данных</h4>

Hive поддерживает богатый набор типов данных, который можно разделить на две основные категории: примитивные и комплексные.

Примитивные типы данных (Primitive Types) - базовые типы, аналогичные тем, что есть в реляционных СУБД:
- Числовые:
  - `TINYINT` (1-байтовое целое)
  - `SMALLINT` (2-байтовое целое)
  - `INT` (4-байтовое целое)
  - `BIGINT` (8-байтовое целое)
  - `FLOAT` (4-байтовое число с плавающей точкой)
  - `DOUBLE` (8-байтовое число с плавающей точкой)
  - `DECIMAL` (число с фиксированной точностью и масштабом, для финансовых расчетов)
- Строковые:
  - `STRING` (строка переменной длины, до 2 ГБ; аналог `VARCHAR` в других СУБД)
  - `VARCHAR` (строка переменной длины с указанием макс. длины)
  - `CHAR` (строка фиксированной длины)
- Дата и время:
  - `TIMESTAMP` (дата и время, с точностью до наносекунд)
  - `DATE` (только дата, без времени)
- Другие:
  - `BOOLEAN` (значения `TRUE` или `FALSE`)
  - `BINARY` (массив байтов)

Комплексные типы данных (Complex Types) - позволяют работать с вложенными структурами, которые часто встречаются в полуструктурированных данных:
- `ARRAY`: Упорядоченная коллекция элементов одного типа. Пример: ARRAY<STRING> — массив строк. Доступ к элементам по индексу: `arr[0]`.
- `MAP`: Неупорядоченная коллекция пар "ключ-значение". Ключи должны быть одного примитивного типа, значения — любого. Пример: `MAP<STRING, INT>` — сопоставление строки целому числу. Доступ по ключу: `map['key']`.
- `STRUCT`: Аналог объекта, который содержит набор именованных полей с произвольными типами. Пример: `STRUCT<name:STRING, age:INT, salary:DOUBLE>`. Доступ к полям через точку: `struct.name`.
- `UNIONTYPE`: Значение, которое может быть одним из нескольких указанных типов.

Таблица со сложными типами:
```sql
CREATE TABLE employees (
  id INT,
  name STRING,
  skills ARRAY<STRING>,           -- например, ['Java', 'Hive', 'Spark']
  contacts MAP<STRING, STRING>,   -- например, {'email': 'alice@company.com', 'phone': '12345'}
  address STRUCT<city:STRING, street:STRING, zip:INT> -- например, {'Moscow', 'Lenina', 101000}
);
```

<h4>Таблицы в Hive</h4>

В Hive можно создавать два основных типа таблиц: внутренние (Managed) и внешние (External) таблицы:
- Внутренние таблицы (Managed Tables): Внутренние таблицы тесно связаны с Hive. Когда вы создаете внутреннюю таблицу и загружаете в нее данные, Hive управляет жизненным циклом этих данных. Если вы удалите внутреннюю таблицу, Hive удалит и данные, и схему таблицы. Внутренние таблицы подходят, когда данные для обработки доступны в локальной файловой системе и вы хотите, чтобы Hive полностью управлял данными, включая их удаление.
- Внешние таблицы (External Tables): Внешние таблицы являются слабосвязанными с Hive. Данные для внешних таблиц хранятся в HDFS, и при создании таблицы Hive создает схему для этих данных. Удаление внешней таблицы приводит к удалению только схемы, в то время как сами данные остаются в HDFS. Внешние таблицы предпочтительны, когда данные уже доступны в HDFS и вы хотите, чтобы эти данные использовались вне Hive, или когда вы не хотите, чтобы Hive управлял жизненным циклом данных.

Синтаксис создания таблицы:
```sql
CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name
(
  column_name data_type [COMMENT column_comment],
  ...
)
[COMMENT table_comment]
[PARTITIONED BY (partition_column data_type [COMMENT partition_comment], ...)]
[CLUSTERED BY (column_name, ...) [SORTED BY (column_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
[ROW FORMAT row_format]
[STORED AS file_format]
[LOCATION hdfs_path]
[TBLPROPERTIES (property_name=property_value, ...)];
```

Управление таблицами:
```sql
-- Просмотр таблиц
SHOW TABLES;
SHOW TABLES LIKE 'emp*';

-- Описание таблицы
DESCRIBE FORMATTED employees;

-- Просмотр партиций
SHOW PARTITIONS sales;

-- Добавление партиции вручную
ALTER TABLE sales ADD PARTITION (sale_date='2023-10-26', region='US');

-- Удаление партиции
ALTER TABLE sales DROP PARTITION (sale_date='2023-10-25', region='US');

-- Изменение таблицы
ALTER TABLE employees ADD COLUMNS (phone STRING COMMENT 'Phone number');
ALTER TABLE employees RENAME TO staff;

-- Удаление таблицы
DROP TABLE employees;
```

<h4>Преобразование данных</h4>
Преобразование данных — это основная задача Hive. Оно выполняется с помощью операторов HiveQL, встроенных функций и пользовательской логики.

Ключевые методы преобразования:
1. Встроенные функции (Built-in Functions):
  - Математические функции: `ROUND()`, `CEIL(),` `FLOOR()`, `ABS()`, `POWER()`, `SQRT()`
  - Строковые функции:
    - `LOWER()`, `UPPER()`, `TRIM()`, `LTRIM()`, `RTRIM()` - обработка строк
    - `CONCAT()`, `CONCAT_WS()` - конкатенация
    - `SUBSTR()`, `SPLIT()` - получение подстроки
    - `REGEXP_EXTRACT()`, `REGEXP_REPLACE()` - для сложных поисков и замен по регулярным выражениям
  - Функции даты и времени:
    - `FROM_UNIXTIME()`, `UNIX_TIMESTAMP()`: преобразует Unix timestamp в строку и наоборот.
    - `TO_DATE()`, `YEAR()`, `MONTH()`, `DAY()`, `HOUR()`: преобразование и элементы даты
    - `DATE_ADD()`, `DATE_SUB()`, `DATEDIFF()`: математические операции с датами
  - Условные функции:
    - `CASE ... WHEN ... THEN ... ELSE ... END`: стандартный условный оператор.
    - `COALESCE()`: возвращает первое не-NULL значение из списка.
    - `NULLIF()`: возвращает NULL, если два выражения равны.
    - `NVL()`: возвращает значение по умолчанию, если выражение NULL.
2. Оператор `TRANSFORM` / `MAP` / `REDUCE`: Это мощная функция для применения пользовательских скриптов (на Python, Ruby, Bash и т.д.) к данным прямо в запросе. Данные передаются скрипту через стандартный ввод (stdin), а результат читается из стандартного вывода (stdout).

<h4>Работа с данными и транзакциями</h4>

Загрузка данных:
```sql
LOAD DATA INPATH hdfs_path
[OVERWRITE] INTO TABLE table_name
[PARTITION (partition_spec)];
```

Поддержка операций вставки, обновления и удаления с полной функциональностью ACID стала доступна с версии 0.14. Чтобы активировать ACID транзакции в Hive, необходимо выполнить несколько шагов:
1. Включить режим параллелизма:
```sql
SET hive.support.concurrency=true;
```
2. Включить менеджер транзакций ACID (DbTxnManager) для сессии Hive:
```sql
SET hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
```
3. Создать транзакционную таблицу:
```sql
TBLPROPERTIES ('transactional'='true')).
```
4. Использовать формат хранения ORC:
```sql
STORED AS ORC
```

Создание ACID-таблицы:
```sql
CREATE TABLE transactional_table (
    id int,
    value string
)
CLUSTERED BY (id) INTO 2 BUCKETS
STORED AS ORC
TBLPROPERTIES ('transactional'='true');
```

Операции с ACID-таблицей:
```sql
-- Вставка (работает как обычно)
INSERT INTO transactional_table VALUES (1, 'A'), (2, 'B');

-- Обновление данных
UPDATE transactional_table SET value = 'Z' WHERE id = 1;

-- Удаление данных
DELETE FROM transactional_table WHERE id = 2;

-- Выборка с учетом последних изменений
SELECT * FROM transactional_table; -- Вернет (1, 'Z')
```

<h4>Функции агрегирования</h4>

Функции агрегирования используются для выполнения вычислений над набором строк и возврата одного результирующего значения. Они обычно используются в связке с оператором GROUP BY.

Основные агрегатные функции:
- `COUNT()`: Подсчитывает количество строк.
- `SUM()`: Возвращает сумму всех значений в столбце.
- `AVG()`: Возвращает среднее арифметическое значений в столбце.
- `MIN()` / `MAX()`: Возвращают минимальное/максимальное значение в столбце.
- `COLLECT_LIST()`: Собирает все значения группы в один список (включая дубликаты). Порядок элементов не гарантируется.
- `COLLECT_SET()`: Собирает все уникальные значения группы в один набор (без дубликатов).

```sql
-- Средняя зарплата по отделам
SELECT department,
       AVG(salary) as avg_salary
  FROM employees
 GROUP BY department;

-- Общее количество сотрудников и максимальная зарплата в компании
SELECT COUNT(*) as total_employees,
       MAX(salary) as max_salary
  FROM employees;

-- Список всех имен сотрудников в каждом отделе (с дубликатами)
SELECT department,
       COLLECT_LIST(name) as employee_names
  FROM employees
 GROUP BY department;
```

<h4>Соединения таблиц и подзапросы</h4>

Hive поддерживает стандартные SQL-соединения для объединения данных из двух или более таблиц.
- `INNER JOIN`: Возвращает строки, имеющие соответствия в обеих таблицах.
- `LEFT OUTER JOIN`: Возвращает все строки из левой таблицы и соответствующие строки из правой. Если соответствия нет, в правой части `NULL`.
- `RIGHT OUTER JOIN`: Аналогично `LEFT JOIN`, но приоритет у правой таблицы.
- `FULL OUTER JOIN`: Возвращает все строки из обеих таблиц, объединяя их по возможности.
- `CROSS JOIN`: Декартово произведение (все со всеми). Используется редко из-за огромного объема результата.

```sql
-- Соединяем таблицу заказов с таблицей клиентов
SELECT o.order_id,
       c.customer_name,
       o.order_date
  FROM orders AS o
       JOIN customers AS c ON o.customer_id = c.customer_id;
```

Подзапросы бывают коррелированные и некоррелированные:
- Некоррелированный подзапрос: Выполняется один раз до основного запроса:
```sql
SELECT name, salary
FROM employees
WHERE salary > (SELECT AVG(salary) FROM employees);
```
- Коррелированный подзапрос: Ссылается на поля из внешнего запроса, выполняется для каждой строки:
```sql
SELECT e1.name, e1.department, e1.salary
FROM employees e1
WHERE e1.salary = (
    SELECT MAX(e2.salary)
    FROM employees e2
    WHERE e2.department = e1.department);
```

Hive также поддерживает подзапросы в FROM (вьюхи на лету, "derived tables"):
```sql
SELECT dept,
       avg_sal
  FROM (SELECT department AS dept,
               AVG(salary) AS avg_sal
          FROM employees
         GROUP BY department) AS subquery
 WHERE avg_sal > 50000;
```

<h4>Пользовательские функции (UDF)</h4>

Когда встроенных функций недостаточно, Hive позволяет создавать собственные.

Типы UDF:
- UDF (User-Defined Function): Работает с одной строкой и возвращает одно значение. Аналогична `LOWER()` или `ROUND()`. Наследуется от org.apache.hadoop.hive.ql.exec.UDF, нужно переопределить метод evaluate().
- UDAF (User-Defined Aggregate Function): Выполняет агрегацию над множеством строк и возвращает одно значение. Аналогична `SUM()` или `COUNT()`. Наследуется от org.apache.hadoop.hive.ql.exec.UDAF. Требует написания внутреннего класса Evaluator.
- UDTF (User-Defined Table-Generating Function): Принимает одну строку и возвращает несколько строк (виртуальную таблицу). Аналогична `EXPLODE()`. Наследуется от org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.

Процесс использования UDF:
1. Написать Java-код, реализующий нужный тип функции.
2. Скомпилировать и упаковать в JAR-файл.
3. Добавить JAR в Hive сессию: 
```sql
ADD JAR /path/to/my_udf.jar;
```
4. Создать временную функцию: 
```sql
CREATE TEMPORARY FUNCTION my_upper AS 'com.mycompany.hive.udf.MyUpperUDF';
```
5. Использовать в запросе: 
```sql
SELECT my_upper(name) FROM table;
```

<h3>3. Оптимизация</h3>
<h4>Оптимизация сканирования и управления данными</h4>

В Hive существуют различные методы для оптимизации сканирования и управления данными, которые позволяют ускорить выполнение запросов и эффективно организовать данные:

- Партицирование (Partitioning): Партицирование в Hive позволяет разделять таблицу на несколько частей на основе значений одного или нескольких столбцов (например, даты, страны и т.д.). Каждая часть называется разделом и хранится в отдельной папке в файловой системе. Партицирование позволяет выполнять запросы эффективнее, так как Hive может обрабатывать только нужные разделы, а не всю таблицу целиком.
- Кластеризация (Bucketing): Кластеризация в Hive позволяет дополнительно организовать данные внутри раздела (если используется партицирование) или всей таблицы по определенному столбцу (или столбцам) путем распределения строк по фиксированному количеству "корзин" (buckets). Это делается на основе хэш-функции от значения столбца. Кластеризация полезна для оптимизации запросов, особенно при выполнении операций соединения (JOIN) и выборки (SAMPLE). Улучшает производительность запросов, особенно для операций соединения, за счет равномерного распределения данных и уменьшения количества данных для обработки. Позволяет эффективно использовать выборку данных для тестирования запросов на меньшем объеме данных.
- Использование колоночных форматов хранения: ORC и Parquet, позволяет значительно сократить объем сканируемых данных за счет сжатия данных, возможности чтения только необходимых столбцов (projection pushdown) и использования индексов и статистик для пропуска блоков данных (predicate pushdown).
- Сбор статистики: позволяет оптимизатору Hive выбирать более эффективные планы выполнения запросов. Статистику можно собирать с помощью команды ANALYZE:
```sql
ANALYZE TABLE sales COMPUTE STATISTICS;
ANALYZE TABLE sales COMPUTE STATISTICS FOR COLUMNS product, amount;
```

<h4>Оптимизация запросов</h4>

В Hive есть возможность тонкой настройки для оптимизации запросов:
- Использование Tez или Spark в качестве движка выполнения: Эти движки могут значительно ускорить выполнение запросов по сравнению с MapReduce.
```sql
SET hive.execution.engine=tez;
```
- Векторизация (Vectorization): Позволяет Hive обрабатывать данные пакетами (по 1024 строки за раз), что уменьшает накладные расходы.
```sql
SET hive.vectorized.execution.enabled=true;
SET hive.vectorized.execution.reduce.enabled=true;
```
- Predicate Pushdown: Hive пытается применить условия фильтрации как можно раньше, чтобы уменьшить объем обрабатываемых данных. Это особенно эффективно при использовании ORC и Parquet.
- Cost-Based Optimization (CBO): Использует статистику по таблицам и столбцам для выбора оптимального плана запроса.
```sql
SET hive.cbo.enable=true;
SET hive.compute.query.using.stats=true;
SET hive.stats.fetch.column.stats=true;
SET hive.stats.fetch.partition.stats=true;
```
- Оптимизация Join:
  - Map Join: Для соединения большой таблицы с маленькой. Маленькая таблица загружается в память всех мапперов.
```sql
SET hive.auto.convert.join=true;
SET hive.mapjoin.smalltable.filesize=25000000; -- 25MB
```
  - Sort-Merge Bucket Join (SMB Join): Для соединения двух больших таблиц, которые забакетированы и отсортированы по ключу соединения.
```sql
SET hive.auto.convert.sortmerge.join=true;
SET hive.optimize.bucketmapjoin=true;
SET hive.optimize.bucketmapjoin.sortedmerge=true;
```
- Параллельное выполнение: Hive может выполнять независимые этапы запроса параллельно.
```sql
SET hive.exec.parallel=true;
SET hive.exec.parallel.thread.number=16; -- количество параллельных задач
```
- Настройка памяти и параллелизма: Регулирование памяти, выделенной для задач, и количества мапперов и редьюсеров.
```sql
SET mapreduce.map.memory.mb=4096;
SET mapreduce.reduce.memory.mb=8192;
SET hive.exec.reducers.bytes.per.reducer=256000000;
```

Предположим, у нас есть большая таблица продаж и небольшая таблица продуктов. Мы хотим получить отчет по продажам только для определенной категории продуктов:

```sql
-- Включим необходимые настройки оптимизации
SET hive.auto.convert.join=true;
SET hive.vectorized.execution.enabled=true;
SET hive.exec.parallel=true;

-- Запрос с использованием map join и предикат pushdown
SELECT s.sale_date,
       p.product_name,
       SUM(s.amount) as total_amount
  FROM sales AS s
       JOIN products AS p ON s.product_id = p.id
 WHERE p.category = 'Electronics'
   AND s.sale_date BETWEEN '2023-01-01' AND '2023-12-31'
 GROUP BY s.sale_date, p.product_name;
```

В этом примере:
- Таблица `products` скорее всего маленькая, поэтому Hive может использовать map join.
- Условия на `category` и `sale_date` будут проталкиваться в сканирование, чтобы уменьшить объем данных.
- Агрегация будет выполняться параллельно.

<h4>Cost-Based Optimizer</h4>

Cost-Based Optimizer (CBO) — это компонент Hive, который выбирает оптимальный план выполнения запроса на основе стоимости различных операций. Стоимость оценивается с использованием статистики по данным (количество строк, распределение значений, гистограммы и т.д.).

Алгоритм работы:
1. Сбор статистики: CBO требует, чтобы статистика по таблицам и столбцам была собрана и актуальна. Это включает:
   - Количество строк в таблице и партициях.
   - Размер данных, минимальные и максимальные значения.
   - Количество уникальных значений (NDV - number of distinct values) для столбцов.
2. Оценка стоимости операций: CBO использует статистику для оценки стоимости различных планов выполнения, например, выбора порядка JOIN, метода JOIN (Map Join, Sort Merge Join и т.д.), а также для применения предикатов.
3. Выбор оптимального плана: CBO сравнивает стоимость разных планов и выбирает тот, который, по его оценке, будет выполнен быстрее всего.

Без CBO Hive использует правило-based оптимизацию, которая может быть неэффективной для сложных запросов. Например, поряд JOIN-ов определяется их порядком в запросе. С CBO Hive может изменить поряд JOIN-ов, чтобы сначала выполнялись JOIN-ы с меньшим объемом данных.

```sql
-- Включение CBO
SET hive.cbo.enable=true;
SET hive.compute.query.using.stats=true;
SET hive.stats.fetch.column.stats=true;
SET hive.stats.fetch.partition.stats=true;

-- Сбор статистики для таблицы
ANALYZE TABLE sales COMPUTE STATISTICS;
ANALYZE TABLE sales COMPUTE STATISTICS FOR COLUMNS product_id, amount;

-- Запрос с несколькими JOIN
SELECT a.product_id,
       b.category,
       c.customer_name
  FROM sales AS a
       JOIN products AS b ON a.product_id = b.id
       JOIN customers AS c ON a.customer_id = c.id;
```

Преимущества CBO:
- Улучшение производительности сложных запросов с множественными JOIN и подзапросами.
- Более точная оценка количества данных на каждом этапе запроса.

<h4>Vectorized Query Execution</h4>

Vectorized Query Execution — это техника выполнения запросов, при которой данные обрабатываются не по одной строке, а блоками (векторами) строк. Это позволяет более эффективно использовать ресурсы CPU, так как современные процессоры оптимизированы для операций с векторами.

В обычном режиме Hive обрабатывает данные построчно, что приводит к большим накладным расходам на виртуальные вызовы функций и плохому использованию кэша CPU. В векторизованном режиме:
1. Блочная обработка: Данные считываются блоками по 1024 строки (или другое количество, в зависимости от настройки).
2. Векторные операции: Операции (как фильтрация, арифметические операции) выполняются над целыми векторами, что позволяет использовать SIMD (Single Instruction, Multiple Data) инструкции процессора.

Условия для векторизации:
- Формат данных: ORC (рекомендуется) или Parquet.
- Поддерживаемые типы данных: большинство примитивных типов и некоторые сложные (но не все).
- Поддерживаемые операторы: фильтрация, агрегация, JOIN и другие.

```sql
-- Включение векторизации для выполнения на MapReduce
SET hive.vectorized.execution.enabled=true;
SET hive.vectorized.execution.reduce.enabled=true;

-- Для Tez
SET hive.vectorized.execution.enabled=true;
SET hive.vectorized.execution.tez.enabled=true;

-- Пример запроса, который может использовать векторизацию
SELECT product_id,
       SUM(amount) as total_amount
  FROM sales
 WHERE sale_date BETWEEN '2023-01-01' AND '2023-12-31'
   AND amount > 0
 GROUP BY product_id;
```

Преимущества векторизации:
- Ускорение выполнения запросов в 2-10 раз за счет более эффективного использования CPU.
- Уменьшение накладных расходов на обработку каждой строки.

<h4>Сжатие данных</h4>

Сжатие данных в Hive позволяет уменьшить объем хранимых данных и ускорить их передачу по сети, но может увеличить нагрузку на CPU при сжатии и распаковке.

Популярные методы сжатия в Hive:
- Gzip: Высокий уровень сжатия, но медленнее в распаковке. Хорош для данных, к которым обращаются редко (холодные данные).
- Snappy: Быстрое сжатие и распаковка, но меньшая степень сжатия. Рекомендуется для горячих данных, когда важна скорость.
- LZO: Аналогичен Snappy, но требует индексирования для splittability.
- Zstandard (ZSTD): Хороший компромисс между скоростью и степенью сжатия.
- BZip2: Очень высокое сжатие, но очень медленное. Подходит для архивных данных.

Влияние сжатия на производительность
- Хранение: Уменьшение объема данных на диске.
- Сеть: Меньший объем данных передается между узлами кластера.
- CPU: Дополнительная нагрузка на сжатие и распаковку.

```sql
-- Установка сжатия для промежуточных данных (между этапами MapReduce/Tez)
SET hive.exec.compress.intermediate=true;
SET mapred.map.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;

-- Установка сжатия для конечных результатов
SET hive.exec.compress.output=true;
SET mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;

-- Создание таблицы со сжатием ORC
CREATE TABLE sales_compressed (
    product_id INT,
    amount DOUBLE,
    customer_id INT
)
STORED AS ORC
TBLPROPERTIES ("orc.compress"="SNAPPY");
```

Рекомендации по сжатию:
- Промежуточные данные: Использование Snappy для быстрого сжатия, так как эти данные временные.
- Хранение данных: Snappy для частого чтения и Gzip для экономии места и редкого чтения.
- Колоночные форматы: ORC и Parquet уже используют сжатие на уровне столбцов, что очень эффективно. Дополнительно можно применить сжатие для всего файла.

<h4>Индексация</h4>

Hive поддерживает индексы для ускорения поиска по определенным колонкам. Однако в последних версиях Hive индексы считаются устаревшими в пользу использования форматов хранения с встроенными индексами (например, ORC и Parquet).
- ORC формат предоставляет встроенные индексы, такие как индексы строк (row indexes) и bloom-фильтры.
- Bloom-фильтры позволяют эффективно проверять наличие значения в файле без его полного сканирования.

Пример создания таблицы ORC с bloom-фильтром:
```sql
CREATE TABLE sales_orc (
    id INT,
    amount DOUBLE,
    product STRING
)
STORED AS ORC
TBLPROPERTIES ("orc.bloom.filter.columns"="id,product");
```

<h4>Настройка размера файлов и партиций</h4>

Hive неэффективно работает с большим количеством маленьких файлов, так как каждый файл создает отдельную задачу (task) в MapReduce/Tez, что ведет к накладным расходам. Для решения этой проблемы нужно объединять файлы.

Настройки для объединения выходных файлов (Merge):
- `hive.merge.mapfiles`: Объединять файлы на выходе из Map-задач (по умолчанию true).
- `hive.merge.mapredfiles`: Объединять файлы на выходе из Reduce-задач (по умолчанию false). Важно включить для запросов с `GROUP BY` или `JOIN`.
- `hive.merge.size.per.task`: Целевой размер объединенного файла (по умолчанию 256 МБ).
- `hive.merge.smallfiles.avgsize`: Если средний размер файла меньше этого значения, будет запущен процесс объединения (по умолчанию 16 МБ).

```sql
SET hive.merge.mapfiles = true;
SET hive.merge.mapredfiles = true;
SET hive.merge.size.per.task = 512000000; -- 512 МБ
SET hive.merge.smallfiles.avgsize = 256000000; -- 256 МБ
```

Использование `DISTRIBUTE BY` и `SORT BY`: Чтобы контролировать, сколько Reduce-задач (и, следовательно, файлов) будет создано, можно принудительно распределить данные. Количество уникальных значений в some_column повлияет на количество файлов:

```sql
INSERT OVERWRITE TABLE my_table
SELECT * FROM source_table
DISTRIBUTE BY some_column -- Распределяет данные по редукторам на основе хэша от some_column
SORT BY some_column; -- Сортирует данные внутри каждого файла
```

Партиционирование — это физическое разделение данных на каталоги по значениям столбцов (например, `year=2023/month=10`).

Настройки, связанные с партициями:
- `hive.exec.dynamic.partition`: Разрешает динамическое партиционирование (по умолчанию false).
- `hive.exec.dynamic.partition.mode`: Режим динамического партиционирования. Может быть `strict` (запрещает партиции без фильтра) или `nonstrict` (разрешает).
- `hive.exec.max.dynamic.partitions`: Максимальное общее количество динамических партиций, которое можно создать одним запросом (по умолчанию 1000).
- `hive.exec.max.dynamic.partitions.pernode`: Максимальное количество динамических партиций на один узел данных.

Рекомендации по партициям:
- Не создавать слишком много партиций. Слишком мелкое партиционирование (например, по дню для 10 лет данных = 3650 партиций) может создать нагрузку на Hive Metastore.
- Выбирайть столбцы с низкой кардинальностью (небольшое количество уникальных значений) для партиционирования (например, `country`, `year`, `month`).
- Использовать бакетирование (Bucketing) вместе с партиционированием для дальнейшего управления размером файлов и ускорения `JOIN`. Бакетирование делит данные внутри партиции на фиксированное количество файлов на основе хэша от столбца.

<h3>4. Интеграция с другими системами</h3>
<h4>Взаимодействие с Hadoop MapReduce</h4>

Hive преобразует SQL-подобные запросы HiveQL в задания MapReduce. Это один из основных способов выполнения запросов в Hive (хотя сейчас также поддерживаются Tez и Spark).

Процесс преобразования HiveQL в MapReduce:
1. Парсинг и планирование: Hive парсит запрос HiveQL и строит абстрактное синтаксическое дерево (AST). Затем это дерево преобразуется в план запроса, который состоит из операторов (таких как `TableScan`, `Filter`, `Join`, `GroupBy` и т.д.).
2. Оптимизация: План запроса оптимизируется с помощью правил оптимизации (например, объединение операторов, предварительные агрегации и т.д.).
3. Генерация MapReduce: Оптимизированный план запроса преобразуется в Directed Acyclic Graph (DAG) заданий MapReduce. Каждое задание MapReduce состоит из:
  - Map-стадия: Чтение данных из таблиц, фильтрация, проекция и преобразование. Ключом является то, что данные распределяются по ключам для reduce-стадии.
  - Reduce-стадия: Выполнение агрегаций, соединений, сортировок и т.д.

Hive позволяет настраивать параметры MapReduce заданий через SET-команды. Например:
```sql
SET mapreduce.job.maps=10;        -- Количество map-задач
SET mapreduce.job.reduces=5;      -- Количество reduce-задач
SET hive.exec.parallel=true;      -- Параллельное выполнение стадий
```

Однако стоит отметить, что Hive на MapReduce может быть медленным из-за накладных расходов на запуск заданий и необходимостью записи промежуточных данных на диск. Поэтому были введены альтернативные движки выполнения, такие как Tez и Spark, которые работают быстрее.

<h4>Интеграция с внешними источниками данных</h4>

Hive изначально разработан для работы с данными в HDFS, но он также может интегрироваться с внешними источниками данных. Это позволяет использовать Hive для запросов к данным, хранящимся за пределами HDFS, например, в реляционных базах данных, Amazon S3, Azure Blob Storage и других.

Ключевой механизм для интеграции — создание внешних таблиц. Внешняя таблица Hive управляет метаданными, но не управляет жизненным циклом данных. Удаление внешней таблицы не приводит к удалению исходных данных.

```sql
CREATE EXTERNAL TABLE my_external_table (
    id INT,
    name STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LOCATION '/path/in/hdfs';  -- или путь во внешней файловой системе, например, s3a://my-bucket/path/
```

Hive поддерживает различные хранилища данных через соответствующие коннекторы:
- Amazon S3 и Azure Blob Storage: Можно использовать специальные схемы URI (например, `s3a://` или `wasb://`) в пункте `LOCATION`. Предварительно необходимо настроить кластер для доступа к этим хранилищам (установить соответствующие библиотеки и настроить учетные данные).
- Реляционные базы данных: С помощью HCatalog и Storage Handlers Hive может обращаться к данным в других базах данных, таких как MySQL, PostgreSQL, Oracle и т.д. Для этого используется конструкция `CREATE EXTERNAL TABLE` с указанием специального `STORED BY` и настройками подключения.

Коннектор для MySQL:
```sql
CREATE EXTERNAL TABLE hive_mysql_table (
    id INT,
    name STRING
)
STORED BY 'org.apache.hadoop.hive.mysql.storagehandler.MySQLStorageHandler'
TBLPROPERTIES (
    'mysql.host' = 'localhost',
    'mysql.port' = '3306',
    'mysql.database' = 'test',
    'mysql.table' = 'mysql_table',
    'mysql.user' = 'hive',
    'mysql.password' = 'hive'
);
```

Коннектор для MongoDB:
```sql
CREATE TABLE mongo_table (
    id STRING,
    name STRING,
    age INT
)
STORED BY 'com.mongodb.hadoop.hive.MongoStorageHandler'
TBLPROPERTIES (
    'mongo.uri' = 'mongodb://localhost:27017/mydb.collection'
);
```

Коннектор для Apache Kafka:
```sql
CREATE TABLE kafka_table (
    key STRING,
    message STRING,
    topic STRING,
    partition INT,
    offset BIGINT
)
STORED BY 'org.apache.hadoop.hive.kafka.KafkaStorageHandler'
TBLPROPERTIES (
    'kafka.bootstrap.servers' = 'kafka-broker:9092',
    'kafka.topic' = 'my-topic'
);
```

<h4>Интеграция Hive с HBase</h4>

HBase — это распределенная, масштабируемая, NoSQL база данных, работающая поверх HDFS. Hive может интегрироваться с HBase, позволяя выполнять SQL-запросы к данным, хранящимся в HBase, и даже объединять их с данными из HDFS.

Для интеграции с HBase при создании таблицы Hive используется STORED BY с указанием специального обработчика хранилища (Storage Handler) для HBase.

```sql
CREATE EXTERNAL TABLE hive_hbase_table (
    key STRING,
    value STRING
)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
    'hbase.columns.mapping' = ':key,cf:val'  -- Отображение столбцов Hive на колонки HBase
)
TBLPROPERTIES (
    'hbase.table.name' = 'hbase_table'       -- Имя таблицы в HBase
);
```

В этом примере:
- `key` в Hive отображается на `row key` в HBase.
- `value` в Hive отображается на значение в семействе колонок `cf` с квалификатором `val`.

Особенности интеграции:
- Типы данных: HBase хранит данные как байтовые массивы, поэтому в Hive необходимо преобразование. Hive использует SerDe для преобразования данных HBase в типы Hive. По умолчанию используется строковое представление.
- Производительность: Запросы Hive к HBase могут быть медленнее, чем нативные запросы HBase, потому что Hive использует MapReduce (или Tez/Spark) для полного сканирования таблицы. Однако это позволяет выполнять сложные аналитические запросы, которые сложно выразить на API HBase.
- Фильтрация: Hive может передавать предикаты (условия в WHERE) в HBase, чтобы использовать фильтры на стороне HBase (pushdown predicates). Это может значительно ускорить запросы.
- Запись данных: Через Hive можно вставлять данные в HBase. Однако это может быть менее эффективно, чем использование API HBase.

Hive по умолчанию интегрирован с HDFS. Однако, есть несколько конфигураций, которые можно задать в hive-site.xml или через командную строку.

Основные настройки в hive-site.xml:
```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- Хост и порт HDFS -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://namenode:8020</value>
        <description>URI HDFS NameNode</description>
    </property>

    <!-- Директория данных Hive в HDFS -->
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
        <description>Локация таблиц Hive в HDFS</description>
    </property>

    <!-- Временная директория -->
    <property>
        <name>hive.exec.scratchdir</name>
        <value>/tmp/hive</value>
        <description>Временная директория для Hive</description>
    </property>
</configuration>
```