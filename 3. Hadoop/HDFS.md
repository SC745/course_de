<h2>HDFS</h2>
<h3>1. Основные концепции и архитектура</h3>

HDFS (Hadoop Distributed File System), является ключевым компонентом экосистемы Hadoop, предназначенным для хранения больших объемов данных на нескольких машинах в распределенном порядке. HDFS обеспечивает высокую пропускную способность к данным, обладает высокой устойчивостью к отказам и предназначен для развертывания на недорогом оборудовании. Она разбивает большие файлы на меньшие блоки (размером по умолчанию 128MB или 256MB, однако этот параметр можно перенастроить) и распределяет их по нескольким узлам в кластере. Это позволяет эффективно обрабатывать большие наборы данных с помощью программной модели MapReduce.

Архитектура:
- NameNode - управляющий узел, который хранит метаданные всей файловой системы. NameNode отслеживает структуру каталогов и метаданные для всех файлов, включая информацию о том, какие блоки данных составляют каждый файл и на каких DataNodes эти блоки расположены. Также регулирует доступ клиентов к файлам и отслеживает работоспособность DataNodes.
- DataNode - узлы хранящие фактические данные и обслуживающие запросы на чтение и запись от NameNode. В HDFS файл разбивается на один или несколько блоков, и каждый блок копируется на несколько DataNodes в соответствии с заданным фактором репликации для обеспечения отказоустойчивости.
- Secondary NameNode - несмотря на свое название, не является резервным узлом для NameNode. Его основная функция — периодически сливать изменения журнала NameNode с его текущим состоянием в файловой системе, помогая уменьшить размер журнала и предотвращая потерю данных.

Основные концепции:
- Блочное хранение: Основной единицей хранения в HDFS является "блок". Когда файл сохраняется в HDFS, он разделяется на один или несколько блоков. Эти блоки хранятся на узлах DataNodes.
- DataNodes и NameNode: Узлы DataNodes отвечают за хранение и извлечение блоков по запросу. NameNode хранит информацию о дереве файловой системы и метаданные всех файлов и директорий. Эти метаданные включают информацию, такую как расположение блоков, хранящихся на DataNodes.
- Распределение данных и согласованность: HDFS разработан для работы с большими файлами. Он распределяет данные по нескольким узлам для обеспечения высокой доступности и устойчивости данных. При хранении данных в HDFS они автоматически реплицируются на узлах DataNodes в соответствии с коэффициентом репликации, обеспечивая наличие копии данных даже в случае сбоя оборудования.

Типы данных:
- Текстовые форматы (CSV, TSV, JSON, лог-файлы)
- Последовательные файлы (Sequence Files)
- Колоночные форматы (Parquet, ORC)
- Форматы сериализации (Avro, Protocol Buffers, Thrift)
- Специализированные форматы (RCFile)

<h4>Чтение и запись</h4>

Процесс чтения:
1. Клиент запрашивает у NameNode расположение блоков файла
2. NameNode возвращает список DataNode, содержащих каждый блок
3. Клиент обращается напрямую к DataNode за блоками
4. DataNode передают данные клиенту
5. Если DataNode недоступен, клиент запрашивает блок у реплики

Процесс записи:
1. Клиент запрашивает у NameNode создание нового файла
2. NameNode проверяет права и возвращает список DataNode для записи
3. Клиент начинает передачу данных первому DataNode в конвейере
4. DataNode передает данные следующему DataNode в цепочке
5. После записи всех блоков клиент сообщает NameNode о завершении
6. NameNode обновляет метаданные файловой системы

Обновление файлов: HDFS изначально разработана для модели "write-once, read-many". Поэтому она не поддерживает произвольное изменение уже записанных файлов. Однако можно дописывать данные в конец файла (append) и переименовывать файлы. Полное обновление (изменение существующих данных) невозможно без перезаписи всего файла.

<h4>Сравнение с традиционными файловыми системами</h4>

HDFS оптимизирован для несколько иных задач, нежели традиционные файловые системы, поэтому в некоторых аспектах может отличаться:
- Цель: HDFS нужна для хранения огромных объемов данных и пакетной обработки, традиционная файловая система - для работы с файлами на одном компьютере и интерактивного использования.
- Модель доступа: HDFS поддерживает запись один раз, чтение много раз, не позволяет измененять файлы быстро. Традиционная файловая система поддерживает многократную запись и чтение и произвольное изменение файлов.
- Взаимодействие с данными: В HDFS код обработки отправляется на те узлы, где уже находятся данные, в традиционных файловых системах данные перемещаются к месту выполнения вычислений (к CPU).
- Размер блока: В HDFS - очень большой (128 МБ+), чтобы минимизировать затраты на поиск, в традиционных файловых системах - маленький (обычно 4 КБ), для эффективной работы с множеством мелких файлов.
- Стоимость: HDFS разработана для работы на недорогом, стандартном оборудовании, традиционная файловая система часто предполагает использование надежного и дорогого оборудования.
- Отказоустойчивость: В HDFS обеспечивается на уровне ПО через репликацию данных на разные узлы, традиционная файловая система часто полагается на аппаратное обеспечение (RAID, резервные блоки питания).
- Задержка (Latency): В HDFS Высокая, так как HDFS оптимизирована для высокой пропускной способности (throughput), а не для скорости отклика. В традиционных файловых системах низкая.

<h4>Масштабируемость и доступность</h4>

Масштабирование:
- Горизонтальное масштабирование: Для увеличения емкости хранилища и пропускной способности можно добавлять новые DataNodes в кластер. NameNode автоматически начнет использовать их для размещения новых блоков и репликации существующих.
- Вертикальное масштабирование: NameNode можно усилить, увеличивая оперативную память и процессорную мощность, так как он хранит все метаданные в памяти.

Обеспечение целостности данных:
- При записи каждого блока данных вычисляется контрольная сумма (checksum) и сохраняется отдельно.
- При чтении данных снова вычисляется контрольная сумма и сравнивается с сохраненной. Если они не совпадают, то данные считаются поврежденными, и клиент запрашивает другую реплику этого блока.
- DataNode также периодически проверяет целостность хранимых блоков, сверяя контрольные суммы. Если блок поврежден, DataNode сообщает об этом NameNode, и тот инициирует репликацию исправной копии.

HDFS располагает механизвами обеспечения высокой доступности:
- Отказоустойчивость: Потеря одного или даже двух (при коэффициенте 3) DataNodes не приведет к потере данных. HDFS автоматически обнаруживает потерю узла (по пропавшим "сердцебиениям") и инициирует процесс перерепликации недостающих блоков на другие живые узлы.
- Целостность данных: DataNodes периодически проверяют целостность хранимых блоков с помощью контрольных сумм. При обнаружении поврежденного блока он помечается как невалидный, и его копия запрашивается с другой реплики.
- Надежность NameNode: Для защиты от единой точки отказа используется режим High Availability (HA). В этом режиме работает два NameNode — Active и Standby. Standby Node постоянно синхронизирует свое состояние с Active Node и мгновенно берет на себя управление в случае сбоя.

<h4>Оптимизация</h4>

Размер блока в HDFS влияет на производительность:
- Чем меньше блок, тем его проще обрабатывать, но чаще приходится обращаться за информацией о блоках к NameNode. Уменьшает расход памяти.
- Чем больше блок, тем реже приходится обращаться за информацией о блоках к NameNode. Также большие блоки оптимизированы для пакетной обработки. Увеличивает расход памяти при использовании с множеством маленьких файлов.

Для оптимизации можно применить следующее:
- Настройка кучи (heap size): Важно настроить размеры кучи NameNode и DataNode в зависимости от размера доступной памяти кластера для оптимизации производительности.
- Репликация данных: Установка правильного коэффициента репликации данных позволяет снизить риски потери данных. Больше количество реплик увеличивает степень сохранности данных, но требует больше места для хранения.
- Размер блока: Большие размеры блоков снижают накладные расходы на управление блоками, но могут быть неэффективны для маленьких файлов.
- Размер памяти NameNode: NameNode требуется достаточно памяти для хранения метаданных всех файлов и блоков в системе. Размер памяти следует увеличивать пропорционально количеству файлов и блоков.

<h3>2. Репликация и хранение данных</h3>
<h4>Коэффициент репликации</h4>

Коэффициент (фактор) репликации - это настройка конфигурации, которая указывает количество копий блока, которые должны быть сохранены на разных DataNodes. По умолчанию коэффициент репликации в HDFS равен трем. Это означает, что для каждого блока данных, сохраненного в HDFS, существует три копии на разных DataNodes. Эта стратегия репликации гарантирует высокую доступность и сохранность данных. Когда данные записываются в HDFS, первая копия блока сохраняется на локальном узле, если узлом записи является DataNode. Вторая копия хранится на узле в другой стойке, а третья копия - на узле в той же стойке, но на другом узле. Эта стратегия снижает риск потери данных из-за сбоя стойки и оптимизирует производительность извлечения данных.

Коэффициент репликации может быть установлен на уровне файла при его создании или изменен позже. Критически важные данные могут иметь более высокий коэффициент репликации, в то время как менее критические данные могут иметь более низкий коэффициент репликации для экономии места.

Изменение коэффициента репликации:
```bash
# Изменить фактор репликации для файла log.txt на 2
hdfs dfs -setrep -w 2 /user/data/log.txt
```

Действия при увеличении коэффициента репликации:
1. NameNode обнаруживает, что количество реплик блока меньше целевого.
2. Он помечает блок в очереди на репликацию.
3. Выбирает подходящие DataNodes (согласно политике размещения) и отдает команду одному из существующих DataNodes передать блок новым узлам.
4. Процесс продолжается до тех пор, пока для всех блоков файла не будет достигнуто новое количество реплик.

Действия при уменьшении коэффициента репликации:
1. NameNode обнаруживает, что количество реплик блока больше целевого.
2. Он помечает "лишние" реплики для удаления.
3. При следующем отправке heartbeat от DataNode, NameNode дает команду на удаление этих реплик.
4. Процесс более медленный, так как он ждет следующего отчета от DataNode.

<h4>Распределение данных</h4>

По умолчанию все HDFS-блоки реплицируются 3 раза, если клиентом (пользователем или приложением) не задано другое значение коэффициента репликации. С целью повышения надежности для хранения 2-ой и 3-ей реплики выбираются те узлы данных, которые расположены в разных серверных стойках. Последующие реплики могут храниться на любых серверах. Чтобы предотвратить потерю данных в случае сбоя кластера, следует настроить сервер имен так, чтобы он знал, на каких серверных стойках расположены узлы данных. Это делается с помощью специального механизма Hadoop — rack awareness.

HDFS периодически балансирует распределение данных по DataNodes, чтобы обеспечить эффективное использование всех узлов и предотвратить перегрузку отдельных узлов. В случае сбоя DataNode, HDFS автоматически реплицирует блоки, хранящиеся на неисправном узле, на другие узлы согласно коэффициенту репликации, обеспечивая отсутствие потери данных. Важно отметить, что в HDFS отсутствуют инструменты поддержки ссылочной целостности данных, которые могут гарантировать идентичность реплик. А, поскольку репликация выполняется в асинхронном режиме, т.е. с задержкой, вопрос идентичности реплик остается открытым – по крайней мере, на время распространения копий. Проверка целостности данных находится в зоне ответственности клиента.

<h4>Меры обеспечения надежности</h4>

HDFS реализует многоуровневую систему обеспечения надежности, выходящую далеко за рамки простой репликации:
- Репликация блоков: Основной механизм, как уже обсуждалось. Гарантирует, что данные сохраняются при потере одного или нескольких узлов.
- Распределение реплик по стойкам (Rack Awareness): Политика размещения реплик в разных стойках защищает от сбоя целого сетевого коммутатора или потери питания в стойке.
- Восстановление после сбоев (Self-Healing): DataNode каждые 3 секунды отправляют NameNode сигнал "сердцебиения" (Heartbeat). Если NameNode не получает heartbeat от DataNode в течение заданного таймаута (например, 10 минут), он помечает этот узел как вышедший из строя и проверяет, для каких блоков количество живых реплик стало ниже фактора репликации. Эти блоки помечаются для немедленной репликации. NameNode инициирует создание новых копий этих блоков на других рабочих DataNodes.
- Проверка целостности данных (Data Integrity): При записи каждого блока данных клиент вычисляет контрольную сумму (checksum) (обычно CRC-32) и сохраняет ее в скрытом файле рядом с блоком, при чтении проверяет полученные данные, вычисляя контрольную сумму заново и сравнивая ее с сохраненной.
- Если обнаружено несоответствие, клиент запрашивает данные с другой реплики этого же блока. Поврежденная реплика затем помечается и заменяется.
- Протоколирование операций (Edit Log и FsImage): NameNode хранит все метаданные файловой системы. Для надежности все изменения (создание файла, удаление и т.д.) сначала записываются в устойчивое журнал операций (Edit Log). Это гарантирует, что метаданные не будут потеряны при перезагрузке NameNode.
- Вторичный NameNode (Standby NameNode в HA-режиме): В современных кластерах с высокой доступностью (High Availability, HA) работает Standby NameNode, который постоянно синхронизируется с активным NameNode и может мгновенно заменить его в случае сбоя, минимизируя простой.
- Снапшоты (Snapshots): Позволяют создавать моментальные снимки данных на определенный момент времени, защищая от случайного удаления или изменения данных пользователем.

<h4>Управление метаданными и Heartbeat</h4>

Управление метаданными осуществляется с помощью NameNode, на котором хранятся метаданные:
- FsImage: Полный "снимок" метаданных файловой системы на определенный момент времени. Содержит иерархию файлов и директорий, права доступа, владельцев и отображение файлов на блоки. Не содержит информации о расположении блоков.
- Edit Log ("Журнал операций"): Последовательность всех операций, изменяющих метаданные файловой системы (запись нового файла, удаление, изменение репликации и т.д.), которые произошли после загрузки последнего FsImage.
- Блоки и их расположение в оперативной памяти: При запуске NameNode загружает FsImage в память и применяет к нему все транзакции из Edit Log. В результате в оперативной памяти формируется полная карта: File -> [Block1, Block2, ...] -> {DataNode1, DataNode2, ...}. Эта информация о расположении блоков не сохраняется на диск и перестраивается динамически из отчетов DataNode.

В HDFS есть метод проверки работоспособности узлов - heartbeat: DataNodes каждые 3 секунды отправляют NameNode короткий сигнал, подтверждая свою работоспособность. Вместе с heartbeat периодически передается BlockReport — полный список блоков, хранящихся на этом DataNode. NameNode интерпретирует отсутствие heartbeat в течение 10 минут как отказ DataNode.

Механизм Heartbeat:
1. Подтверждение работоспособности (Liveness): Каждый DataNode отправляет короткий сигнал (heartbeat) на NameNode каждые 3 секунды (значение настраивается). Получая heartbeat, NameNode понимает, что узел жив и работает.
2. Передача команд (Command Propagation): Ответ NameNode на heartbeat содержит команды для DataNode.
3. Передача отчетов о блоках (Blockreport): Периодически (по умолчанию каждые 6 часов или при перезапуске) каждый DataNode отправляет NameNode полный список всех блоков, которые у него хранятся. Именно на основе этих отчетов NameNode и восстанавливает в оперативной памяти полную карту расположения блоков по кластеру.

Обработка отказов:
1. Обнаружение отказа: NameNode перестает получать heartbeat от DataNode
2. Помечание узла неактивным: NameNode исключает "мертвый" узел из списка доступных для операций чтения/записи
3. Перерепликация блоков: NameNode проверяет, какие блоки стали недореплицированными (количество копий меньше заданного коэффициента репликации)
4. Восстановление: Запускается процесс копирования недостающих реплик с других DataNodes на новые живые узлы
5. Автоматическое восстановление: Процесс полностью автоматизирован и не требует вмешательства администратора

<h4>Отслеживание состояния NameNode и балансировка</h4>

NameNode является центральным компонентом HDFS, который хранит метаданные файловой системы и управляет всеми операциями с файлами, поэтому его состояние критически важно для работы кластера. В традиционной конфигурации HDFS имеется один активный NameNode, что создает единую точку отказа. При сбое NameNode весь кластер становится недоступным, DataNode продолжают хранить данные, но к ним нельзя обратиться до перезапуска NameNode.

Есть несколько способов отслеживания состояния NameNode:
- Веб-интерфейс (Web UI): Доступен по умолчанию на порту 9870 (http://namenode-host:9870). Показывает ключевые метрики: объем данных, количество Live/Dead Nodes, количество недостающих блоков, емкость кластера
- Журналы (Logs): NameNode пишет подробные логи, которые помогают диагностировать проблемы
- JMX-метрики: NameNode предоставляет метрики через JMX, которые можно собирать системами мониторинга (Prometheus, Grafana)

Балансировка решает проблему неравномерного распределения данных по кластеру, которая возникает из-за добавления новых DataNode, удаления старых DataNode и разного размера узлов в гетерогенном кластере. Балансировка требуется при разнице в загрузке между узлами больше порогового значение (по умолчанию 10%), после добавления новых пустых DataNode и после удаления заполненных DataNode. Она выполняется с помощью утилиты `balancer`:
```bash
# Запуск балансировки
hdfs balancer -threshold 10

# Запуск с ограничением пропускной способности
hdfs balancer -D dfs.balancer.max-size-to-move=10g -D dfs.datanode.balance.bandwidthPerSec=10485760

# Проверить статус балансировки
hdfs balancer -status

# Посмотреть распределение данных
hdfs dfsadmin -report | grep "Configured Capacity\|Used"
```

Ключевые параметры `balancer`:
- `-threshold`: Процентное отклонение от среднего (по умолчанию 10%)
- `-policy`: Политика балансировки (datanode или blockpool)
- `-exclude`: Исключение определенных узлов из балансировки

Алгоритм балансировки:
1. Анализ: Собирает статистику по использованию дискового пространства на всех DataNode
2. Планирование: Определяет, какие блоки и куда перемещать для выравнивания загрузки
3. Перемещение: Последовательно перемещает блоки с перегруженных узлов на менее загруженные
4. Соблюдение политик: Сохраняет правила размещения реплик (разные стойки)

<h3>3. Высокодоступная HDFS</h3>

Высокая доступность HDFS относится к способности системы продолжать функционировать без перерывов даже при отказе одного или нескольких компонентов. До введения HA HDFS полагалась на один NameNode для управления пространством имен файловой системы и метаданными всех файлов и директорий. Если NameNode выходил из строя, вся файловая система становилась недоступной. Высокодоступный режим решает эту проблему, позволяя настроить несколько узлов NameNode в конфигурации "активный-резервный".

В высокодоступной конфигурации HDFS мы имеем следующие ключевые компоненты:
- Активный NameNode (Active NameNode): обрабатывает все операции клиентов и управляет пространством имен файловой системы.
- Резервный NameNode (Standby NameNode): находится в режиме ожидания, синхронизирует свое состояние с активным NameNode и готов взять на себя роль активного в случае сбоя.
- Журнальные узлы (JournalNodes, JNs): образуют распределенную систему для хранения журналов редактирования (edit logs). Обычно их нечетное количество (3 или более) для обеспечения кворума.
- Менеджер контроля доступа (ZKFC - ZooKeeper Failover Controller): запускается на каждой машине с NameNode и отслеживает состояние NameNode с помощью ZooKeeper. Он отвечает за инициирование переключения (failover) в случае сбоя активного NameNode.
- Apache ZooKeeper: используется для координации между NameNode и для выбора активного NameNode. ZKFC использует ZooKeeper для мониторинга и управления состоянием NameNode.

<h4>Secondary NameNode и Edit Log</h4>

Метаданные хранятся на NameNode в двух местах:
- FsImage  - полный снимок (snapshot) метаданных на диск. Загружается при запуске NameNode.
- EditLog (Журнал редактирования) — последовательность всех операций, изменяющих метаданные после загрузки FsImage (создание файла, переименование, удаление, изменение прав доступа, назначение блоков DataNode).

При работе кластера EditLog постоянно растет, а FsImage устаревает. Если NameNode перезапустится, ему придется очень долго применять огромный EditLog к старой FsImage. Кроме того, размер EditLog на диске может стать опасно большим. Эту проблему решает Secondary NameNode. Его главная функция — периодическое объединение FsImage и EditLog для создания новой, актуальной FsImage.

Алгоритм работы Secondary NameNode:
- Запрос: Secondary NameNode периодически опрашивает NameNode и просит его начать новый сегмент EditLog.
- Копирование: NameNode отправляет Secondary NameNode текущую FsImage и все EditLog'и.
- Объединение: Secondary NameNode загружает FsImage в оперативную память, применяет к ней все операции из EditLog, создавая новую, актуальную FsImage.
- Отправка: Новая FsImage отправляется обратно в NameNode.
- Применение: NameNode принимает новую FsImage и начинает использовать ее как основную, старый FsImage и старые EditLog'и удаляются.

В высокодоступной HDFS нет Secondary NameNode, его заменяет Standby NameNode - узел, который выполняет аналогичную роль, но являющийся настоящим резервным узлом в отличие от него.

<h4>Standby NameNode и Failover</h4>

Standby NameNode играет следующие роли в высокодоступной HDFS:
- Постоянная синхронизация состояния: Standby Node непрерывно читает журналы изменений (Edits Log) из общего хранилища и применяет их к своей копии пространства имен в памяти. Это делает его состояние практически идентичным состоянию Active Node.
- Выполнение обязанностей Secondary NameNode: В некоторых конфигурациях Standby Node также берет на себя функцию checkpoint'инга, которая в классической архитектуре лежала на Secondary NameNode. Он периодически сливает FsImage и Edits Log в новое, компактное FsImage.

Failover - это процесс переключения при отказе Active NameNode, в ходе которого Standby NameNode становится новым Active. Этот процесс выполняется следующим образом:
1. Обнаружение сбоя: Служба ZKFailoverController (ZKFC), работающая на каждом NameNode, постоянно посылает сигнал своему локальному NameNode.
2. Сессия в Zookeeper: Каждый ZKFC создает эфемерный узел (ephemeral node) в Zookeeper. Узел того NameNode, который является Active, блокируется.
3. Отказ Active: Если Active NameNode перестает отвечать на сигнал, его ZKFC умирает, и его эфемерный узел в Zookeeper автоматически удаляется.
4. Выбор нового лидера: Standby ZKFC видит, что узел освободился, и немедленно пытается создать свой собственный эфемерный узел. Если ему это удается, он "выигрывает выборы".
5. Переход в Active: ZKFC на бывшем Standby Node выполняет серию действий для своего NameNode:
6. "Изящный" переход (fencing): Убеждается, что старый Active Node действительно мертв (например, посылает команду kill -9 его процессу через скрипт fencing), чтобы предотвратить ситуацию "split-brain" (когда оба NameNode считают себя Active).
7. Перевод в режим Active: Командует своему NameNode перейти в состояние Active.

<h4>Настройка высокой доступности</h4>

Настройка HA для HDFS включает в себя несколько ключевых компонентов. Рассмотрим на примере использования Quorum Journal Manager (QJM).

Требования к инфраструктуре:
- Два (или более) хоста для NameNode (например, nn1.cluster.local и nn2.cluster.local).
- Минимум три хоста для JournalNodes (это отдельные легковесные процессы, могут быть размещены на тех же хостах, что и DataNode). JournalNodes образуют кворум, поэтому для отказоустойчивости нужен нечетное количество (3, 5...).
- Общая настройка SSH без пароля между NameNode для работы скриптов fencing.

Шаги настройки:
1. Запустить JournalNodes на выделенных хостах: `hdfs --daemon start journalnode`
2. Запустить оба NameNode.
3. Отформатировать один из NameNode и инициализировать общее хранилище журналов (JournalNodes): `hdfs namenode -format -clusterId <clusterId>`
4. Запустить этот отформатированный NameNode. Он станет Active.
5. На втором NameNode синхронизировать метаданные: `hdfs namenode -bootstrapStandby`
6. Запустить второй NameNode. Он станет Standby.
7. Инициализировать Zookeeper для автоматического failover (если используется): `hdfs zkfc -formatZK`
8. Запустить службы ZKFC на каждом NameNode: `hdfs --daemon start zkfc`

<h4>Zookeeper в высокодоступной HDFS</h4>

Zookeeper играет критически важную роль в реализации автоматического failover. Без него автоматическое переключение невозможно.

Основные функции Zookeeper в HA HDFS:
- Хранение состояния лидерства (Active Lock): ZKFC от Active NameNode создает в Zookeeper эфемерный узел (например, /hadoop-ha/mycluster/ActiveStandbyElectorLock). Сам факт существования этого узла означает, что его владелец — текущий Active NameNode. Эфемерность узла — ключевое свойство: если сессия ZKFC с Zookeeper разрывается (из-за сбоя NameNode или сети), узел автоматически удаляется. Это мгновенный и надежный сигнал для всех участников, что Active Node перестал быть активным.
- Обнаружение сбоев (Failure Detection): ZKFC на каждом NameNode постоянно мониторит здоровье своего локального NameNode через health checks. Одновременно он следит за состоянием эфемерного узла в Zookeeper, если ZKFC Standby Node видит, что узел удалился, и при этом его локальный NameNode здоров, он понимает, что пора инициировать перевыборы.
- Координация выборов (Leader Election): Когда эфемерный узел освобождается, все Standby ZKFC (чьи NameNode здоровы) одновременно пытаются создать его. Механизм атомарности в Zookeeper гарантирует, что узел создаст только один из них. Этот ZKFC и его NameNode становятся новыми лидерами (Active).
- Предотвращение "Split-Brain" (Fencing): ZKFC, выигравший выборы, перед тем как перевести свой NameNode в Active, обязан выполнить fencing старого Active. Часто для этого используется скрипт, который пытается завершить процесс старого NameNode. Zookeeper здесь предоставляет достоверную информацию о том, кто "настоящий" новый лидер, уполномочивая его на выполнение этой критической операции.

<h4>QJM</h4>

Quorum Journal Manager является ключевым компонентом HA HDFS. Он обеспечивает механизм общего хранения, который используется как активным, так и резервным NameNode для синхронизации их состояния.

Как работает QJM:
- Настраивается группа JournalNodes (обычно нечетное количество), каждый из которых действует как общее внешнее хранилище, записывающее каждое изменение, сделанное в метаданных HDFS активным NameNode.
- Когда активный NameNode вносит изменения в метаданные, он записывает запись об изменениях в JournalNodes. Этот процесс известен как запись в журнал изменений.
- Резервный NameNode находится в постоянном режиме чтения этих журналов изменений JournalNodes и применяет изменения к своему собственному образу пространства имен в памяти, гарантируя, что он готов немедленно взять на себя управление в случае сбоя активного NameNode.

Преимущества:
- Высокая отказоустойчивость: Выдерживает отказ до (N-1)/2 JournalNodes
- Строгая согласованность данных
- Производительность: Параллельная запись уменьшает задержки, асинхронная репликация к Standby NameNode и оптимизированная работа с сегментами Edit Log
- Отсутствие единой точки отказа: Нет зависимости от внешних систем хранения (NFS), распределенная архитектура
- Автоматическое восстановление: Самовосстановление после временных сбоев сети, автоматическое переподключение JournalNodes

Недостатки:
- Сложность настройки и управления: Требует настройки нескольких компонентов, необходимость мониторинга состояния кворума, сложность отладки проблем сети
- Требования к сетевой инфраструктуре: Низкая задержка между узлами критична для производительности, высокие требования к пропускной способности, чувствительность к сетевому разделению (split-brain)
- Накладные расходы на запись
- Ограничения масштабирования: Увеличение количества JournalNodes не всегда улучшает производительность, рост количества узлов увеличивает вероятность сбоев
- Сложность восстановления при потере кворума: При одновременном отказе большинства JournalNodes требуется ручное вмешательство, процедура восстановления сложна и рискованна

Настройка высокодоступной HDFS с QJM включает в себя несколько шагов:
- Конфигурацию нескольких NameNode и указание одного как активного, а другого как резервного.
- Настройку JournalNodes на отдельных машинах для обеспечения избыточности и устойчивости к отказам. Файлы конфигурации HDFS (hdfs-site.xml) на всех NameNode и DataNodes необходимо обновить, указав JournalNodes.
- Реализацию автоматического переключения на резервный режим включает настройку ZooKeeper и ZKFailoverController на каждом NameNode. ZooKeeper используется для выбора активного NameNode, а ZKFailoverController управляет процессом переключения.

Файл hdfs-site.xml:
```xml
<!-- Включение HA -->
<property>
  <name>dfs.nameservices</name>
  <value>mycluster</value>
</property>

<!-- Список NameNodes в кластере -->
<property>
  <name>dfs.ha.namenodes.mycluster</name>
  <value>nn1,nn2</value>
</property>

<!-- RPC адреса для каждого NameNode -->
<property>
  <name>dfs.namenode.rpc-address.mycluster.nn1</name>
  <value>namenode1:8020</value>
</property>
<property>
  <name>dfs.namenode.rpc-address.mycluster.nn2</name>
  <value>namenode2:8020</value>
</property>

<!-- HTTP адреса -->
<property>
  <name>dfs.namenode.http-address.mycluster.nn1</name>
  <value>namenode1:9870</value>
</property>
<property>
  <name>dfs.namenode.http-address.mycluster.nn2</name>
  <value>namenode2:9870</value>
</property>

<!-- Настройка JournalNodes -->
<property>
  <name>dfs.namenode.shared.edits.dir</name>
  <value>qjournal://journalnode1:8485;journalnode2:8485;journalnode3:8485/mycluster</value>
</property>

<!-- Класс для журнала -->
<property>
  <name>dfs.journalnode.edits.dir</name>
  <value>/var/lib/hadoop/journal</value>
</property>

<!-- Включение автоматического failover -->
<property>
  <name>dfs.ha.automatic-failover.enabled</name>
  <value>true</value>
</property>
```

Запуск QJM:

```bash
# Запуск JournalNodes
hdfs --daemon start journalnode    # На каждой машине JournalNode

# Инициализация HA в JournalNodes
hdfs namenode -initializeSharedEdits    # На одном из NameNodes (активном)

#Запуск NameNodes
hdfs --daemon start namenode       # Запуск первого NameNode
hdfs namenode -bootstrapStandby    # Запуск второго NameNode в режиме standby
hdfs --daemon start namenode

# Настройка ZooKeeper для автоматического failover
hdfs zkfc -formatZK         # Инициализация HA в Zookeeper
hdfs --daemon start zkfc    # Запуск ZKFC на каждом NameNode
```

<h3>4. Хранилища данных в HDFS</h3>

HDFS разработан для хранения больших объемов данных и предоставляет гибкие механизмы для управления данными на основе их важности, частоты доступа и требований к производительности. Среди этих механизмов — политики хранения данных, позволяющие оптимально использовать различные типы хранилищ, такие как холодное (cold), теплое (warm) и горячее (hot) хранилища. Эти категории отражают частоту доступа к данным и помогают определить, как и где лучше всего хранить различные наборы данных в HDFS.

Типы хранилищ:
- Горячее Хранилище (Hot Storage): Данные, к которым часто обращаются и которые требуют быстрого доступа. Обычно это критически важные данные, используемые в текущих вычислениях и аналитике. Применяется для реальных или почти реальных аналитических процессов, важных бизнес-приложений. Размещается на SSD (твердотельных накопителях) для обеспечения быстрого доступа.
- Теплое Хранилище (Warm Storage): Данные, к которым доступ необходим не так часто, как к горячим данным, но которые все еще важны для бизнеса и могут потребоваться для анализа в краткосрочной перспективе. Это данные, используемые для еженедельных отчетов, архивы последних месяцев. Размещаются на обычных жестких дисках с достаточной производительностью для обеспечения приемлемого времени доступа.
- Холодное Хранилище (Cold Storage): Редко используемые данные, которые необходимо сохранять из-за юридических требований или для исторических исследований. Доступ к этим данным не требует высокой скорости. Это архивы, логи старых транзакций, исторические данные. Размещаются на экономичных хранилищах с большой емкостью, например, на ленточных накопителях или дешевых жестких дисках.

<h4>Политики хранения</h4>

Политики хранения данных в HDFS позволяют администраторам кластера управлять распределением данных между различными типами хранилищ на основе требований к доступу и производительности. Эти политики определяют, какие данные должны быть перемещены на горячее, теплое или холодное хранилище.

Администраторы могут настроить политики хранения через HDFS Shell или через API, указывая, какие файлы или директории подлежат определенным политикам. Также можно настроить автоматическое перемещение данных между хранилищами на основе их возраста, частоты доступа или других критериев, определяемых политикой.

Эффективное использование типов хранилищ и политик хранения данных в HDFS позволяет организациям оптимизировать стоимость хранения данных и производительность доступа к ним, обеспечивая при этом соответствие требованиям бизнеса и регуляторным нормам.

Конфигурация политик хранения данных в HDFS позволяет администраторам управлять распределением и размещением данных в кластере на основе их важности, частоты доступа и требований к производительности. Это включает в себя возможность определения, какие данные должны быть размещены на горячем, теплом или холодном хранилище. Настройка политик в HDFS происходит следующим образом:

Встроенные политики хранения:
- `HOT`: Все реплики на DISK. (Цель: производительность)
- `WARM`: Одна реплика на DISK, остальные на ARCHIVE. Это компромисс: вы сохраняете быстрый доступ к одной копии данных, но все остальные копии хранятся дешево.
- `COLD`: Все реплики на ARCHIVE. (Цель: минимизация стоимости)
- `ALL_SSD`: Все реплики на SSD. (Цель: максимальная производительность)
- `ONE_SSD`: Одна реплика на SSD, остальные на DISK. Аналогично WARM, но для ускорения доступа к одной копии.
- `LAZY_PERSIST`: Одна реплика пишется в RAM_DISK, а затем асинхронно сбрасывается на DISK. Остальные реплики сразу пишутся на DISK. (Цель: максимальная скорость записи)

При политике `ALL_SSD` все реплики хранятся на SSD, что очень затратно по стоимости. В большинстве случаях будет достаточно использования `ONE_SSD` для компромисса между стоимостью и производительностью. Эта политика почти не уступает по скорости чтения `ALL_SSD`, но стоит значительно дешевле.

<h4>Управление политиками</h4>

Базовые команды:
```bash
# Доступные политики
hdfs storagepolicies -list

# Установка политики
hdfs storagepolicies -setStoragePolicy -path <путь> -policy <политика>

# Снятие политики (возврат к HOT)
hdfs storagepolicies -unsetStoragePolicy -path <путь>

# Принудительное применение политики
hdfs storagepolicies -satisfyStoragePolicy -path <путь>

# Проверка политики
hdfs storagepolicies  -getStoragePolicy -path <путь>
```

Продвинутые команды:
```bash
# Для установки политики на всю директорию рекурсивно
hdfs dfs -find /data -type d -exec hdfs storagepolicies -setStoragePolicy -policy WARM {} \;

# Проверка состояния перемещения данных
hdfs fsck /data/analytics -files -blocks -storagepolicies
```

<h3>5. Настройка HDFS</h3>
<h4>Файл core-site.xml</h4>

Этот файл содержит общие настройки для всего Hadoop кластера, включая настройки HDFS и других компонентов. Здесь определяются такие параметры, как URI файловой системы по умолчанию и другие основные свойства.

```xml
<configuration>
    <!-- URI файловой системы по умолчанию -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://namenode:8020</value>
        <description>URI файловой системы по умолчанию</description>
    </property>
    
    <!-- Временная директория для Hadoop -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/hadoop/tmp</value>
        <description>Базовая директория для временных файлов</description>
    </property>
</configuration>
```

<h4>Файл hdfs-site.xml</h4>

Этот файл содержит специфические для HDFS настройки, такие как параметры репликации, настройки NameNode и DataNode.

В hdfs-site.xml задаются параметры, которые управляют поведением HDFS. Некоторые важные настройки:
- dfs.replication: Определяет количество реплик для каждого блока данных. По умолчанию 3.
- dfs.blocksize: Размер блока данных. По умолчанию 128 МБ (в более новых версиях, в старых 64 МБ).
- dfs.namenode.name.dir: Путь на локальной файловой системе, где NameNode хранит метаданные (образ файловой системы и журналы транзакций).
- dfs.datanode.data.dir: Путь на локальной файловой системе, где DataNode хранит блоки данных.

Изменение настроек: необходимо отредактировать соответствующий XML-файл и перезапустить соответствующий демон (NameNode или DataNode) для применения изменений. Однако некоторые параметры можно изменить динамически без перезапуска, используя команды hdfs dfsadmin -refreshNodes или через web-интерфейс.

Настройки NameNode:
```xml
<!-- Директории для метаданных (рекомендуется несколько для резервирования) -->
<property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///opt/hadoop/namenode1,file:///opt/hadoop/namenode2</value>
</property>

<!-- Размер памяти для метаданных NameNode -->
<property>
    <name>dfs.namenode.handler.count</name>
    <value>100</value>
    <description>Количество потоков для обработки RPC-запросов</description>
</property>
```

Настройки DataNode:
```xml
<!-- Директории для данных (можно указать несколько) -->
<property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///opt/hadoop/datanode1,file:///opt/hadoop/datanode2</value>
</property>

<!-- Максимальный объем хранилища на DataNode -->
<property>
    <name>dfs.datanode.du.reserved</name>
    <value>10737418240</value>
    <description>Резервируемое место на диске (10GB)</description>
</property>
```

Настройки репикации и блоков:
```xml
<!-- Размер блока данных -->
<property>
    <name>dfs.blocksize</name>
    <value>134217728</value>
    <description>Размер блока 128MB в байтах</value>
</property>

<!-- Минимальная репликация -->
<property>
    <name>dfs.namenode.replication.min</name>
    <value>1</value>
</property>

<!-- Максимальная репликация -->
<property>
    <name>dfs.replication.max</name>
    <value>512</value>
</property>
```

<h4>Безопасный режим</h4>

Безопасный режим (Safe Mode) — это состояние NameNode, при котором файловая система доступна только для чтения. Запрещены любые операции модификации: создание, удаление, изменение файлов.

Назначение Safe Mode:
- Защита метаданных при запуске: NameNode загружает образ файловой системы (fsimage) и журналы редактирования (edits) и проверяет целостность метаданных
- Сбор отчетов от DataNodes: NameNode ожидает, пока DataNodes сообщат о состоянии своих блоков. Проверяет, какие блоки доступны, а какие отсутствуют или недореплицированы
- Восстановление репликации: Выявление недостающих или недореплицированных блоков, планирование восстановления репликации

Использование безопасного режима:
```bash
# Проверка статуса
hdfs dfsadmin -safemode get

# Вход в безопасный режим:
hdfs dfsadmin -safemode enter

# Выход из безопасного режима:
hdfs dfsadmin -safemode leave
```

<h4>Federation</h4>

Federation позволяет иметь несколько NameNode'ов, каждый из которых управляет своим собственным namespace (пространством имен). Это решает проблему масштабируемости и единой точки отказа. Каждый NameNode управляет различным набором блоков, причем DataNode могут хранить блоки различных наборов.

Настройка Federation:
```xml
<!-- hdfs-site.xml -->

<!-- Nameservice для федерации -->
<property>
    <name>dfs.nameservices</name>
    <value>ns1,ns2</value>
</property>

<!-- NameNode для ns1 -->
<property>
    <name>dfs.namenode.rpc-address.ns1.nn1</name>
    <value>namenode1:8020</value>
</property>
<property>
    <name>dfs.namenode.http-address.ns1.nn1</name>
    <value>namenode1:50070</value>
</property>

<!-- NameNode для ns2 -->
<property>
    <name>dfs.namenode.rpc-address.ns2.nn1</name>
    <value>namenode2:8020</value>
</property>
<property>
    <name>dfs.namenode.http-address.ns2.nn1</name>
    <value>namenode2:50070</value>
</property>
```

<h4>Rack Awareness</h4>

Rack Awareness - механизм, который позволяет HDFS знать о топологии сети (расположение узлов в стойках). Это важно для обеспечения отказоустойчивости и оптимизации сетевого трафика. Для настройки необходимо создать скрипт, возвращающий идентификатор стойки и указать его в core-site.xml.

Скрипт:
```bash
#!/bin/bash
# /opt/hadoop/etc/hadoop/topology.sh

# Аргумент - IP адрес или хостнейм DataNode
while [ $# -gt 0 ] ; do
    nodeArg=$1
    shift

    # Преобразование IP в хостнейм
    hostname=$(nslookup $nodeArg | grep "name =" | awk '{print $4}')
    hostname=${hostname%.}

    # Логика определения стойки
    # Пример: хосты rack1-host1, rack1-host2 -> /rack1
    if [[ $hostname =~ ^rack1-.* ]]; then
        echo "/rack1"
    elif [[ $hostname =~ ^rack2-.* ]]; then
        echo "/rack2"
    elif [[ $hostname =~ ^dc1-rack.* ]]; then
        # Для многоуровневой топологии
        rack=$(echo $hostname | cut -d'-' -f1,2)
        echo "/$rack"
    else
        # По умолчанию
        echo "/default-rack"
    fi
done
```

Настройка в core-site.xml:
```xml
<property>
    <name>net.topology.script.file.name</name>
    <value>/opt/hadoop/etc/hadoop/topology.sh</value>
</property>
<property>
    <name>net.topology.node.switch.mapping.impl</name>
    <value>org.apache.hadoop.net.ScriptBasedMapping</value>
</property>
```

<h4>Управление дисковым пространством</h4>

Мониторинг использования:
```bash
# Общий объем использования
hdfs dfs -df -h

# Детальный анализ по директориям
hdfs dfs -du -h /data

# Отчет DataNodes
hdfs dfsadmin -report

# Проверка квот
hdfs dfs -count -q /user/username
```

Установка квот:
```bash
# Установка квоты на пространство (1TB)
hdfs dfsadmin -setSpaceQuota 1t /user/username

# Установка квоты на количество файлов
hdfs dfsadmin -setQuota 10000 /user/username

# Снятие квот
hdfs dfsadmin -clrSpaceQuota /user/username
hdfs dfsadmin -clrQuota /user/username
```

<h3>6. Работа с HDFS</h3>
<h4>Работа с файлами и каталогами</h4>

Работа с каталогами:
```bash
# Создание каталога
hdfs dfs -mkdir /user/data
hdfs dfs -mkdir -p /user/data/raw  # Создание с родительскими каталогами

# Просмотр содержимого
hdfs dfs -ls /user/
hdfs dfs -ls -h /user/data  # Человеко-читаемый формат
hdfs dfs -ls -R /user/      # Рекурсивный обход

# Удаление
hdfs dfs -rmdir /user/empty_dir  # Удаление пустого каталога
hdfs dfs -rm -r /user/temp_data  # Рекурсивное удаление
```

Работа с файлами:
```bash
# Загрузка файлов
hdfs dfs -put local_file.txt /user/data/
hdfs dfs -copyFromLocal local_file.txt /user/data/  # Аналог put

# Скачивание файлов
hdfs dfs -get /user/data/file.txt ./local_copy.txt
hdfs dfs -copyToLocal /user/data/file.txt ./local/  # Аналог get

# Создание пустого файла
hdfs dfs -touchz /user/data/empty_file.txt
```

Поиск:
```bash
# Поиск файлов по имени (рекурсивно)
hdfs dfs -find /user/ -name "*.log"

# Поиск файлов по шаблону
hdfs dfs -find /user/data -name "data_*2024*.csv"

# Поиск каталогов
hdfs dfs -find /user/ -type d -name "temp*"

# Поиск файлов, измененных за последние N дней
hdfs dfs -find /user/logs/ -mtime -7    # Последние 7 дней
hdfs dfs -find /user/ -mtime +30        # Старше 30 дней

# Поиск файлов по размеру
hdfs dfs -find /user/ -size +100M  # Больше 100MB
hdfs dfs -find /user/ -size -10M   # Меньше 10MB

# Комбинированные условия
hdfs dfs -find /user/ -name "*.txt" -and -size +1M
```

<h4>Просмотр и управление содержимым файлов и каталогов</h4>

Просмотр содержимого:
```bash
# Просмотр файлов
hdfs dfs -cat /user/data/file.txt
hdfs dfs -tail /user/data/logfile.log  # Последние строки
hdfs dfs -tail -f /user/data/logfile.log  # Режим follow

# Постраничный просмотр
hdfs dfs -cat /user/data/large_file.txt | less

# Проверка размера
hdfs dfs -du -h /user/data/  # Размер каталога (человеко-читаемый)
hdfs dfs -du -s -h /user/data/  # Суммарный размер
hdfs dfs -df -h /user/         # Свободное место в HDFS
```

Управление содержимым:
```bash
# Проверка существования
hdfs dfs -test -e /user/data/file.txt && echo "Файл существует"

# Подсчет строк в файле
hdfs dfs -cat /user/data/file.txt | wc -l

# Поиск в файлах
hdfs dfs -cat /user/data/*.log | grep "ERROR"
```

<h4>Копирование и перемещение</h4>

Копирование:
```bash
# Копирование файлов
hdfs dfs -cp /user/data/source.txt /user/data/backup/
hdfs dfs -cp /user/data/file1.txt /user/data/file2.txt /user/destination/

# Рекурсивное копирование каталогов
hdfs dfs -cp -r /user/source_dir/ /user/backup_dir/
```

Перемещение:
```bash
# Перемещение файлов
hdfs dfs -mv /user/data/old_name.txt /user/data/new_name.txt
hdfs dfs -mv /user/temp/file.txt /user/final/

# Перемещение между каталогами
hdfs dfs -mv /user/source/*.log /user/destination/logs/
```

Копирование между локальной ФС и HDFS:
```bash
# Из локальной в HDFS
hdfs dfs -put /local/path/*.csv /user/data/input/
hdfs dfs -copyFromLocal /local/file.json /user/data/

# Из HDFS в локальную
hdfs dfs -get /user/data/results/*.csv ./local_results/
hdfs dfs -copyToLocal /user/data/report.txt ./reports/
```

<h4>Изменение прав доступа</h4>

Базовые права:
```bash
# Просмотр текущих прав
hdfs dfs -ls -h /user/data/
# Вывод: -rw-r--r--  3 user group  1.2G 2024-01-15 10:30 file.txt

# Изменение прав (аналогично chmod в Linux)
hdfs dfs -chmod 755 /user/data/script.py
hdfs dfs -chmod u+x /user/data/executable.sh  # Добавить выполнение владельцу
hdfs dfs -chmod go-w /user/data/config.conf   # Запретить запись группе и другим

# Рекурсивное изменение прав
hdfs dfs -chmod -R 750 /user/private_data/
```

Управление владельцами и группами:
```bash
# Изменение владельца
hdfs dfs -chown newuser:newgroup /user/data/file.txt
hdfs dfs -chown -R hdfsuser:hdfsgroup /user/project/

# Изменение только группы
hdfs dfs -chgrp analytics /user/data/reports/
hdfs dfs -chgrp -R developers /user/source_code/
```

Специальные права и ACL:
```bash
# Установка sticky bit (только владелец может удалять)
hdfs dfs -chmod +t /user/shared/temp/

# Расширенные ACL (Access Control Lists)
hdfs dfs -setfacl -m user:john:r-x /user/data/sensitive/
hdfs dfs -setfacl -m group:readonly:r-- /user/data/reports/
hdfs dfs -getfacl /user/data/protected/  # Просмотр ACL

# Рекурсивное применение ACL
hdfs dfs -setfacl -R -m user:auditor:r-- /user/financial_data/
```

<h4>Изменение репликации и проверка состояния</h4>

Изменение коэффициента репликации:
```bash
# Изменить коэффициент репликации для файла
hdfs dfs -setrep 2 /user/data/important_file.txt

# Рекурсивно изменить для всей директории
hdfs dfs -setrep -R 3 /user/data/

# Проверить текущий коэффициент репликации
hdfs dfs -ls /user/data/important_file.txt
# В выводе смотрим третье число (количество реплик)
# -rw-r--r--   3 hdfsuser supergroup   123456 2024-01-15 10:30 /user/data/file.txt
```

Проверка состояния:
```bash
# Детальная проверка файла (fsck)
hdfs fsck /user/data/file.txt -files -blocks -locations

# Проверка всей файловой системы
hdfs fsck / -files -blocks -racks

# Проверка конкретной директории
hdfs fsck /user/data/ -files -blocks

# Поиск недостающих блоков
hdfs fsck / -list-corruptfileblocks

# Проверка с лечением
hdfs fsck / -move   # Переместить поврежденные блоки в /lost+found
hdfs fsck / -delete # Удалить поврежденные блоки
```

<h4>Выполнение операций от другого пользователя</h4>

```bash
# Через sudo (на edge-узле)
sudo -u hdfsuser hdfs dfs -ls /user/data/

# Подключиться к узлу и выполнить команду от другого пользователя
ssh datanode01.cluster.local "sudo -u hdfs hdfs dfsadmin -report"

# Или подключиться как целевой пользователь
ssh hdfs@namenode01.cluster.local "hdfs dfsadmin -report"
```

<h4>Объединение файлов и проверка целостности</h4>

Объединение файлов:
```bash
# Объединение файлов из HDFS в локальный файл
hdfs dfs -getmerge /user/data/part-* ./merged_file.txt

# Объединение с добавлением переносов строк
hdfs dfs -getmerge -nl /user/logs/day* ./all_logs.txt

# Объединение напрямую в HDFS (через временный файл)
hdfs dfs -getmerge /user/data/chunk* /tmp/merged.tmp
hdfs dfs -put /tmp/merged.tmp /user/data/final_merged.txt
hdfs dfs -rm /tmp/merged.tmp

# Объединение через pipe (для больших файлов)
hdfs dfs -cat /user/data/part-* | hdfs dfs -put - /user/data/combined.txt
```

Проверка целостности:
```bash
# Проверка контрольных сумм файла
hdfs dfs -checksum /user/data/file.parquet

# Сравнение контрольных сумм локального и HDFS файла
hdfs dfs -checksum /user/data/file.txt | awk '{print $3}'
md5sum local_file.txt

# Проверка целостности блоков
hdfs fsck /user/data/ -blocks -locations

# Проверка на наличие поврежденных блоков
hdfs fsck / -list-corruptfileblocks

# Мониторинг целостности в реальном времени
hdfs fsck / -files -blocks | grep -i "corrupt\|missing"

# Восстановление поврежденных блоков
hdfs debug recoverLease -path /user/data/corrupted_file -retries 3
```

<h4>Использование дискового пространства</h4>

Базовые команды:
```bash
# Общее использование для пути (в байтах)
hdfs dfs -du /user/data/

# Суммарное использование директории
hdfs dfs -du -s /user/data/

# Человеко-читаемый формат
hdfs dfs -du -h /user/data/

# Суммарно в человеко-читаемом формате
hdfs dfs -du -s -h /user/data/

# Рекурсивный просмотр с сортировкой по размеру
hdfs dfs -du -h /user/ | sort -hr
```

Анализ свободного места:
```bash
# Общая статистика по файловой системе
hdfs dfs -df
hdfs dfs -df -h  # Человеко-читаемый формат

# Детальный отчет по всем DataNodes
hdfs dfsadmin -report

# Статистика использования по узлам
hdfs dfsadmin -report | grep -A 5 "Configured Capacity"

# Проверка квот
hdfs dfs -count -q /user/data/
```