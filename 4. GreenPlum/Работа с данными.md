<h2>Работа с данными</h2>
<h3>1. Загрузка данных</h3>
<h4>Основы</h4>

Greenplum поддерживает различные форматы файлов для загрузки данных, включая:
- Текстовые форматы (CSV, TSV) с настраиваемыми разделителями.
- Двоичный формат (не рекомендуется для внешних данных, но используется внутри).
- Пользовательские форматы (например, с использованием пользовательских функций для чтения/записи).

Существует два основных подхода к загрузке данных, которые определяются объемом данных и требованиями к производительности:
- Последовательная загрузка (Через Master-узел): Данные проходят через Master-узел, который их парсит, распределяет по сегментам и отправляет им для вставки. Преимуществом является простота использования, подходит для небольших объемов данных, однако Master становится "бутылочным горлышком" и загрузка больших объемов данных происходит медленно, так как весь трафик идет через один узел. Примеры: Команды `COPY` и `INSERT`.
- Параллельная загрузка (напрямую в сегменты): Данные загружаются параллельно и напрямую на все сегменты базы, минуя Master. Это достигается с помощью внешних таблиц и утилиты `gpfdist`. Преимуществом является высокая производительность и масштабируемость, что идеально для загрузки больших объемов данных, однако данный способ требует дополнительной настройки (запуск службы `gpfdist`).

Команда `COPY`:
```sql
-- Загрузка из файла на сервере Master
COPY my_table FROM '/path/to/local/file.csv' WITH (FORMAT CSV, DELIMITER '|', HEADER);
-- Выгрузка в файл
COPY my_table TO '/path/to/output/file.csv' WITH (FORMAT CSV, DELIMITER '|', HEADER);
```
Инструменты загрузки данных:
- `gpfdist` и `gpload`:
  - `gpfdist`: Это утилита, которая служит в качестве высокопроизводительного сервера распределенных данных. Она читает внешние файлы и предоставляет их всем сегментам Greenplum параллельно. Обычно используется вместе с внешними таблицами.
  - `gpload`: Это утилита, которая использует `gpfdist` и конфигурационный файл YAML для загрузки данных. Она управляет созданием внешних таблиц и запуском операции загрузки.
- `COPY` (SQL команда): Команда `COPY` может использоваться для загрузки данных из файлов, находящихся на сервере Greenplum (на мастере или сегментах). Однако, для больших объемов данных предпочтительнее использовать `gpfdist`, потому что он обеспечивает параллельную загрузку.
- Внешние таблицы: Вы можете создать внешнюю таблицу, которая ссылается на внешние данные (например, файлы на диске или в Hadoop). При запросе к внешней таблице данные считываются на лету. Для загрузки данных во внутреннюю таблицу можно использовать `INSERT INTO ... SELECT * FROM external_table`.
- `INSERT` и `SELECT`: Обычные SQL-команды `INSERT` и `SELECT` могут использоваться для загрузки данных из одной таблицы в другую, в том числе из внешних таблиц.
- Утилиты ETL: Greenplum может интегрироваться с ETL-инструментами, такими как Apache NiFi, Talend, Pentaho, и другими, которые используют JDBC или ODBC драйверы для загрузки данных.
- psql команда `\copy`: Утилита psql имеет команду `\copy`, которая работает аналогично `COPY`, но данные читаются/записываются на клиентской машине, а не на сервере. Это удобно для небольших объемов данных.
- Инструменты облачной интеграции: Если Greenplum развернут в облаке, можно использовать облачные инструменты, например, AWS S3, Google Cloud Storage, и загружать данные из них с помощью внешних таблиц.

Для проверки статуса загрузки можно использовать:
- Системные представления, такие как `pg_stat_activity` для наблюдения за активными операциями.
- Таблицы ошибок, если загрузка использует механизм отлова ошибок (например, `LOG ERRORS` в `COPY`).
- Логи сервера (файлы логов на мастер-сегменте и сегментах).
- Утилиты мониторинга, такие как `gp_toolkit`, которая предоставляет различные представления для мониторинга состояния кластера.

Методы оптимизации загрузки:
- Отключение индексов и ограничений: Перед загрузкой большого объема данных рекомендуется удалить индексы и ограничения (`UNIQUE`, `PRIMARY KEY`), так как их поддержка во время вставки очень дорогая. После загрузки индексы пересоздаются.
- Использование таблиц с `APPENDOPTIMIZED=TRUE` (AO): Таблицы, оптимизированные на добавление, работают значительно лучше при пакетной загрузке, чем обычные heap-таблицы. Они также лучше сжимаются.
- Сжатие: Использование сжатия (`COMPRESSTYPE=zstd`) для AO-таблиц экономит место на диске и уменьшает время чтения/записи.
- Бatch-размер: При использовании стандартных инструментов вроде `COPY` можно настроить размер пачки для вставки.
- При загрузке JSON использовать текстовый или JSONB формат, а после загрузки уже производить парсинг

<h4>gpfdist</h4>

`gpfdist` — это утилита, которая служит в качестве высокопроизводительного сервера распределенных данных. Она позволяет сегментам Greenplum параллельно загружать или выгружать данные, что значительно ускоряет процесс по сравнению с загрузкой через мастер-узел. `gpfdist` поставляется в составе Greenplum Database и находится в каталоге `$GPHOME/bin`. Убедитесь, что этот каталог находится в переменной PATH.

Базовый запуск:
```bash
gpfdist -d /path/to/data -p 8081 -l /home/gpadmin/logs/gpfdist.log &
```
- `-d` — каталог с данными (корневой каталог, из которого gpfdist будет обслуживать файлы)
- `-p` — порт (по умолчанию 8080)
- `-l` — файл лога
- `-b` - адрес (опционально)
- `-m` - размер блока (в байтах)

Можно запустить несколько экземпляров gpfdist на разных портах для увеличения производительности и использовать их все в одной внешней таблице:
```bash
gpfdist -d /path/to/data -p 8081 -l /home/gpadmin/logs/gpfdist1.log &
gpfdist -d /path/to/data -p 8082 -l /home/gpadmin/logs/gpfdist2.log &
```

Для постоянной работы можно использовать `nohup`:
```bash
nohup gpfdist -d /path/to/data -p 8081 -l /home/gpadmin/logs/gpfdist.log > /dev/null 2>&1 &
```

Создание внешней таблицы для загрузки данных:
```sql
CREATE EXTERNAL TABLE ext_table (id int, name text) 
LOCATION ('gpfdist://datahost:8081/data.txt',
          'gpfdist://datahost:8082/data.txt')
FORMAT 'TEXT' (DELIMITER '|');
```
`LOCATION` — указывает на URL `gpfdist`. Можно указать несколько URL для балансировки нагрузки.
`FORMAT` — определяет формат файла (TEXT или CSV) и дополнительные параметры (разделитель, наличие заголовка и т.д.)

Создание внешней таблицы для выгрузки данных:
```sql
CREATE WRITABLE EXTERNAL TABLE ext_output (id int, name text)
LOCATION ('gpfdist://datahost:8081/output/')
FORMAT 'TEXT' (DELIMITER '|')
DISTRIBUTED BY (id);
```

Процесс загрузки/выгрузки данных:
1. Запуск `gpfdist` на хосте, где находятся данные (или куда нужно выгрузить)
2. Создание внешней таблицы для загрузки или выгрузки данных, указывающую на целевую директорию
3. Вставка данных в целевую таблицу из внешней (наоборот для выгрузки)

<h4>gpload</h4>

`gpload` — это утилита, которая использует YAML-конфигурацию для определения процесса загрузки. Она управляет запуском gpfdist (если необходимо) и выполняет загрузку через внешние таблицы. Запуск: `gpload -f config.yml`. `gpload` автоматически создает внешнюю таблицу для чтения данных, а затем загружает их в целевую таблицу. Она также может выполнять предзагрузочные операции (например, `TRUNCATE`) и обрабатывать ошибки.

Пример конфигурационного файла для `gpload` (`config.yml`):

```yaml
VERSION: 1.0.0.1
DATABASE: my_database
USER: my_user
HOST: master_host
PORT: 5432
GPLOAD:
  INPUT:
    - SOURCE:
        LOCAL_HOSTNAME:
          - host1
        PORT: 8080
        FILE:
          - /path/to/input_file.csv
    - FORMAT: csv
    - DELIMITER: ','
    - HEADER: true
    - ERROR_LIMIT: 100
    - LOG_ERRORS: true
  OUTPUT:
    - TABLE: public.target_table
    - MODE: INSERT
  PRELOAD:
    - TRUNCATE: true
```

<h4>Транзакционная целостность и внешние источники</h4>

Greenplum обеспечивает транзакционную целостность (ACID) для операций DML в обычных таблицах. Однако, при работе с внешними таблицами (читаемыми) данные не хранятся в Greenplum и не являются частью транзакций. То есть, если вы читаете из внешней таблицы, то данные, которые вы читаете, могут измениться в источнике, и это не управляется транзакциями Greenplum.

Для записываемых внешних таблиц (writable external tables) операция записи также не является транзакционной. Если операция записи во внешнюю таблицу прерывается (например, из-за сбоя), то часть данных может быть уже записана в целевой файл, а часть — нет. Если вставлять данные из внешней таблицы в обычную таблицу внутри транзакции, то вставка в обычную таблицу будет транзакционной. То есть, если транзакция откатывается, то данные из внешней таблицы не будут вставлены в целевую таблицу. Однако, если в процессе чтения из внешнего источника произойдет ошибка, то транзакция может быть прервана, и тогда все изменения будут откачены.

Внешние таблицы не участвуют в транзакциях как хранимые данные, но операции, которые используют внешние таблицы (например, `SELECT ... INTO или INSERT ... SELECT`) выполняются в транзакции, если они вызваны в блоке транзакции. Пример:
```sql
BEGIN;
INSERT INTO internal_table SELECT * FROM external_table;
COMMIT;
```

В этом случае, если внешний источник изменится во время транзакции, то это может повлиять на повторяемость чтения. Поэтому для обеспечения согласованности данных при чтении из внешних источников, которые могут изменяться, рекомендуется использовать снимки данных (snapshots) или блокировки на стороне источника, если это возможно.

<h4>Партицирование и инкрементальная загрузка</h4>

Сочетание партиционирования и внешних таблиц — идеальная основа для инкрементальной загрузки:
- Стратегия "Партиция на день": Данные за каждый день загружаются в отдельную партицию.
  - Создайте внешнюю таблицу на новые файлы (например, `sales_20231027.csv`).
  - Используйте `INSERT INTO target_table SELECT * FROM ext_table`, и данные автоматически попадут в нужную партицию, если она существует.
  - Если партиции для новой даты нет, ее нужно создать заранее с помощью `ALTER TABLE ... ADD PARTITION`.
- Стратегия "Обмен партициями" (Partition Exchange): Cамый эффективный метод для минимального времени простоя. Основная таблица блокируется на очень короткое время (только на время обмена), а не на все время загрузки данных:
  - Создайте отдельную таблицу (`staging_table`) с той же структурой, что и партиция.
  - Загрузите данные в `staging_table` (можно использовать все методы параллельной загрузки).
  - Построить индексы и проверить данные в `staging_table`.
  - Обменяйте `staging_table` на пустую партицию в основной таблице с помощью одной атомарной операции `ALTER TABLE ... EXCHANGE PARTITION`.
- Использование `MERGE` (`UPSERT`): Если данные не только добавляются, но и обновляются, используется операция `MERGE` (или `INSERT ... ON CONFLICT UPDATE` в стиле PostgreSQL). `MERGE` в Greenplum может быть не таким эффективным, как обмен партициями, так как он выполняет построчную проверку. Его стоит использовать для небольших объемов изменяющихся данных.

Итоговый пример пайплайна инкрементальной загрузки:
1. Новые данные поступают в файл `incremental_YYYYMMDD.csv`.
2. Запускается `gpfdist` на этом файле.
3. Создается внешняя таблица `ext_incremental`.
4. Создается промежуточная таблица `staging_YYYYMMDD` (как `APPENDOPTIMIZED`).
5. Данные загружаются в нее: `INSERT INTO staging_YYYYMMDD SELECT * FROM ext_incremental`.
6. Выполняется `ALTER TABLE main_table EXCHANGE PARTITION for_date_YYYYMMDD WITH TABLE staging_YYYYMMDD`.
7. Старая партиция архивируется или удаляется.

<h4>Управление нагрузкой, ресурсные квоты и безопасность</h4>

В Greenplum управление нагрузкой осуществляется для того, чтобы запросы не конкурировали за ресурсы и система оставалась отзывчивой.

Ресурсные очереди позволяют ограничивать количество одновременно выполняемых запросов для пользователя или группу пользователей, а также устанавливать лимиты на стоимость запросов (в единицах, основанных на параметре `gp_vmem_protect_limit`).

Типы ограничений:
- `ACTIVE_STATEMENTS`: максимальное количество одновременно выполняемых запросов в очереди.
- `MAX_COST`: максимальная общая стоимость запросов, которые могут выполняться одновременно.
- `MIN_COST`: запросы с стоимостью ниже этого значения не учитываются в лимитах.
- `PRIORITY`: приоритет очереди (от `LOW` до `HIGH`), который влияет на распределение ресурсов CPU.
- `MEMORY_LIMIT`: максимальный объем памяти, который может использовать очередь.

Пример создания и назначения очереди пользователю:
```sql
CREATE RESOURCE QUEUE etl_queue WITH (ACTIVE_STATEMENTS=5, PRIORITY=HIGH);
ALTER ROLE etl_user RESOURCE QUEUE etl_queue;
```

Greenplum настраивает память на двух уровнях: на уровне сегмента и на уровне оператора. Параметры:
- `gp_vmem_protect_limit`: максимальный объем памяти, который может использовать один сегмент. Запросы, превышающие этот лимит, будут отменены.
- `statement_mem`: объем памяти, выделяемый для одного запроса на сегменте. Может быть установлен на уровне сессии или для конкретного запроса.
- `gp_workfile_limit_files_per_query`: ограничивает количество временных файлов, которые может создать запрос, что помогает контролировать "раздувание" запросов.

Начиная с версии 5.x, в Greenplum представлены Resource Groups как более современная альтернатива Resource Queues. Resource Groups позволяют управлять ресурсами (CPU, памятью) на уровне cgroups (требуется настройка на уровне ОС). Преимущества: более точное управление CPU и памятью, изоляция workload.

Пример создания группы и назначения ее пользователю:
```sql
CREATE RESOURCE GROUP etl_group WITH (CPU_RATE_LIMIT=50, MEMORY_LIMIT=50);
ALTER ROLE etl_user RESOURCE GROUP etl_group;
```
Обеспечение безопасности при загрузке:
- Аутентификация и авторизация:
  - Настройка pg_hba.conf для контроля доступа по IP и методу аутентификации.
  - Использование ролей и привилегий (GRANT/REVOKE) для предоставления минимально необходимых прав.
- Шифрование:
  - Шифрование на уровне приложения: данные шифруются до загрузки в БД.
  - Шифрование на уровне базы данных: использование функций шифрования (например, `pgcrypto`).
  - Шифрование на уровне диска: использование прозрачного шифрования диска (TDE) или шифрованных файловых систем.
- Маскирование данных: Использование представлений (VIEWs) для скрытия чувствительных данных от пользователей и динамическое маскирование с помощью функций.
- Аудит: Включение аудита с помощью `pg_audit` или использования триггеров для логирования изменений. Мониторинг попыток доступа и выполнения DDL/DML.
- Безопасность при загрузке:
  - Проверка данных на этапе загрузки (например, с помощью внешних таблиц с опцией `LOG ERRORS`).
  - Использование форматов с строгой структурой (например, Avro) для избежания инъекций.
  - Ограничение доступа к внешним таблицам и утилитам загрузки (`gpfdist`, `gpload`) только для доверенных пользователей.
- Сетевые настройки: Использование SSL для шифрования соединений между клиентами и Greenplum, а также между узлами кластера. Изоляция сети, где работает Greenplum, и использование VPN для доступа.
- Резервное копирование и восстановление: Регулярное использование `gpbackup` и `gprestore` для создания резервных копий и обеспечения точки восстановления.
- Обновления и исправления: Своевременное применение исправлений безопасности для Greenplum и операционной системы.

<h3>2. Подготовка данных</h3>
<h4>Основы ETL и ELT</h4>

Ключевое различие между ETL и ELT заключается в том, где происходит трансформация данных.

ETL (Extract, Transform, Load): Классический подход:
1. Extract (Извлечение): Данные извлекаются из систем-источников (базы данных, файлы, API и т.д.).
2. Transform (Трансформация): Данные очищаются, обогащаются, агрегируются и преобразуются вне целевой базы данных, часто в специальном ETL-инструменте (например, Informatica, Talend, Airflow с Python-скриптами).
3. Load (Загрузка): Готовые, преобразованные данные загружаются в целевую систему (в нашем случае — в Greenplum).

ETL cнижает нагрузку на целевую СУБД, что подходит для сложной бизнес-логики, которую трудно выразить SQL, но не использует главное преимущество Greenplum — ее распределенную вычислительную мощность для обработки больших объемов данных. Трансформация происходит в одном узле ETL-инструмента.

ELT (Extract, Load, Transform): Современный подход для больших данных и DWH.
1. Extract (Извлечение): Данные извлекаются из систем-источников.
2. Load (Загрузка): Сырые данные как можно быстрее загружаются в Greenplum, часто в staging-таблицы.
3. Transform (Трансформация): Все преобразования выполняются внутри Greenplum с помощью SQL

Почему ELT идеально подходит для Greenplum:
- Масштабируемость: Greenplum распределяет нагрузку трансформации по всем своим сегментам. Запрос на преобразование 1 ТБ данных будет выполняться параллельно на десятках узлов.
- Производительность: Используется оптимизированный SQL-движок и column-oriented хранилище (если используется).
- Гибкость: Легко изменить логику трансформации с помощью SQL и переначать данные, так как "сырые" данные уже внутри.

Для Greenplum ELT является предпочтительной архитектурой. Вы загружаете сырые данные через утилиты вроде `gpfdist` или `COPY`, а всю сложную логику реализуете на SQL, используя распределенную мощь кластера.

<h4>Трансформация данных</h4>

Поскольку в парадигме ELT трансформация происходит внутри СУБД, основным инструментом является SQL. Вот ключевые методы и подходы:
- CREATE TABLE AS (CTAS): Самый распространенный способ. Вы создаете новую таблицу как результат сложного SELECT-запроса:
```sql
CREATE TABLE fact_sales_daily AS
SELECT
    date_trunc('day', sale_time) as sale_date,
    product_id,
    SUM(amount) as total_amount,
    COUNT(*) as transaction_count
FROM stg_raw_sales
GROUP BY sale_date, product_id
DISTRIBUTED BY (sale_date, product_id);
```

- INSERT / UPDATE / DELETE (DML): Для инкрементального обновления данных.
```sql
# Инкрементальное обновление витрины
INSERT INTO fact_sales_daily
SELECT ... FROM stg_raw_sales_incremental s
WHERE s.sale_time > (SELECT MAX(sale_date) FROM fact_sales_daily);
# Очистка данных (например, удаление дубликатов)
DELETE FROM your_table
WHERE ctid NOT IN (
    SELECT min(ctid)
    FROM your_table
    GROUP BY all_columns_except_ctid
);
```

- Оконные функции (Window Functions): Мощный инструмент для сложных расчетов без группировки.
```sql
# Расчет скользящего среднего и ранжирование
SELECT
    customer_id,
    sale_time,
    amount,
    AVG(amount) OVER (PARTITION BY customer_id ORDER BY sale_time ROWS 5 PRECEDING) as avg_5_last,
    ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY sale_time DESC) as last_transaction_rank
FROM stg_raw_sales;
```

<h4>Инструменты ETL и оптимизация</h4>

Инструменты ETL для Greenplum:
- Встроенные утилиты Greenplum:
  - `gpfdist`: это высокопроизводительный сервер файлов, который обслуживает внешние таблицы. Он позволяет параллельно загружать данные из нескольких файлов в сегменты Greenplum.
  - `gpload`: обертка над gpfdist, которая использует YAML-конфигурацию для определения процесса загрузки, упрощая использование.
  - `COPY`: стандартная команда PostgreSQL, которая может использоваться для загрузки данных, но без параллелизма gpfdist.
- Инструменты управления рабочими процессами:
  - Apache Airflow: популярный оркестратор, который можно использовать для планирования и мониторинга ETL-задач Greenplum. Есть операторы для выполнения SQL в Greenplum и использования gpfdist.
  - Pentaho, Talend, Informatica: коммерческие ETL-инструменты, которые имеют коннекторы к Greenplum и поддерживают параллельную загрузку.

Оптимизация процесса ETL и обработки больших объемов данных:
- Использование ELT-подхода: загружайте сырые данные в staging-таблицы, а затем используйте мощь SQL Greenplum для трансформаций.
- Параллельная загрузка с `gpfdist`: запуск нескольких экземпляров `gpfdist` для обслуживания файлов, что позволяет загружать данные несколькими потоками.
- Настройка внешних таблиц: использование внешних таблиц с правильными параметрами (например, `LOG ERRORS INTO error_table` для обработки ошибок, `SEGMENT REJECT LIMIT` для ограничения отклоненных записей).
- Партиционирование таблиц: создание таблиц с партициями по дате или другому ключу, чтобы можно было загружать данные в конкретные партиции и быстро удалять старые данные.
- Индексы: часто индексы замедляют загрузку, поэтому их можно удалять перед загрузкой и создавать после. Однако в Greenplum индексы не всегда необходимы для запросов, так как полное сканирование может быть быстрее благодаря параллелизму.
- Настройка параметров Greenplum: увеличение `max_connections`, `shared_buffers`, `work_mem` и др. для обработки больших объемов.
- Использование колоночного хранения: для аналитических запросов, которые читают много столбцов, но не все строки, колоночное хранение (с помощью `WITH (APPENDONLY=true, ORIENTATION=column)`) может значительно ускорить выполнение запросов.
- Сжатие данных: использование сжатия (например, `COMPRESSTYPE=zlib`) для уменьшения объема хранимых данных и ускорения I/O.

Greenplum автоматически распределяет обработку запросов по сегментам. При работе с внешними таблицами, каждый сегмент может читать/писать свою часть данных, если используется несколько gpfdist-серверов или один, но с несколькими файлами:
Используйте несколько экземпляров `gpfdist` для балансировки нагрузки и увеличения пропускной способности.
Разбивайте большие файлы на несколько меньших, чтобы каждый сегмент мог обрабатывать свой файл параллельно.
Для выгрузки данных используйте `WRITABLE EXTERNAL TABLE с gpfdist`, чтобы данные выгружались параллельно.

<h4>Мониторинг производительности ETL-процессов</h4>

Аудит производительности ETL-процессов в Greenplum включает мониторинг и анализ различных метрик для выявления узких мест и оптимизации.

Основные метрики для аудита:
- Временные метрики:
  - Общее время выполнения ETL-процесса
  - Время загрузки данных (извлечение и загрузка)
  - Время трансформации данных (выполнение SQL-запросов)
  - Время на отдельных этапах (например, загрузка в staging, преобразования, обновление витрин)
- Метрики использования ресурсов:
  - Загрузка CPU на мастер-узле и сегментах
  - Использование памяти (RAM)
  - Дисковый I/O (чтение/запись)
  - Сетевая нагрузка (между сегментами и при загрузке данных)
- Метрики данных:
  - Объем обрабатываемых данных (строки, байты)
  - Скорость обработки (строк/сек, МБ/сек)
  - Количество ошибок (отклоненные строки, ошибки преобразования)

Инструменты для аудита:
- Системные представления Greenplum:
  - `pg_stat_activity` - текущие активные запросы
  - `pg_stat_all_tables` - статистика по таблицам (количество последовательных сканирований, индексных сканирований, обновлений)
  - `gp_toolkit` - набор представлений для мониторинга и диагностики
- Мониторинг производительности загрузки данных:
  - Использование `gpfdist` и `gpload` (логи, количество обработанных строк, ошибки)
  - Анализ планов выполнения запросов (`EXPLAIN ANALYZE`)

<h4>Комплексная обработка данных и динамическое распределение ресурсов</h4>

В Greenplum часто используется комбинация ETL и ELT:
- ETL для первоначальной загрузки и очистки данных (например, с помощью внешних инструментов)
- ELT для сложных преобразований внутри Greenplum, использующих распределенные вычисления.

Пример гибридного подхода:
1. Extract (E): Извлечение данных из источников (файлы, базы данных, API) с помощью инструментов (Airflow, NiFi) или встроенных утилит (gpfdist, PXF).
2. Transform (T): Предварительная очистка и валидация данных во временной зоне (staging) с помощью ETL-инструмента или SQL в Greenplum.
3. Load (L): Загрузка подготовленных данных в staging-таблицы Greenplum.
4. Transform (T): Дальнейшая трансформация средствами Greenplum (SQL, UDF) с использованием распределенных вычислений (сложные JOIN, агрегации, оконные функции).
5. Load (L): Загрузка окончательных результатов в целевую витрину или таблицу фактов.

Для динамического распределения ресурсов используются ресурсные группы и очереди, а также настройки параметров `gp_vmem_protect_limit`, `statement_mem` для контроля использования памяти.

<h3>3. Обработка данных с помощью Spark</h3>
<h4>Использование Spark</h4>

Для подключения к Greenplum из Spark нужно использовать JDBC драйвер. Необходимо скачать JDBC драйвер для Greenplum (это тот же драйвер, что и для PostgreSQL, так как Greenplum основан на PostgreSQL). Убедиться, что драйвер (например, `postgresql-42.2.5.jar`) доступен для Spark. Можно указать путь к драйверу при запуске приложения с помощью `--jars`.

Пример подключения:
```python
# Создаем SparkSession
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("GreenplumIntegration") \
    .config("spark.jars", "/path/to/postgresql-42.2.5.jar") \
    .getOrCreate()

# Настройки подключения к Greenplum
jdbc_url = "jdbc:postgresql://greenplum_host:5432/database_name"
connection_properties = {
    "user": "username",
    "password": "password",
    "driver": "org.postgresql.Driver"
}

# Чтение таблицы из Greenplum
df = spark.read.jdbc(url=jdbc_url, table="table_name", properties=connection_properties)
```

После загрузки данных в DataFrame можно использовать Spark SQL для выполнения запросов:
```python
# Создание временного представления для DataFrame
df.createOrReplaceTempView("my_table")

# Выполнение SQL запроса
result_df = spark.sql("SELECT * FROM my_table WHERE column1 > 100")

# Можно также записывать данные обратно в Greenplum
result_df.write.jdbc(url=jdbc_url, table="new_table", mode="overwrite", properties=connection_properties)
```

Преимущества использования Spark:
- Распределенная обработка: Spark может обрабатывать большие объемы данных, распределяя их across a cluster.
- Скорость: Благодаря in-memory вычислениям Spark может быть значительно быстрее, чем традиционные подходы, особенно для итеративных алгоритмов.
- Поддержка различных источников данных: Spark может работать с различными источниками, включая Greenplum, через JDBC.
- Богатый набор библиотек: Spark предоставляет библиотеки для SQL (Spark SQL), машинного обучения (MLlib), потоковой обработки (Structured Streaming) и графов (GraphX).
- Гибкость: Поддержка нескольких языков (Scala, Java, Python, R) и возможность запуска в различных средах (standalone, YARN, Kubernetes).

<h4>Работа с DataFrame</h4>

Базовые операции:
```python
# Просмотр схемы
df.printSchema()

# Выборка и фильтрация
from pyspark.sql import functions as F

result_df = df.select(F.col("id"), F.col("name"), F.col("amount")) \
.filter(F.col("amount") > 1000) \
.filter(F.col("category").isin("A", "B"))

# Агрегации
aggregated_df = df.groupBy("category") \
    .agg(
        F.sum("amount").alias("total_amount"),
        F.avg("amount").alias("avg_amount"),
        F.count("*").alias("record_count")
    )

# JOIN операция
dimension_df = spark.read.jdbc(url=gp_url, table="dimension_table",  properties=gp_properties)
joined_df = result_df.join(dimension_df, result_df.category == dimension_df.category_code, "left")
```

Оконные функции:
```python
from pyspark.sql.window import Window

window_spec = Window.partitionBy("customer_id").orderBy("transaction_date")

windowed_df = df \
.withColumn("running_total", F.sum("amount").over(window_spec)) \
.withColumn("transaction_rank",  F.row_number().over(window_spec))
```

Обработка дат и строк:
```python
# Работа с датами
date_processed_df = df \
.withColumn("year_month", F.date_format(F.col("transaction_date"), "yyyy-MM")) \
.withColumn("days_since_transaction", F.datediff(F.current_date(), F.col("transaction_date")))

# Работа со строками
string_processed_df = df \
.withColumn("name_upper", F.upper(F.col("name"))) \
.withColumn("email_domain", F.split(F.col("email"), "@").getItem(1))
```

<h4>Создание пользовательских агрегатных функций</h4>

```python
from pyspark.sql.types import StructType, StructField, DoubleType, StringType
from pyspark.sql.functions import pandas_udf, PandasUDFType

# Пример 1: Медиана через Pandas UDF
@pandas_udf(DoubleType(), PandasUDFType.GROUPED_AGG)
def pandas_median(series):
    return series.median()

# Использование pandas UDF для сложных агрегаций
@pandas_udf("double", PandasUDFType.GROUPED_MAP)
def calculate_complex_metric(pdf):
    # pdf - pandas DataFrame для каждой группы
    result = {}
    result['custom_metric'] = (pdf['value1'].mean() * pdf['value2'].std()) / len(pdf)
    return pd.DataFrame([result])

# Применение UDAF
result_with_custom = df.groupBy("group_col").agg(
    F.avg("amount").alias("average"),
    pandas_median("amount").alias("median"),  # Наша кастомная функция
    F.count("*").alias("count")
)
```

<h4>Параллельное чтение</h4>

Для параллельного чтения данных из Greenplum в Spark мы можем использовать механизм разделения данных на партиции. Это позволяет нескольким исполнителям Spark одновременно читать различные части данных.

Ключевые параметры для параллельного чтения через JDBC:
1. `partitionColumn`: столбец, по которому будет происходить партиционирование. Обычно это числовой столбец с равномерно распределенными значениями (например, первичный ключ).
2. `lowerBound`: минимальное значение `partitionColumn` (для определения диапазона).
3. `upperBound`: максимальное значение `partitionColumn`.
4. `numPartitions`: количество партиций (задает степень параллелизма).

Пример:
```python
df = spark.read \
    .format("jdbc") \
    .option("url", jdbc_url) \
    .option("dbtable", "schema_name.table_name") \
    .option("user", username) \
    .option("password", password) \
    .option("driver", "org.postgresql.Driver") \
    .option("partitionColumn", "id") \
    .option("lowerBound", 1) \
    .option("upperBound", 1000000) \
    .option("numPartitions", 10) \
    .load()
```
Важно:
- Выбранный столбец для партиционирования должен быть числовым и иметь индекс для ускорения запросов.
- Значения `lowerBound` и `upperBound` должны быть выбраны правильно, чтобы каждая партиция имела примерно одинаковое количество строк.

<h4>Оптимизация чтения</h4>

Для оптимизации чтения больших объемов данных можно использовать следующие подходы:
- Использование предикатов для фильтрации на стороне БД: Мы можем передать в запрос условия, которые уменьшат объем данных, загружаемых в Spark. Пример с использованием условия в запросе:
```python
query = "(SELECT * FROM large_table WHERE date_column >= '2020-01-01') AS tmp"
df = spark.read \
    .format("jdbc") \
    .option("url", jdbc_url) \
    .option("query", query) \
    .option("user", username) \
    .option("password", password) \
    .option("driver", "org.postgresql.Driver") \
    .option("partitionColumn", "id") \
    .option("lowerBound", 1) \
    .option("upperBound", 1000000) \
    .option("numPartitions", 10) \
    .load()
```
- Выбор только необходимых столбцов:
```python
query = "(SELECT id, name, date FROM large_table) AS tmp"
```
- Настройка размера выборки (fetchsize) для уменьшения количества сетевых кругых поездок:
```python
.option("fetchsize", 10000)
```
- Использование сжатия при передаче данных:
```python
.option("compress", true)
```
- Настройка таймаутов и других параметров соединения
- Использование внешних таблиц Greenplum и форматов columnar (например, ORC, Parquet) для больших таблиц. Однако, при чтении через JDBC это не применимо, но можно рассмотреть экспорт данных в колоночный формат и чтение из него.

<h4>Оптимизация обработки</h4>

При обработке больших объемов данных в Spark важно следовать лучшим практикам:
- Распределение данных и партиционирование: Убедитесь, что данные распределены равномерно между партициями. Если нет, то может потребоваться репартиционирование.
- Избегание операций shuffle, когда это возможно.
- Использование кэширования для данных, которые используются многократно:
- Настройка конфигурации Spark для больших данных:
  - Увеличение памяти исполнителей (executor memory)
  - Настройка количества ядер и исполнителей
  - Настройка параметров garbage collection
- Использование аккуратного управления памятью:
  - Избегание коллекций в драйвере (collect) для больших данных.
  - Использование преобразований, которые не требуют хранения всех данных в памяти (например, использование оконных функций с условиями).
- Мониторинг и анализ плана выполнения запросов с помощью `df.explain()` для выявления узких мест.
- Использование форматов хранения, эффективных для больших данных (Parquet, ORC) для промежуточных данных.
- Если данные не помещаются в память, рекомендуется использование дисковых операций (настройка `spark.sql.adaptive.enabled` и `spark.sql.adaptive.coalescePartitions`).
- Для агрегаций, которые могут быть выполнены на стороне Greenplum, следует использовать предварительную агрегацию в Greenplum и затем чтение уже агрегированных данных.

Пример полного процесса:
```python
# Настройка SparkSession для больших данных
spark = SparkSession.builder \
    .appName("GreenplumLargeData") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.sql.adaptive.skew.enabled", "true") \
    .config("spark.executor.memory", "8g") \
    .config("spark.executor.cores", "4") \
    .config("spark.executor.instances", "10") \
    .config("spark.sql.shuffle.partitions", "200") \
    .getOrCreate()

# Параллельное чтение с фильтрацией
jdbc_url = "jdbc:postgresql://greenplum_host:5432/db"
connection_properties = {
    "user": "user",
    "password": "password",
    "driver": "org.postgresql.Driver",
    "fetchsize": "10000",
    "compress": "true"
}

df = spark.read
    .jdbc(url=jdbc_url,
          table="(SELECT * FROM large_table WHERE date_column >= '2020-01-01') AS tmp",
          column="id",
          lowerBound=1,
          upperBound=1000000,
          numPartitions=10,
          properties=connection_properties)

# Репартиционирование, если нужно
df = df.repartition(100, "some_column")

# Кэширование, если данные используются многократно
df.cache()

# Обработка данных
result = df.groupBy("some_column").agg({"value": "avg", "amount": "sum"})

# Запись результата
result.write.format("parquet").mode("overwrite").save("/path/to/output")

# Освобождение кэша
df.unpersist()
```

<h4>Особенности работы с транзакциями</h4>

Greenplum - это распределенная СУБД, основанная на PostgreSQL, но с некоторыми отличиями в поведении транзакций. При использовании Spark для чтения и записи в Greenplum важно понимать, как управлять транзакциями, чтобы обеспечить согласованность данных.
- Чтение: При чтении данных из Greenplum с помощью Spark, по умолчанию используется уровень изоляции `READ COMMITTED`. В Greenplum, как и в PostgreSQL, это означает, что каждое выражение видит только те данные, которые были зафиксированы до начала выполнения этого выражения. Однако, в распределенной системе Greenplum есть нюансы, связанные с тем, что данные распределены по сегментам.
- Запись: При записи данных в Greenplum через Spark (например, через JDBC) каждая запись выполняется в рамках транзакции. Важно учитывать, что Greenplum не поддерживает такие же уровни изоляции, как однопроцессные БД. Например, он не поддерживает `SERIALIZABLE` так, как это делается в PostgreSQL. Вместо этого Greenplum использует "Snapshot Isolation" для распределенных транзакций.
- Параллелизм: Greenplum предназначен для обработки больших объемов данных и параллельных операций. При использовании Spark с Greenplum, особенно при параллельной записи, могут возникать конфликты. Рекомендуется использовать таблицы без обновлений (append-only) или использовать стратегии обновления через промежуточные таблицы.
- Управление транзакциями в Spark: В Spark при использовании JDBC каждая операция записи (например, `df.write.jdbc`) по умолчанию выполняется в автоматическом режиме фиксации (auto-commit). Однако, если вы хотите выполнить несколько операций в одной транзакции, вам нужно использовать пользовательское соединение и управлять транзакцией вручную.

<h4>Использование Spark Streaming для обработки потоковых данных</h4>

Spark Streaming позволяет обрабатывать потоковые данные. Однако, Greenplum не является традиционным источником потоковых данных (как Kafka, например). Но мы можем использовать Spark Streaming для периодического опроса Greenplum на наличие новых данных.

Стратегия:
- Использование микро-батчей для чтения изменений из Greenplum.
- Определение способа идентификации новых записей (например, мониторинг по столбцу с временной меткой или по увеличивающемуся идентификатору).

Пример с использованием Spark Structured Streaming и JDBC:

```python
from pyspark.sql.streaming import DataStreamWriter, StreamingQuery
import pyspark.sql.types as T

class GreenplumStreamProcessor:
    def __init__(self, spark, gp_config):
        self.spark = spark
        self.gp_config = gp_config
        self.checkpoint_location = "/streaming/checkpoints"

    def create_streaming_source(self, table_name, timestamp_col):
        # Стратегия: polling изменений по временной метке
        initial_timestamp = "2024-01-01 00:00:00"

        streaming_df = self.spark.readStream \
            .format("jdbc") \
            .option("url", self.gp_config.url) \
            .option("dbtable", 
                   f"(SELECT * FROM {table_name} WHERE {timestamp_col} > ?) as incremental_data") \
            .option("partitionColumn", timestamp_col) \
            .option("lowerBound", initial_timestamp) \
            .option("upperBound", "CURRENT_TIMESTAMP") \
            .option("numPartitions", "5") \
            .option("pollingInterval", "30s") \  # Опрос каждые 30 секунд
            .options(**self.gp_config.base_properties) \
            .load()

        return streaming_df

def process_streaming_data(stream_df):
    # Очистка и трансформация
    processed_stream = stream_df \
    .filter(F.col("operation_type").isin("INSERT", "UPDATE")) \
    .select(
        F.col("id"),
        F.col("data"),
        F.when(F.col("operation_type") == "INSERT", "new") \
         .when(F.col("operation_type") == "UPDATE", "updated") \
         .alias("record_status"),
        F.current_timestamp().alias("processed_at")
    )

    # Агрегация в окнах для потоковых данных
    windowed_aggregations = processed_stream \
        .withWatermark("processed_at", "10 minutes") \  # Водяной знак для обработки задержек
        .groupBy(
            F.window(F.col("processed_at"), "5 minutes"),  # 5-минутные окна
            F.col("record_status")
        ) \
        .agg(
            F.count("id").alias("record_count"),
            F.approx_count_distinct("id").alias("unique_ids")
        )

    return windowed_aggregations

# Запуск потоковой обработки
def start_streaming_pipeline():
    processor = GreenplumStreamProcessor(spark, gp_config)
    source_stream = processor.create_streaming_source("transaction_log", "created_timestamp")
    processed_stream = process_streaming_data(source_stream)

    # Определение стоков для записи результатов
    def stream_writer(stream_df, output_mode="update"):
        return stream_df.writeStream \
            .outputMode(output_mode) \  # complete, update, append
            .format("console") \  # Можно заменить на jdbc, kafka, etc.
            .option("truncate", "false") \
            .option("checkpointLocation", processor.checkpoint_location) \
            .option("numRows", 100) \  # Количество строк для вывода в консоль
            .start()

    # Альтернатива: запись обратно в Greenplum
    def greenplum_sink_writer(stream_df, table_name):
        return stream_df.writeStream.foreachBatch(lambda batch_df, batch_id:
                batch_df.write \
                    .format("jdbc") \
                    .option("url", gp_url) \
                    .option("dbtable", table_name) \
                    .option("batchsize", 10000) \
                    .options(**gp_properties) \
                    .mode("append") \
                    .save()
            ) \
            .outputMode("update") \
            .option("checkpointLocation", processor.checkpoint_location) \
            .start()

    # Запуск потокового запроса
    query = stream_writer(processed_stream)

    # Управление жизненным циклом потока
    try:
        query.awaitTermination()  # Ожидание завершения
    except KeyboardInterrupt:
        print("Stopping streaming query...")
        query.stop()
```