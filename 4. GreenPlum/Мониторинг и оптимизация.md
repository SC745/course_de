<h2>Мониторинг и оптимизация</h2>
<h3>1. Мониторинг</h3>
<h4>Состояние и доступность кластера</h4>

В Greenplum есть несколько способов проверить состояние кластера:
- Проверка состояния всех сегментов: Команда gpstate является основной для проверки состояния кластера. Например:
```bash
gpstate -s          # Сводка состояния кластера
gpstate -e          # Показать сегменты с ошибками
gpstate -m          # Информация о зеркальных сегментах
gpstate -Q          # Быстрая проверка (все сегменты вверх/вниз)
```
- Проверка доступности кластера и подключения: Можно использовать `gpcheckperf` для проверки производительности, но для простой проверки доступности можно использовать утилиту `gpadmin` (например, подключиться через `psql`) и выполнить простой запрос:
```bash
gpcheckperf -f host_list -r dsM   # Тест производительности сети и дисков
```
- Проверка конфигурации и состояния через SQL: Подключиться к базе данных через `psql` и выполнить запросы к системным каталогам и представлениям:
```sql
SELECT * FROM gp_segment_configuration;       -- Роль, статус и хост каждого сегмента
SELECT * FROM gp_toolkit.gp_system_status;    -- Проверка системных метрик
```

<h4>Подключения и запросы</h4>

Для просмотра текущих подключений можно использовать представление `pg_stat_activity`. В Greenplum оно расширено и включает информацию о сегментах. Однако, обычно запросы к `pg_stat_activity` на главном узле показывают подключения к главному узлу. Также можно использовать утилиту `gpconfig` для настройки параметров, связанных с подключениями, например, максимального числа подключений. Пример запроса для просмотра активных подключений:
```sql
SELECT * FROM pg_stat_activity;
```

Для мониторинга выполняемых запросов можно использовать `pg_stat_activity` и обратить внимание на столбцы `query` (текущий запрос) и `state` (состояние). Также можно использовать представление `gp_toolkit.gp_resqueue_status` для просмотра состояния очередей запросов, если используется управление ресурсами. Для более детального анализа длительных запросов можно использовать представления, такие как `gp_toolkit.gp_locks_on_relation` и другие. Пример для просмотра текущих запросов:
```sql
SELECT datname, usename, application_name, state, query FROM pg_stat_activity WHERE state != 'idle';
SELECT * FROM gp_toolkit.gp_resqueue_status;      -- Очереди ресурсов
SELECT * FROM gp_toolkit.gp_locks_on_relation;    -- Блокировки
```

<h4>Использование памяти и дискового пространства</h4>

Greenplum настраивает память для каждого сегмента в зависимости от параметров конфигурации. Основные параметры: `gp_vmem_protect_limit` (лимит памяти на сегмент) и `shared_buffers` (кэш для данных):
```bash
gpconfig -s shared_buffers
gpconfig -s statement_mem
```

Также есть параметры для работы с оперативной памятью для конкретных операций (например, `work_mem`). Для мониторинга использования памяти в реальном времени можно использовать операционную систему (например, `top`, `htop`) или инструменты мониторинга, такие как GPCC (Greenplum Command Center) для этого. Также можно использовать запросы к `gp_toolkit` схеме, например, `gp_toolkit.gp_resgroup_status` (если используются ресурсные группы) для мониторинга использования памяти группой:
```sql
SELECT * FROM gp_toolkit.gp_resgroup_status; -- Группы ресурсов
SELECT * FROM gp_toolkit.gp_workfile_usage;   # Временные файлы
```
Для проверки дискового пространства, используемого базами данных, таблицами и индексами, можно использовать следующие запросы:

Размер базы данных:
```sql
SELECT datname, pg_size_pretty(pg_database_size(datname)) as size FROM pg_database;
```

Размер таблиц (включая индексы и TOAST):
```sql
SELECT schemaname, tablename, pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as total_size
FROM pg_tables
WHERE schemaname NOT IN ('pg_catalog', 'information_schema') 
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
```

Также можно использовать представление `gp_toolkit.gp_disk_free` для проверки свободного места на файловых системах, где расположены данные сегментов и `gp_toolkit.gp_skew_coefficients` для оценки перекоса данных:
```sql
SELECT * FROM gp_toolkit.gp_disk_free;
SELECT * FROM gp_toolkit.gp_skew_coefficients;
```

Еще одна полезная утилита - `gpexpand` (если кластер расширялся) может показывать распределение данных по сегментам. Для мониторинга дискового пространства на уровне операционной можно использовать `df -h` на узлах кластера.

<h4>Метрики и инструменты</h4>

Ключевые метрики для мониторинга:
- Производительность запросов:
  - Время выполнения запросов (avg, max)
  - Количество запросов в единицу времени
  - Медленные запросы
- Ресурсы:
  - Загрузка CPU (на мастер-узле и сегментах)
  - Использование памяти (оперативной и swap)
  - Дисковый I/O (чтение/запись, latency)
  - Сетевой трафик (особенно между сегментами)
- Состояние кластера:
  - Доступность сегментов
  - Перекос данных
  - Размер базы данных и таблиц

Инструменты мониторинга:
- Системные представления (System Catalogs and Views): `gp_toolkit` - схема, содержащая множество представлений для мониторинга и диагностики. Некоторые полезные представления:
  - `gp_toolkit.gp_resqueue_status` - показывает состояние очередей запросов.
  - `gp_toolkit.gp_locks_on_relation` - информация о блокировках на таблицах.
  - `gp_toolkit.gp_disk_free` - свободное место на диске.
  - `gp_toolkit.gp_size_*` - различные представления для размера объектов (таблиц, индексов и т.д.).
- Представления статистики (Statistics Views):
  - `pg_stat_activity` - показывает активные запросы и процессы.
  - `pg_stat_database` - статистика по базам данных.
  - `pg_stat_all_tables` - статистика по таблицам (количество чтений, обновлений и т.д.).
- Утилиты командной строки:
  - `gpstate` - показывает состояние кластера (сегменты, мастер и т.д.).
  - `gpcheckperf` - проверка производительности системы (диск, сеть, память).

Интерфейс управления (Management Interface): GreenPlum Command Center (GPCC) - веб-интерфейс для мониторинга производительности и управления (доступен в коммерческой версии). Он предоставляет дашборды для мониторинга запросов, использования ресурсов, нагрузки и т.д.

<h4>Мониторинг ресурсов</h4>

Процессор:
- На уровне ОС: `top`, `htop`, `mpstat`
- В Greenplum:
  - Запросы, нагружающие CPU: `pg_stat_activity` в сочетании с `pg_stat_statements` (если включен)
  - Сегменты с высокой загрузкой: `gp_toolkit.gp_resgroup_status` (для `resource groups`) или `gp_toolkit.gp_resqueue_status` (для `resource queues`)

Память:
- На уровне ОС: `free -m, vmstat`
- В Greenplum:
  - Настройки памяти: `gpconfig -s shared_buffers`, `gpconfig -s statement_mem`
  - Мониторинг использования памяти сегментами: `gp_toolkit.gp_workfile_usage` (временные файлы, которые являются признаком нехватки памяти)
  - Представление `gp_toolkit.gp_resgroup_status` для памяти, используемой группами ресурсов.

Сеть:
- На уровне ОС: `netstat`, `iftop`, `nethogs`
- В Greenplum: Межсегментный трафик: можно оценить по объему данных, передаваемых между сегментами (через объяснение плана запроса - `EXPLAIN ANALYZE`). Утилита `gpcheckperf` для проверки производительности сети.

<h4>Интеграция с внешними системами мониторинга</h4>

GreenPlum можно интегрировать с внешними системами мониторинга, такими как:
- Prometheus: Использование экспортеров для сбора метрик из GreenPlum. Например, можно использовать `postgres_exporter` (который работает с GreenPlum, так как GreenPlum основан на PostgreSQL) для сбора метрик БД.
- Настройка GreenPlum для предоставления метрик через представления, которые может scraping Prometheus.
- Grafana: Подключение к Prometheus или другому источнику данных для визуализации метрик GreenPlum.
- Интеграция с системами управления кластерами: Например, интеграция с VMware Tanzu (ранее Pivotal) или другими платформами, которые могут использовать API GreenPlum для сбора метрик.
- SNMP: Настройка SNMP-агентов для мониторинга серверов, на которых работает GreenPlum.
- Скрипты и кастомные решения: Написание скриптов (на bash, python и т.д.) для сбора метрик из системных представлений GreenPlum и отправки в системы мониторинга (Zabbix, Nagios и т.д.).

Пример настройки мониторинга с Prometheus и Grafana:
1. Установите `postgres_exporter` на мастер-сервере GreenPlum (или на отдельном сервере мониторинга, который имеет доступ к БД).
2. Настройте `postgres_exporter` для подключения к GreenPlum и сбора метрик.
3. Настройте Prometheus для сбора метрик с `postgres_exporter`.
4. В Grafana создайте дашборды, используя данные из Prometheus.

<h4>Алертинг и восстановление</h4>

Алертинг в Greenplum можно настроить с помощью различных инструментов, но одним из распространенных способов является использование встроенных возможностей Greenplum Command Center (GPCC) или интеграция с внешними системами, такими как Prometheus с Alertmanager и Grafana.

Шаги по настройке алертинга:
1. Определение критических метрик: например, недоступность сегментов, нехватка дискового пространства, высокая загрузка CPU, большое количество ошибок в логах.
2. Настройка порогов для метрик: например, если диск заполнен на 90%, то отправить предупреждение; если сегмент недоступен, то критическое оповещение.
3. Выбор каналов оповещения: email, Slack, PagerDuty, etc.

GPCC позволяет настроить алерты через веб-интерфейс. Можно задать условия, например, по проценту использования диска, по состоянию сегментов (вниз, в режиме восстановления и т.д.), по производительности запросов.

Если сегмент недоступен, Greenplum автоматически переключится на его зеркало (если зеркалирование настроено). Однако, если зеркало отсутствует или тоже недоступно, то требуется ручное вмешательство.

Основные шаги по восстановлению:
1. Проверить состояние кластера: `gpstate -s` (сводка) и `gpstate -e` (сегменты с ошибками).
2. Попытаться восстановить упавшие сегменты с помощью `gprecoverseg`. Эта команда попытается поднять упавшие сегменты и синхронизировать данные с зеркалами.
3. Если `gprecoverseg` не сработал, может потребоваться перебалансировка кластера с помощью `gprecoverseg -r`.
4. В случае потери сегмента без зеркала, может потребоваться восстановление из резервной копии (с помощью `gpbackup`/`gprestore`).

<h4>Использование и настройка gpperfmon</h4>

gpperfmon — это встроенная база данных мониторинга Greenplum, которая собирает метрики о производительности и состоянии кластера. Она состоит из трех основных компонентов:
- Агенты сбора данных (gpmmon): запускаются на каждом узле (мастер и сегменты) и собирают метрики.
- База данных мониторинга (gpperfmon): находится на мастере и хранит собранные данные.
- Веб-интерфейс (GPCC) для визуализации данных.

Настройка gpperfmon:
1. Включение gpperfmon: В файле `postgresql.conf` (на мастере) установить параметр `gp_enable_gpperfmon = on` и перезапустить кластер: `gpstop -ra`.
2. Настройка агентов: Агенты настраиваются автоматически при включении gpperfmon. Они запускаются как демоны и собирают метрики каждые 15 секунд (по умолчанию).
3. Доступ к данным мониторинга: Данные хранятся в базе данных gpperfmon в схемах `now` (текущие данные) и `history` (исторические данные, которые периодически архивируются из `now`). Можно писать SQL-запросы к этой базе для получения метрик.
4. Настройка интервалов сбора и хранения:
  - `gpperfmon_interval` - интервал сбора метрик (по умолчанию 15 секунд).
  - `gp_gpperfmon_retention_period` - период хранения данных в днях (по умолчанию 90 дней).
5. Использование GPCC для визуализации: GPCC предоставляет веб-интерфейс для просмотра метрик в реальном времени и исторических данных. Установка GPCC отдельная, но она использует данные из базы gpperfmon.

Пример запроса к gpperfmon для получения текущей загрузки CPU:
```sql
SELECT * FROM gpperfmon.host_metrics_now;
```

Для просмотра истории запросов:
```sql
SELECT * FROM gpperfmon.queries_history;
```

<h3>2. Планы запросов</h3>
<h4>План выполнения запроса</h4>

Greenplum использует планировщик запросов, который генерирует план выполнения для каждого запроса. План описывает, как будет выполнено сканирование таблиц (последовательное, индексное и т.д.), как будут соединены таблицы (nested loop, hash join, merge join), а также операции агрегации, сортировки и т.д.

Особенность Greenplum в том, что это распределенная СУБД, поэтому план выполнения также включает этапы обмена данными между сегментами (motion). Это могут быть:
- Broadcast motion: когда данные с одного сегмента рассылаются всем остальным.
- Redistribute motion: когда данные перераспределяются между сегментами по ключу распределения.
- Gather motion: когда данные с всех сегментов собираются на одном узле (например, для выдачи результата клиенту).

Основные команды для анализа планов запросов в Greenplum — это `EXPLAIN` и `EXPLAIN ANALYZE`.
- `EXPLAIN`: показывает предполагаемый план выполнения запроса, без фактического выполнения запроса.
- `EXPLAIN ANALYZE`: не только показывает план, но и выполняет запрос, добавляя в вывод фактические затраты времени, количество строк и т.д, что позволяет сравнить фактические результаты с предполагаемыми.

Дополнительные опции `EXPLAIN`:
- `FORMAT <format>`: можно указать формат вывода (TEXT, XML, JSON, YAML).
- `VERBOSE`: показывать дополнительную информацию, включая вывод столбцов.
- `COSTS`: показывать затраты (включено по умолчанию).
- `BUFFERS`: показывать информацию о буферных операциях (требует `ANALYZE`).
- `ANALYZE`: выполнить запрос и показать фактические данные.

<h4>Анализ плана запроса</h4>

`EXPLAIN ANALYZE` — это основной инструмент для поиска узких мест в запросе. Вот что нужно учитывать:
- Оценки vs Факт: Сравнивайте оценку количества строк (rows) и фактическое количество. Если они сильно различаются, возможно, необходимо обновить статистику (команда `ANALYZE`).
- Время выполнения: Обращайте внимание на узлы, которые занимают наибольшее время. Часто это операции соединения (`Join`) или сортировки (`Sort`), которым не хватает памяти.
- Операции Motion (Перемещение данных): В Greenplum это критически важно. Обращайте внимание на `Broadcast Motion` и `Redistribute Motion`. Если большая таблица broadcast'ится, это может быть очень дорого.
- Использование индексов: Проверяйте, используются ли индексы, когда вы ожидаете. В Greenplum индексы не всегда используются, особенно если селективность низкая.
- Память и дисковые операции: Если узел плана использует временные файлы (например, при сортировке или хэш-соединении), это означает, что операция не поместилась в памяти. Возможно, нужно увеличить параметр work_mem.
- Распределение данных: Проверяйте, что распределение данных по сегментам равномерно. Неравномерное распределение (skew) может привести к тому, что один сегмент будет обрабатывать значительно больше данных, чем другие.


Для сложных запросов с несколькими `JOIN`, подзапросами и агрегациями план может быть очень большим. Вот шаги для анализа:
- Читаем план от внутренних узлов к корню: Начинайте с самых вложенных операций (например, сканирование таблиц). Обращайте внимание на оценку числа строк и фактическое число.
- Ищем самые дорогие узлы: Обычно это узлы с наибольшим временем выполнения или с наибольшим числом строк.
- Анализируем операции соединения:
  - `Nested Loop`: Может быть медленным, если внутренняя таблица большая. Часто используется при соединении с маленькой таблицей или при наличии индекса.
  - `Hash Join`: Требует построения хэш-таблицы по одной из таблиц. Проверьте, что хэш-таблица строится по меньшей таблице.
  - `Merge Join`: Требует, чтобы обе таблицы были отсортированы по ключу соединения. Если данные не отсортированы, то будет дорогая сортировка.
- Смотрим на агрегации:
  - `HashAggregate`: Группировка с использованием хэш-таблицы. Может потреблять много памяти.
  - `GroupAggregate`: Предполагает, что входные данные отсортированы по ключам группировки.
- Обращаем внимание на операции над данными между сегментами (Motion):
  - `Gather Motion`: Сбор данных с сегментов на мастере (финальная стадия).
  - `Redistribute Motion`: Перераспределение данных по сегментам по новому ключу.
  - `Broadcast Motion`: Рассылка данных всех сегментов на каждый сегмент.

Предположим, у нас есть запрос с несколькими `JOIN` и агрегацией. В плане мы видим, что один из шагов `Hash Join` имеет большое расхождение в оценке и факте по строке. Это может привести к неправильному выбору плана. Решение: обновить статистику по таблицам, участвующим в этом соединении.

<h4>Интерпретация плана запроса</h4>

Пример запроса для анализа плана:
```sql
EXPLAIN ANALYZE
SELECT customer_name,
       SUM(amount) as total
  FROM customers
       JOIN sales USING(customer_id)
 WHERE sale_date >= '2023-01-01'
 GROUP BY customer_name
HAVING SUM(amount) > 5000;
```

План запроса в Greenplum представляется в виде дерева операций, где каждая операция (узел) может иметь дочерние операции. Вывод плана имеет иерархическую структуру, обозначаемую отступами или стрелками (->). Каждый узел представляет собой шаг в выполнении запроса, такие как сканирование таблицы, соединение, агрегация, сортировка и т.д:
```sql
QUERY PLAN
------------------------------------------------------------------
Gather Motion 4:1  (slice3; segments: 4)
 (cost=12500.25..12600.50 rows=100 width=24)
 (actual time=450.250..550.500 rows=85 loops=1)
  ->  HashAggregate  (cost=12500.25..12575.25 rows=25 width=24)
       (actual time=400.150..425.300 rows=21 loops=1)
        Group By: customer_name
        Filter: (sum(amount) > 5000::numeric)
        ->  Redistribute Motion 4:4  (slice2; segments: 4)
             (cost=10000.00..12450.00 rows=25000 width=24)
             (actual time=200.100..350.750 rows=100000 loops=1)
             Hash Key: customer_name
              ->  Hash Join  (cost=5000.25..9500.75 rows=25000 width=24)
                   (actual time=150.050..275.500 rows=100000 loops=1)
                    Hash Cond: (sales.customer_id = customers.customer_id)
                    ->  Seq Scan on sales
                         (cost=0.00..4000.00 rows=50000 width=16)
                         (actual time=0.100..75.200 rows=200000 loops=1)
                          Filter: (sale_date >= '2023-01-01'::date)
                          Rows Removed by Filter: 300000
                    ->  Hash  (cost=2500.10..2500.10 rows=10000 width=16)
                         (actual time=149.800..149.800 rows=10000 loops=1)
                          Buckets: 32768  Batches: 1  Memory Usage: 1024kB
                          ->  Seq Scan on customers
                               (cost=0.00..2500.10 rows=10000 width=16)
                               (actual time=0.050..75.100 rows=10000 loops=1)
Planning time: 10.500 ms
Memory used:  2048kB
Execution time: 600.250 ms
```

Уровень 1:
- Чтение таблицы customers:
```sql
Seq Scan on customers                             # Seq Scan - последовательное чтение
(cost=0.00..2500.10 rows=10000 width=16)          # Ожидается 10000 строк по 16 байт
(actual time=0.050..75.100 rows=10000 loops=1)    # Получено 10000 строк за 1 цикл
```

Уровень 2:
- Чтение таблицы sales:
```sql
Seq Scan on sales                                  # Seq Scan - последовательное чтение
(cost=0.00..4000.00 rows=50000 width=16)           # Ожидается 50000 строк по 16 байт
(actual time=0.100..75.200 rows=200000 loops=1)    # Получено 200000 строк за 1 цикл
Filter: (sale_date >= '2023-01-01'::date)          # Фильтрация по условию (sale_date >= '2023-01-01'::date)
Rows Removed by Filter: 300000                     # 300000 строк было удалено фильтром
```
- Построение хеш-таблицы:
```sql
Hash  (cost=2500.10..2500.10 rows=10000 width=16)     # Ожидается 10000 строк по 16 байт
(actual time=149.800..149.800 rows=10000 loops=1)     # Получено 10000 строк за 1 цикл
Buckets: 32768  Batches: 1  Memory Usage: 1024kB      # Создано 32768 бакетов, использовано 1024 kB памяти
```

Уровень 3:
- Hash Join таблиц sales и customers:
```sql
Hash Join  (cost=5000.25..9500.75 rows=25000 width=24)    # Ожидается 25000 строк по 24 байта
(actual time=150.050..275.500 rows=100000 loops=1)        # Получено 100000 строк за 1 цикл
Hash Cond: (sales.customer_id = customers.customer_id)    # Условие соединения таблиц
```

Уровень 4:
- Перераспределение данных:
```sql
Redistribute Motion 4:4  (slice2; segments: 4)        # Перераспределение с 4 на 4 сегмента
(cost=10000.00..12450.00 rows=25000 width=24)         # Ожидается 25000 строк по 24 байта
(actual time=200.100..350.750 rows=100000 loops=1)    # Получено 100000 строк за 1 цикл
Hash Key: customer_name                               # Ключ распределения - все одинаковые значения на сегментах
```

Уровень 5:
- Агрегация данных:
```sql
HashAggregate  (cost=12500.25..12575.25 rows=25 width=24)    # Ожидается 25 строк по 24 байта
(actual time=400.150..425.300 rows=21 loops=1)               # Получена 21 строка за 1 цикл
Group By: customer_name                                      # Группировка по customer_name
Filter: (sum(amount) > 5000::numeric)                        # Фильтрация по условию (sum(amount) > 5000::numeric)
```

Уровень 6:
- Сбор результатов:
```sql
Gather Motion 4:1  (slice3; segments: 4)          # Перераспределение с 4 на 1 сегмент
(cost=12500.25..12600.50 rows=100 width=24)       # Ожидается 100 строк по 24 байта
(actual time=450.250..550.500 rows=85 loops=1)    # Получено 85 строк за 1 цикл
```

<h3>3. Оптимизация производительности</h3>
<h4>Типы индексов</h4>

Поскольку Greenplum унаследовала свою базу от PostgreSQL, она поддерживает большинство ее продвинутых типов индексов. Однако в контексте распределенной MPP-системы их применение и эффективность имеют свои особенности. Типы индексов:
- B-Tree (B-дерево): Стандартный индекс, используемый по умолчанию. Эффективен для поиска по диапазону (`BETWEEN`, >, <) и точного совпадения (=). Когда использовать:
  - Для столбцов с высоким уровнем уникальности (первичные ключи, уникальные ограничения).
  - Для часто используемых предикатов `WHERE` с операторами =, >, <, `BETWEEN`.
  - Для операций `ORDER BY` и `DISTINCT`.
- Bitmap Index (Битовый индекс): Индекс, который для каждого уникального значения хранит битовую карту строк. Каждый бит указывает, присутствует ли данное значение в соответствующей строке. Идеально подходит для столбцов с низкой кардинальностью (малое количество уникальных значений). Классические примеры: пол, статус заказа (`'open'`, `'closed'`), флаги (`true`/`false`). Очень эффективен для запросов с условиями `AND`/`OR` над несколькими такими столбцами (например, `WHERE status = 'open' AND region = 'west')`. Bitmap-индексы работают в рамках одного сегмента. Они не глобальны для всего кластера, но благодаря архитектуре MPP это не является проблемой.
- GiST (Generalized Search Tree): Обобщенное дерево поиска, которое позволяет индексировать сложные типы данных и нетривиальные операции над ними. Используется для геопространственных данных (типы geometry, geography). Например, для поиска "всех объектов в пределах заданного радиуса". Также используется для полнотекстового поиска и индексирования массивов и других сложных структур.
- GIN (Generalized Inverted Index): Обобщенный инвертированный индекс. Оптимизирован для работы с составными значениями, где один документ или запись может содержать несколько ключей (например, массив, JSONB, текст). Тип данных JSONB — это основной случай использования. Очень эффективен для поиска по ключам и значениям внутри JSON-документов. Также используется для полнотекстового поиска.
- SP-GiST (Space-Partitioned GiST): Пространственно-разделенное GiST-дерево. Полезно для индексирования данных, которые имеют неоднородную структуру или могут быть разделены по пространству (не только географическому). Менее распространен, но может быть полезен для определенных типов геопространственных данных или данных с иерархической структурой (например, квадродеревья).
- Hash Index: Индекс, который использует хеш-функцию для быстрого поиска точного совпадения. В Greenplum и PostgreSQL используется редко. B-Tree почти всегда предпочтительнее, так как он поддерживает диапазонные запросы и более устойчив к росту. Hash-индекс может быть немного быстрее для операций =, но не поддерживает другие операторы.

<h4>Использование индексов</h4>

В Greenplum философия использования индексов отличается от OLTP-систем (как PostgreSQL в чистом виде):
- OLTP (PostgreSQL): Индексы критически важны для быстрого поиска отдельных строк. Почти все запросы используют индексы.
- OLAP (Greenplum): Основная производительность достигается за счет сегментирования (distribution) и партиционирования (partitioning), а также полного сканирования таблиц параллельно на всех сегментах.

Когда СЛЕДУЕТ использовать индексы в Greenplum:
- Точечные запросы (OLAP-запросы с высокой селективностью): Запросы, которые возвращают небольшое подмножество данных (менее 1-5% от таблицы). Например, `SELECT * FROM sales WHERE order_id = 12345`. Без индекса системе придется выполнить полное сканирование огромной таблицы.
- Условия на ключе распределения (Distribution Key): Если запрос содержит условие `WHERE` на столбец, который является ключом распределения, то запрос будет выполняться только на одном сегменте. В этом случае индекс на этом сегменте может сильно ускорить поиск.
- Условия на ключ партиционирования (Partition Key): Если таблица партиционирована по дате, то индекс внутри каждой партиции поможет быстро найти данные, после того как система определила нужную партицию. Например, `WHERE event_date = '2023-10-25' AND user_id = 100500`.
- Джойны (Joins): Индексы на столбцах, по которым происходит соединение таблиц, могут значительно ускорить процесс джойна, особенно если одна из таблиц небольшая (или после фильтрации становится небольшой).

Когда индексы могут быть не нужны или даже вредны:
- Пакетная загрузка данных (ETL): При загрузке миллионов строк поддержка индексов создает значительные накладные расходы. Частая практика: `DROP INDEX -> LOAD DATA -> CREATE INDEX`.
- Аналитические запросы с полным сканированием: Запросы типа `SELECT SUM(revenue) FROM sales WHERE year = 2023` обрабатывают большую часть данных таблицы. В этом случае быстрее выполнить параллельное полное сканирование, чем обращаться к индексу.
- Таблицы, которые часто обновляются: Каждое `INSERT`, `UPDATE`, `DELETE` требует обновления индексов, что замедляет работу.

<h4>Партицирование таблиц</h4>

Партиционирование — это метод разделения большой таблицы на меньшие, более управляемые части, называемые партициями. Каждая партиция хранится как отдельная таблица, но при работе с данными они представляются как единая таблица.

Задачи партицирования:
- Улучшение производительности запросов: Запросы могут читать только необходимые партиции (исключение партиций, partition pruning), что уменьшает объем сканируемых данных.
- Упрощение управления данными: Легко удалять или архивировать старые данные, удаляя партиции (например, по дате). Также можно загружать данные в конкретные партиции.
- Эффективное использование хранилища: Можно использовать разные параметры хранения для разных партиций (например, сжатие для старых данных).

Типы партицирования:
- Диапазонное партицирование (Range Partitioning):
```sql
CREATE TABLE sales (
    id INT,
    sale_date DATE,
    amount DECIMAL(10,2)
)
DISTRIBUTED BY (id)
PARTITION BY RANGE (sale_date)
(
    START (DATE '2023-01-01') END (DATE '2024-01-01') EVERY (INTERVAL '1 month'),
    DEFAULT PARTITION extra
);
```
- Списковое партицирование (List Partitioning):
```sql
CREATE TABLE sales_by_region (
    id INT,
    region TEXT,
    amount DECIMAL(10,2)
)
DISTRIBUTED BY (id)
PARTITION BY LIST (region)
(
    PARTITION europe VALUES ('UK', 'France', 'Germany'),
    PARTITION asia VALUES ('India', 'China', 'Japan'),
    DEFAULT PARTITION other_regions
);
```

Управление партицированием:
```sql
-- Добавление партиции
ALTER TABLE sales ADD PARTITION START (DATE '2024-01-01') END (DATE '2024-02-01'); -- диапазонная партиция
ALTER TABLE sales ADD PARTITION australia VALUES ('Australia', 'New Zealand');     -- списковая партиция

-- Удаление партиции
ALTER TABLE sales DROP PARTITION FOR (DATE '2023-01-01');                          -- диапазонная партиция
ALTER TABLE sales DROP PARTITION extra;                                            -- списковая партиция

-- Очистка партиции
ALTER TABLE sales TRUNCATE PARTITION FOR (DATE '2023-01-01');                      -- диапазонная партиция
ALTER TABLE sales TRUNCATE PARTITION extra;                                        -- списковая партиция
```

Подпартиции:

```sql
CREATE TABLE sales_multi (
    id INT,
    sale_date DATE,
    region TEXT,
    amount DECIMAL(10,2)
)
DISTRIBUTED BY (id)
PARTITION BY RANGE (sale_date)
SUBPARTITION BY LIST (region)
SUBPARTITION TEMPLATE (
    SUBPARTITION europe VALUES ('UK', 'France', 'Germany'),
    SUBPARTITION asia VALUES ('India', 'China', 'Japan'),
    DEFAULT SUBPARTITION other_regions
)
(
    START (DATE '2023-01-01') END (DATE '2024-01-01') EVERY (INTERVAL '1 month'),
    DEFAULT PARTITION extra
);
```

<h4>Сжатие данных</h4>

Greenplum предлагает мощные механизмы сжатия данных, которые позволяют экономить место на диске, но их использование требует взвешенного подхода из-за компромисса между CPU и I/O.

Методы сжатия:
- Сжатие на уровне столбцов (Column-Oriented Compression): Применяется только к колоночным таблицам (`WITH (APPENDONLY=true, ORIENTATION=column)`). Алгоритмы:
  - RLE (Run-Length Encoding): Эффективно для данных с низкой кардинальностью, где повторяются длинные последовательности одинаковых значений (например, `status`, `country_code`).
  - ZLIB: Алгоритм общего назначения, обеспечивает хорошую степень сжатия для любых данных, но требует больше CPU.
  - QuickLZ: Быстрее чем ZLIB, но степень сжатия обычно ниже.
- Сжатие на уровне строк (Row-Oriented Compression): Применяется к строчным таблицам (`WITH (APPENDONLY=true, ORIENTATION=row`)), использует алгоритм ZLIB. Хорошо подходит для OLTP-подобных нагрузок, где запросы часто выбирают много столбцов из одной строки.

Влияние на производительность:
- Чтение: Данные читаются с диска в сжатом виде и распаковываются в памяти. Это значительно снижает нагрузку на подсистему I/O, так как физически с диска считывается меньше данных. Для запросов, ограниченных скоростью диска (I/O-bound), это дает огромный прирост производительности.
- Запись: Данные нужно сжимать перед записью на диск, что создает дополнительную нагрузку на CPU.
- Операции вставки (`INSERT`) и обновления (`UPDATE`) становятся дороже, так как данные необходимо сжимать. Это может быть "бутылочным горлышком" для высоконагруженных процессов загрузки данных.

Общие рекомендации:
- Используйте сжатие для больших, редко изменяемых таблиц: Фактовые таблицы в хранилищах данных, архивные данные, большие логи.
- Для колоночных таблиц выбирайте алгоритм в зависимости от данных: RLE для столбцов с повторениями, ZLIB для всего остального.
- Избегайте сжатия для небольших или часто изменяемых таблиц: Нагрузка на CPU может перевесить выгоду от экономии места.

<h4>Факторы производительности</h4>

Производительность Greenplum определяется взаимодействием нескольких ключевых компонентов:
- Распределение данных (Data Distribution): Неудачный выбор ключа распределения ведет к двум проблемам:
  - Перекос данных (Data Skew): Данные распределены неравномерно между сегментами. Один сегмент работает с гораздо большим объемом данных, чем другие, что сводит на нет преимущества параллелизма.
  - Перемещение данных (Data Movement/Redistribution): Если для выполнения JOIN или GROUP BY данные находятся на разных сегментах, системе приходится их перемешивать (broadcast или redistribute). Это самая дорогая операция.
- Аппаратное обеспечение и конфигурация кластера:
  - Количество сегментов: Больше сегментов — выше параллелизм. Каждый сегмент должен иметь выделенные CPU, память и диски.
  - Сеть: Высокопроизводительная сеть (например, 10 GbE и выше) критична для минимизации затрат на перемещение данных.
  - Диски: Производительность дисковой подсистемы (I/O). Рекомендуются RAID-10 или SSD. "Shared-nothing" архитектура означает, что каждый сегмент работает со своим набором дисков.
  - Память (RAM): Достаточный объем оперативной памяти позволяет обрабатывать большие наборы данных в памяти, минуя медленные диски.
- Партиционирование (Partitioning): Позволяет разбить большие таблицы на меньшие, логические части. Это позволяет использовать "partition pruning", когда оптимизатор исключает из сканирования целые партиции, не соответствующие условию `WHERE`.
- Статистика и оптимизатор:
  - Cтатистика: Оптимизатор (как legacy, так и ORCA) строит планы выполнения на основе статистики по таблицам (размер, селективность, распределение данных). Если статистика неактуальна, оптимизатор может выбрать неэффективный план (например, полное сканирование вместо индексного, или неправильный тип JOIN).
  - Выбор оптимизатора: Greenplum имеет два оптимизатора — legacy (на основе Postgres) и ORCA. ORCA лучше справляется со сложными запросами и большими объемами данных.
- Индексы: В отличие от OLTP-систем, в Greenplum индексы используются менее часто. Часто полное сканирование с параллелизацией по всем сегментам оказывается эффективнее. Однако индексы (особенно b-tree) полезны для точечных запросов (по первичному ключу) и некоторых типов `JOIN`.

<h4>Оптимизация запросов</h4>

Оптимизация распределения данных:
- Выбор правильного ключа распределения: Идеальный ключ — тот, который часто используется в условиях JOIN и GROUP BY, и который обеспечивает равномерное распределение данных. Часто это какой-то идентификатор (например, user_id, order_id).
- Избегание перекоса: Всегда проверяйте распределение данных после загрузки с помощью запроса:
sql
```sql
SELECT gp_segment_id, count(*)
FROM my_table
GROUP BY gp_segment_id
ORDER BY 2 DESC;
```

Оптимизация структуры таблиц:
- Партиционирование: Например, партиционирование таблицы продаж по месяцу.
- Выбор типа хранения:
  - Heap: Стандартный для PostgreSQL. Подходит для частых операций `UPDATE`/`DELETE`.
  - AOCO (Append-Optimized, Column-Oriented): Настоятельно рекомендуется для аналитических нагрузок. Данные хранятся колоночно, что сильно повышает скорость агрегаций и сканирования только нужных столбцов. Данные сжимаются.
- Сжатие: Для AOCO таблиц используется сжатие (например, `COMPRESSTYPE=zlib`). Это уменьшает объем I/O.

Оптимизация самих запросов:
- Избегание перемещения данных: Строить запросы так, чтобы операции выполнялись локально на сегментах.
- Использование предикатов для отсечения партиций: Всегда включать в условие ключ партиционирования в WHERE, чтобы активировать Partition Pruning.
- Селективность: Формулируйте условия `WHERE` как можно более специфичными.
- Анализ плана выполнения (`EXPLAIN ANALYZE`): Операции `Redistribute Motion` и `Broadcast Motion` — это "узкие места". Также стоит обращать внимание на оценки строк и их реальное количество. Сильное расхождение говорит об устаревшей статистике.

Своевременное обслуживание:
- Сбор статистики: Регулярно запускайте `ANALYZE` после загрузки или значительного изменения данных.
- Очистка мусора (Vacuum): После больших операций `UPDATE`/`DELETE` запускать `VACUUM` для освобождения места и обновления карты видимости. Для AOCO таблиц это менее критично.

Устранение узких мест:
- Дисковый ввод/вывод:
  - Проверка дисковых операций: `iostat`, `gp_toolkit.gp_disk_free`
  - Распределение данных по нескольким дискам (если используется JBOD)
- Сеть: Межсегментное взаимодействие: `gpcheckperf` для проверки пропускной способности сети.
- Память: Настройка памяти для операций сортировки, хеширования и т.д.

<h4>Настройка конфигурации</h4>

Настройки производятся в файлах `postgresql.conf` и `gpconfig`.

Память:
- `gp_vmem_protect_limit`: Лимит памяти на один сегмент. Слишком высокое значение может привести к OOM (Out Of Memory). Рассчитывается исходя из общей памяти узла, количества сегментов на узле и запаса для ОС.
- `statement_mem`: Память, выделяемая для одного запроса на сегменте. Если запросы падают с ошибкой нехватки памяти, это значение можно увеличить.
- `shared_buffers`: Кеш для данных. Обычно устанавливается в ~25% от ОЗУ сегмента.
- `work_mem`: Память для операций сортировки и хеширования. Увеличение может ускорить сложные `JOIN` и `ORDER BY`.

Параллелизм:
- `max_parallel_workers_per_gather`: Количество рабочих процессов для операций внутри одного сегмента. Увеличение ускоряет сложные агрегации и сканирования.
- `gp_interconnect_type`: Протокол для передачи данных между сегментами. Обычно рекомендуется `UDPIFC` (улучшенный UDP) для высокопроизводительных сетей.

Оптимизатор:
- `optimizer`: Ключевой параметр. Для современных версий и сложных запросов обязательно используйте `optimizer=on` (включает ORCA).
- `optimizer_analyze_root_partition`: Для ORCA. Сбор статистики по корневым партициям. Рекомендуется `on`.

Производительность дискового ввода-вывода и WAL:
- `effective_io_concurrency`: Устанавливает количество параллельных I/O-операций, которые может выполнять ОС. Для SSD устанавливается в более высокое значение (например, 200).
- `checkpoint_segments / max_wal_size`: Управляют частотой контрольных точек. Более редкие контрольные точки снижают нагрузку на I/O, но увеличивают время восстановления.

Мониторинг и обслуживание:
- `log_statement = 'none'` (или `'ddl'`) в продакшене, чтобы не логировать каждый запрос.
- `autovacuum = on`: Включено по умолчанию, но за его работой нужно следить на активно изменяемых таблицах.

Пример настройки с помощью `gpconfig`:
```bash
# Включение ORCA
gpconfig -c optimizer -v on

# Увеличение work_mem
gpconfig -c work_mem -v 256MB

# Установка эффективного параллелизма I/O для SSD
gpconfig -c effective_io_concurrency -v 200 --masteronly
```
Любые изменения параметров требуют перезагрузки кластера (`gpstop -ra`) или перечитывания конфигурации (`gpstop -u`).