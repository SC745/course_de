<h3>1. Хранилища данных в HDFS</h3>

HDFS разработан для хранения больших объемов данных и предоставляет гибкие механизмы для управления данными на основе их важности, частоты доступа и требований к производительности. Среди этих механизмов — политики хранения данных, позволяющие оптимально использовать различные типы хранилищ, такие как холодное (cold), теплое (warm) и горячее (hot) хранилища. Эти категории отражают частоту доступа к данным и помогают определить, как и где лучше всего хранить различные наборы данных в HDFS.

Типы хранилищ:
- Горячее Хранилище (Hot Storage): Данные, к которым часто обращаются и которые требуют быстрого доступа. Обычно это критически важные данные, используемые в текущих вычислениях и аналитике. Применяется для реальных или почти реальных аналитических процессов, важных бизнес-приложений. Размещается на SSD (твердотельных накопителях) для обеспечения быстрого доступа.
- Теплое Хранилище (Warm Storage): Данные, к которым доступ необходим не так часто, как к горячим данным, но которые все еще важны для бизнеса и могут потребоваться для анализа в краткосрочной перспективе. Это данные, используемые для еженедельных отчетов, архивы последних месяцев. Размещаются на обычных жестких дисках с достаточной производительностью для обеспечения приемлемого времени доступа.
- Холодное Хранилище (Cold Storage): Редко используемые данные, которые необходимо сохранять из-за юридических требований или для исторических исследований. Доступ к этим данным не требует высокой скорости. Это архивы, логи старых транзакций, исторические данные. Размещаются на экономичных хранилищах с большой емкостью, например, на ленточных накопителях или дешевых жестких дисках.

<h4>Политики хранения</h4>

Политики хранения данных в HDFS позволяют администраторам кластера управлять распределением данных между различными типами хранилищ на основе требований к доступу и производительности. Эти политики определяют, какие данные должны быть перемещены на горячее, теплое или холодное хранилище.

Администраторы могут настроить политики хранения через HDFS Shell или через API, указывая, какие файлы или директории подлежат определенным политикам. Также можно настроить автоматическое перемещение данных между хранилищами на основе их возраста, частоты доступа или других критериев, определяемых политикой.

Эффективное использование типов хранилищ и политик хранения данных в HDFS позволяет организациям оптимизировать стоимость хранения данных и производительность доступа к ним, обеспечивая при этом соответствие требованиям бизнеса и регуляторным нормам.

Конфигурация политик хранения данных в HDFS позволяет администраторам управлять распределением и размещением данных в кластере на основе их важности, частоты доступа и требований к производительности. Это включает в себя возможность определения, какие данные должны быть размещены на горячем, теплом или холодном хранилище. Настройка политик в HDFS происходит следующим образом:

Встроенные политики хранения:
- `HOT`: Все реплики на DISK. (Цель: производительность)
- `WARM`: Одна реплика на DISK, остальные на ARCHIVE. Это компромисс: вы сохраняете быстрый доступ к одной копии данных, но все остальные копии хранятся дешево.
- `COLD`: Все реплики на ARCHIVE. (Цель: минимизация стоимости)
- `ALL_SSD`: Все реплики на SSD. (Цель: максимальная производительность)
- `ONE_SSD`: Одна реплика на SSD, остальные на DISK. Аналогично WARM, но для ускорения доступа к одной копии.
- `LAZY_PERSIST`: Одна реплика пишется в RAM_DISK, а затем асинхронно сбрасывается на DISK. Остальные реплики сразу пишутся на DISK. (Цель: максимальная скорость записи)

При политике `ALL_SSD` все реплики хранятся на SSD, что очень затратно по стоимости. В большинстве случаях будет достаточно использования `ONE_SSD` для компромисса между стоимостью и производительностью. Эта политика почти не уступает по скорости чтения `ALL_SSD`, но стоит значительно дешевле.

<h4>Управление политиками</h4>

Базовые команды:
```bash
# Доступные политики
hdfs storagepolicies -list

# Установка политики
hdfs storagepolicies -setStoragePolicy -path <путь> -policy <политика>

# Снятие политики (возврат к HOT)
hdfs storagepolicies -unsetStoragePolicy -path <путь>

# Принудительное применение политики
hdfs storagepolicies -satisfyStoragePolicy -path <путь>

# Проверка политики
hdfs storagepolicies  -getStoragePolicy -path <путь>
```

Продвинутые команды:
```bash
# Для установки политики на всю директорию рекурсивно
hdfs dfs -find /data -type d -exec hdfs storagepolicies -setStoragePolicy -policy WARM {} \;

# Проверка состояния перемещения данных
hdfs fsck /data/analytics -files -blocks -storagepolicies
```

<h3>2. UDF</h3>
<h4>Основы и регистрация</h4>

Apache Spark предоставляет мощные возможности для обработки данных, но иногда требуются специализированные операции, которых нет в стандартном наборе функций. В таких случаях на помощь приходят User-Defined Functions (UDF) – функции, определенные пользователем, которые позволяют интегрировать собственную логику обработки данных непосредственно в Spark SQL и DataFrame API. UDF в PySpark определяются с помощью функции `udf`, которая преобразует обычную функцию Python в объект, пригодный для использования в DataFrame API и Spark SQL.

Допустим, мы хотим добавить новую колонку, которая будет указывать, является ли клик "позитивным" (`positive`) или "негативным" (`negative`) на основе атрибута `click_type`:

```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

# Определение функции Python, которая будет классифицировать клики
def classify_click(click_type):
    if click_type in ["normal_click", "lazy_click"]: return "positive"
    else: return "negative"

# Регистрация функции как UDF
classify_click_udf = udf(classify_click, StringType())

# Добавление новой колонки с использованием UDF
click_transactions_df_with_classification = click_transactions_df.withColumn("click_classification", classify_click_udf(click_transactions_df["click_type"]))

# Показать результат
click_transactions_df_with_classification.show()
```

Также можно определять UDF c помощью декоратора и использовать для работы с массивами:
```python
from pyspark.sql.types import ArrayType, IntegerType
from pyspark.sql.functions import udf

@udf(ReturnType=ArrayType(IntegerType()))
def double_array(arr):
    if arr is None:
        return None
    return [x * 2 for x in arr]
```

UDF также можно использовать в SQL запросах. Для этого сначала нужно зарегистрировать DataFrame как временную таблицу, а UDF - в SparkSession с помощью метода `spark.udf.register`:

```python
# Регистрация UDF для использования в Spark SQL
spark.udf.register("classify_click", classify_click, StringType())

click_transactions_df.createOrReplaceTempView("click_transactions")

# Теперь можно использовать UDF в SQL запросе
spark.sql("""
SELECT *, classify_click(click_type) AS click_classification
FROM click_transactions
""").show()
```

<h4>Проблемы и оптимизация</h4>

Возможные проблемы при использовании UDF:
- Производительность: UDF в PySpark работают медленнее, чем встроенные функции, потому что данные должны быть сериализованы и десериализованы между JVM и Python процессом. Python UDF не могут использовать оптимизации Catalyst Optimizer и Tungsten в Spark (кодгенирацию, оптимизацию запросов и т.д.). Нет возможности использовать предикатный pushdown или другие оптимизации для UDF.
- Сложность отладки: Ошибки в UDF могут быть сложными для отладки, особенно когда UDF выполняются на удаленных узлах.
- Ограничения по типам данных: Не все типы данных Python могут быть автоматически преобразованы в типы Spark и наоборот. Нужно явно указывать тип возвращаемого значения, иначе может быть использован неподходящий тип по умолчанию.
- Совместимость версий: При обновлении Spark может измениться поведение UDF, поэтому необходимо тестировать на новых версиях.
- Распределенные вычисления: UDF выполняются на рабочих узлах, поэтому необходимо убедиться, что на этих узлах установлены все необходимые зависимости (например, сторонние библиотеки Python).
- Побочные эффекты: Не рекомендуется использовать UDF с побочными эффектами (например, запись в файл), так как это может привести к непредсказуемому поведению.

Методы устранения проблем:
- Указание правильного типа возвращаемого значения: Всегда явно указывайте возвращаемый тип в UDF, чтобы избежать неожиданностей.
- Обработка null-значений: Убедитесь, что ваша UDF правильно обрабатывает null-значения. Если UDF не обрабатывает null, это может привести к исключениям.
- Тестирование UDF: Тщательно тестируйте UDF на небольших наборах данных перед использованием в распределенной среде.
- Использование логов: Для отладки можно использовать логирование внутри UDF, но помните, что логи будут находиться на рабочих узлах, а не на драйвере.
- Мониторинг и профилирование: Используйте инструменты мониторинга Spark (например, Spark UI) для выявления узких мест в производительности.

Оптимизация выполнения UDF:
- Использование встроенных функций Spark: Всегда отдавайте предпочтение встроенным функциям Spark, так как они оптимизированы и выполняются непосредственно в JVM.
- Pandas UDF (векторизованные UDF): Начиная с Spark 2.3, появились Pandas UDF, которые используют Apache Arrow для эффективной передачи данных и векторизованных операций. Они значительно быстрее, потому что работают с целыми столбцами сразу, а не по одной строке.
- Избегание ненужных UDF: Если преобразование можно выразить с помощью встроенных функций, не используйте UDF.
- Минимизация передачи данных: Старайтесь использовать UDF, которые работают с примитивными типами и не возвращают сложные структуры, чтобы уменьшить накладные расходы на сериализацию.
- Кэширование данных: Если UDF используются многократно, может быть полезно закэшировать DataFrame, чтобы избежать повторных вычислений.







1) Apache Nifi vs другие инструменты:

Визуальное программирование: Интуитивный UI vs код/конфиги (Kafka Connect, Airflow).
Готовые процессоры: Более 400 встроенных коннекторов vs разработка кастомных логик.
Data Agnostic: Работа с любыми данными "как есть" vs строгая схема (Avro/Protobuf в Kafka).
Ротация данных: Приоритет на доставку, буферизацию и QoS vs только передачу.
Мониторинг: Сквозное отслеживание данных в реальном времени vs логи и метрики.
Ключевое преимущество: Скорость разработки ETL/ELT для разнородных источников без программирования.

2) Масштабирование GreenPlum + Kafka + Nifi:

Nifi: Горизонтальное масштабирование в кластере, балансировка нагрузки на процессоры.
Kafka: Увеличение числа партиций топиков, добавление брокеров в кластер.
GreenPlum: Добавление сегментов (сегментирование по ключу), использование PolyMC для параллельной загрузки.

Оптимизация Nifi:
- Распараллеливание потоков для чтения из Kafka.
- Настройка ExecuteSQL на выгрузку данных пачками.
- Использование PutDatabaseRecord для эффективной пакетной записи.


1) Оптимизация HBase + Hadoop:
Локальность данных: Размещение RegionServer на тех же узлах, что и DataNode HDFS.
Блочный кэш HBase: Настройка размера и приоритетов для частых операций чтения.
MemStore & BlockSize: Оптимизация под паттерн записи (больше MemStore) или чтения (большие блоки).
Compaction: Настройка политики мержа HFile (уменьшение числа файлов, баланс I/O).
Кэш HDFS: Использование централизованного кэша HDFS для часто читаемых данных.

2) Масштабирование и балансировка HBase:
Горизонтальное масштабирование: Добавление RegionServer в кластер.
Автобалансировка: HBase автоматически перераспределяет регионы между серверами.
Ручная балансировка: Принудительное перемещение регионов через HMaster Shell.
Row Key Design: Равномерное распределение данных (избегать монотонных ключей).

3) Резервное копирование Big Data:
Export/Import: Утилиты hbase org.apache.hadoop.hbase.mapreduce.Export.
Snapshot: Мгновенные снимки HBase без остановки (интеграция с HDFS).
Replication: Настройка репликации на другой кластер (Near Real-Time).
Хранение: Копирование снапшотов в объектное хранилище (S3) или другую файловую систему.

4) Мониторинг и кеширование HBase:
Мониторинг: HBase UI (Metrics), JMX, интеграция с Prometheus/Grafana.
Ключевые метрики: Request Latency, RegionServer metrics, Compaction Queue.

Кеширование:
- BlockCache: Кэш данных HFile (на чтение).
- MemStore: Буфер записи (перед сбросом в HFile).
- BucketCache: Off-heap кэш для больших кластеров.

